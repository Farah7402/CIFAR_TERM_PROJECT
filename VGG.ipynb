{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5d97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "USE 1 GPUs!\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 0 |  Loss: (2.3263) | Acc: (3.12%) (4/128)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 10 |  Loss: (2.3154) | Acc: (9.66%) (136/1408)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 20 |  Loss: (2.3034) | Acc: (11.27%) (303/2688)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 30 |  Loss: (2.2927) | Acc: (12.07%) (479/3968)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 40 |  Loss: (2.2827) | Acc: (13.11%) (688/5248)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 50 |  Loss: (2.2745) | Acc: (14.00%) (914/6528)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 60 |  Loss: (2.2640) | Acc: (14.97%) (1169/7808)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 70 |  Loss: (2.2530) | Acc: (15.62%) (1420/9088)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 80 |  Loss: (2.2408) | Acc: (16.21%) (1681/10368)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 90 |  Loss: (2.2270) | Acc: (16.85%) (1963/11648)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 100 |  Loss: (2.2122) | Acc: (17.47%) (2258/12928)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 110 |  Loss: (2.1995) | Acc: (17.93%) (2548/14208)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 120 |  Loss: (2.1849) | Acc: (18.61%) (2882/15488)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 130 |  Loss: (2.1703) | Acc: (19.09%) (3201/16768)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 140 |  Loss: (2.1549) | Acc: (19.53%) (3524/18048)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 150 |  Loss: (2.1396) | Acc: (20.09%) (3883/19328)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 160 |  Loss: (2.1246) | Acc: (20.62%) (4249/20608)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 170 |  Loss: (2.1082) | Acc: (21.23%) (4647/21888)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 180 |  Loss: (2.0948) | Acc: (21.51%) (4983/23168)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 190 |  Loss: (2.0816) | Acc: (21.86%) (5345/24448)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 200 |  Loss: (2.0675) | Acc: (22.33%) (5746/25728)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 210 |  Loss: (2.0534) | Acc: (22.74%) (6142/27008)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 220 |  Loss: (2.0397) | Acc: (23.28%) (6585/28288)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 230 |  Loss: (2.0264) | Acc: (23.71%) (7010/29568)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 240 |  Loss: (2.0141) | Acc: (24.12%) (7440/30848)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 250 |  Loss: (2.0030) | Acc: (24.48%) (7865/32128)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 260 |  Loss: (1.9906) | Acc: (24.93%) (8329/33408)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 270 |  Loss: (1.9779) | Acc: (25.32%) (8782/34688)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 280 |  Loss: (1.9655) | Acc: (25.70%) (9242/35968)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 290 |  Loss: (1.9559) | Acc: (26.02%) (9692/37248)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 300 |  Loss: (1.9457) | Acc: (26.34%) (10147/38528)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 310 |  Loss: (1.9358) | Acc: (26.65%) (10607/39808)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 320 |  Loss: (1.9254) | Acc: (27.03%) (11105/41088)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 330 |  Loss: (1.9156) | Acc: (27.39%) (11603/42368)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 340 |  Loss: (1.9052) | Acc: (27.80%) (12133/43648)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 350 |  Loss: (1.8942) | Acc: (28.17%) (12654/44928)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 360 |  Loss: (1.8847) | Acc: (28.53%) (13183/46208)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 370 |  Loss: (1.8754) | Acc: (28.85%) (13698/47488)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 380 |  Loss: (1.8660) | Acc: (29.24%) (14261/48768)\n",
      "#TRAIN: Epoch: 0 | Batch_idx: 390 |  Loss: (1.8571) | Acc: (29.57%) (14785/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.4169) | Acc: (47.65%) (4765/10000)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 0 |  Loss: (1.5127) | Acc: (43.75%) (56/128)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 10 |  Loss: (1.5203) | Acc: (41.97%) (591/1408)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 20 |  Loss: (1.5025) | Acc: (42.75%) (1149/2688)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 30 |  Loss: (1.4863) | Acc: (43.45%) (1724/3968)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 40 |  Loss: (1.4782) | Acc: (43.62%) (2289/5248)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 50 |  Loss: (1.4769) | Acc: (43.60%) (2846/6528)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 60 |  Loss: (1.4713) | Acc: (44.20%) (3451/7808)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 70 |  Loss: (1.4711) | Acc: (44.48%) (4042/9088)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 80 |  Loss: (1.4663) | Acc: (44.61%) (4625/10368)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 90 |  Loss: (1.4608) | Acc: (45.07%) (5250/11648)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 100 |  Loss: (1.4514) | Acc: (45.38%) (5867/12928)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 110 |  Loss: (1.4483) | Acc: (45.56%) (6473/14208)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 120 |  Loss: (1.4468) | Acc: (45.69%) (7076/15488)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 130 |  Loss: (1.4433) | Acc: (45.93%) (7701/16768)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 140 |  Loss: (1.4364) | Acc: (46.10%) (8321/18048)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 150 |  Loss: (1.4294) | Acc: (46.35%) (8959/19328)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 160 |  Loss: (1.4281) | Acc: (46.41%) (9564/20608)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 170 |  Loss: (1.4250) | Acc: (46.44%) (10164/21888)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 180 |  Loss: (1.4212) | Acc: (46.67%) (10813/23168)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 190 |  Loss: (1.4188) | Acc: (46.72%) (11423/24448)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 200 |  Loss: (1.4155) | Acc: (46.86%) (12055/25728)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 210 |  Loss: (1.4134) | Acc: (47.07%) (12712/27008)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 220 |  Loss: (1.4093) | Acc: (47.21%) (13355/28288)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 230 |  Loss: (1.4045) | Acc: (47.39%) (14011/29568)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 240 |  Loss: (1.4020) | Acc: (47.56%) (14670/30848)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 250 |  Loss: (1.3991) | Acc: (47.74%) (15338/32128)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 260 |  Loss: (1.3932) | Acc: (48.00%) (16036/33408)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 270 |  Loss: (1.3900) | Acc: (48.13%) (16697/34688)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 280 |  Loss: (1.3870) | Acc: (48.32%) (17379/35968)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 290 |  Loss: (1.3853) | Acc: (48.48%) (18056/37248)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 300 |  Loss: (1.3826) | Acc: (48.55%) (18707/38528)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 310 |  Loss: (1.3788) | Acc: (48.71%) (19392/39808)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 320 |  Loss: (1.3750) | Acc: (48.89%) (20088/41088)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 330 |  Loss: (1.3721) | Acc: (49.00%) (20760/42368)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 340 |  Loss: (1.3679) | Acc: (49.16%) (21459/43648)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 350 |  Loss: (1.3652) | Acc: (49.34%) (22166/44928)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 360 |  Loss: (1.3613) | Acc: (49.50%) (22872/46208)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 370 |  Loss: (1.3581) | Acc: (49.62%) (23563/47488)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 380 |  Loss: (1.3533) | Acc: (49.87%) (24321/48768)\n",
      "#TRAIN: Epoch: 1 | Batch_idx: 390 |  Loss: (1.3500) | Acc: (50.02%) (25010/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.1218) | Acc: (58.72%) (5872/10000)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 0 |  Loss: (1.0513) | Acc: (62.50%) (80/128)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 10 |  Loss: (1.1967) | Acc: (55.68%) (784/1408)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 20 |  Loss: (1.1987) | Acc: (55.73%) (1498/2688)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 30 |  Loss: (1.1796) | Acc: (56.78%) (2253/3968)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 40 |  Loss: (1.1771) | Acc: (57.03%) (2993/5248)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 50 |  Loss: (1.1764) | Acc: (56.95%) (3718/6528)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 60 |  Loss: (1.1800) | Acc: (56.83%) (4437/7808)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 70 |  Loss: (1.1789) | Acc: (56.97%) (5177/9088)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 80 |  Loss: (1.1743) | Acc: (57.18%) (5928/10368)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 90 |  Loss: (1.1694) | Acc: (57.43%) (6690/11648)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 100 |  Loss: (1.1715) | Acc: (57.46%) (7429/12928)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 110 |  Loss: (1.1696) | Acc: (57.50%) (8170/14208)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 120 |  Loss: (1.1703) | Acc: (57.50%) (8905/15488)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 130 |  Loss: (1.1710) | Acc: (57.44%) (9632/16768)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 140 |  Loss: (1.1676) | Acc: (57.67%) (10408/18048)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 150 |  Loss: (1.1679) | Acc: (57.68%) (11148/19328)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 160 |  Loss: (1.1642) | Acc: (57.86%) (11923/20608)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 170 |  Loss: (1.1617) | Acc: (57.84%) (12661/21888)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 180 |  Loss: (1.1647) | Acc: (57.85%) (13402/23168)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 190 |  Loss: (1.1642) | Acc: (57.95%) (14168/24448)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 200 |  Loss: (1.1627) | Acc: (57.97%) (14915/25728)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 210 |  Loss: (1.1588) | Acc: (58.13%) (15701/27008)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 220 |  Loss: (1.1556) | Acc: (58.28%) (16486/28288)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 230 |  Loss: (1.1556) | Acc: (58.24%) (17220/29568)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 240 |  Loss: (1.1535) | Acc: (58.30%) (17985/30848)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 250 |  Loss: (1.1499) | Acc: (58.40%) (18764/32128)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 260 |  Loss: (1.1491) | Acc: (58.45%) (19528/33408)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 270 |  Loss: (1.1481) | Acc: (58.53%) (20303/34688)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 280 |  Loss: (1.1467) | Acc: (58.63%) (21087/35968)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 290 |  Loss: (1.1434) | Acc: (58.76%) (21888/37248)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 300 |  Loss: (1.1409) | Acc: (58.82%) (22662/38528)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 310 |  Loss: (1.1384) | Acc: (58.93%) (23457/39808)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 320 |  Loss: (1.1359) | Acc: (59.01%) (24245/41088)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 330 |  Loss: (1.1349) | Acc: (59.06%) (25022/42368)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 340 |  Loss: (1.1322) | Acc: (59.16%) (25821/43648)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 350 |  Loss: (1.1314) | Acc: (59.21%) (26601/44928)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 360 |  Loss: (1.1294) | Acc: (59.29%) (27397/46208)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 370 |  Loss: (1.1276) | Acc: (59.35%) (28186/47488)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 380 |  Loss: (1.1272) | Acc: (59.38%) (28958/48768)\n",
      "#TRAIN: Epoch: 2 | Batch_idx: 390 |  Loss: (1.1252) | Acc: (59.46%) (29730/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (1.0371) | Acc: (63.18%) (6318/10000)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 0 |  Loss: (0.8752) | Acc: (70.31%) (90/128)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 10 |  Loss: (1.0580) | Acc: (61.01%) (859/1408)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 20 |  Loss: (1.0715) | Acc: (61.38%) (1650/2688)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 30 |  Loss: (1.0667) | Acc: (61.82%) (2453/3968)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 40 |  Loss: (1.0587) | Acc: (62.29%) (3269/5248)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 50 |  Loss: (1.0545) | Acc: (62.67%) (4091/6528)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 60 |  Loss: (1.0564) | Acc: (62.74%) (4899/7808)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 70 |  Loss: (1.0552) | Acc: (62.89%) (5715/9088)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 80 |  Loss: (1.0483) | Acc: (63.10%) (6542/10368)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 90 |  Loss: (1.0431) | Acc: (63.21%) (7363/11648)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 100 |  Loss: (1.0378) | Acc: (63.30%) (8184/12928)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 110 |  Loss: (1.0314) | Acc: (63.42%) (9011/14208)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 120 |  Loss: (1.0264) | Acc: (63.58%) (9847/15488)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 130 |  Loss: (1.0218) | Acc: (63.66%) (10675/16768)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 140 |  Loss: (1.0225) | Acc: (63.60%) (11478/18048)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 150 |  Loss: (1.0246) | Acc: (63.49%) (12272/19328)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 160 |  Loss: (1.0245) | Acc: (63.56%) (13098/20608)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 170 |  Loss: (1.0237) | Acc: (63.53%) (13906/21888)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 180 |  Loss: (1.0237) | Acc: (63.58%) (14730/23168)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 190 |  Loss: (1.0257) | Acc: (63.61%) (15552/24448)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 200 |  Loss: (1.0236) | Acc: (63.70%) (16388/25728)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 210 |  Loss: (1.0232) | Acc: (63.71%) (17208/27008)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 220 |  Loss: (1.0211) | Acc: (63.75%) (18034/28288)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 230 |  Loss: (1.0204) | Acc: (63.73%) (18844/29568)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 240 |  Loss: (1.0191) | Acc: (63.87%) (19704/30848)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 250 |  Loss: (1.0190) | Acc: (63.80%) (20499/32128)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 260 |  Loss: (1.0171) | Acc: (63.86%) (21336/33408)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 270 |  Loss: (1.0167) | Acc: (63.89%) (22161/34688)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 280 |  Loss: (1.0144) | Acc: (63.97%) (23007/35968)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 290 |  Loss: (1.0132) | Acc: (64.04%) (23854/37248)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 300 |  Loss: (1.0147) | Acc: (63.96%) (24642/38528)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 310 |  Loss: (1.0112) | Acc: (64.09%) (25512/39808)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 320 |  Loss: (1.0105) | Acc: (64.15%) (26357/41088)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 330 |  Loss: (1.0074) | Acc: (64.27%) (27230/42368)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 340 |  Loss: (1.0072) | Acc: (64.29%) (28063/43648)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 350 |  Loss: (1.0052) | Acc: (64.35%) (28910/44928)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 360 |  Loss: (1.0036) | Acc: (64.41%) (29762/46208)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 370 |  Loss: (1.0014) | Acc: (64.49%) (30623/47488)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 380 |  Loss: (0.9991) | Acc: (64.59%) (31500/48768)\n",
      "#TRAIN: Epoch: 3 | Batch_idx: 390 |  Loss: (0.9976) | Acc: (64.64%) (32322/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.9694) | Acc: (65.41%) (6541/10000)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 0 |  Loss: (0.8574) | Acc: (67.97%) (87/128)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 10 |  Loss: (0.9329) | Acc: (67.90%) (956/1408)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 20 |  Loss: (0.9276) | Acc: (67.26%) (1808/2688)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 30 |  Loss: (0.9175) | Acc: (67.82%) (2691/3968)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 40 |  Loss: (0.9168) | Acc: (67.70%) (3553/5248)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 50 |  Loss: (0.9157) | Acc: (67.75%) (4423/6528)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 60 |  Loss: (0.9198) | Acc: (67.66%) (5283/7808)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 70 |  Loss: (0.9176) | Acc: (67.69%) (6152/9088)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 80 |  Loss: (0.9153) | Acc: (67.66%) (7015/10368)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 90 |  Loss: (0.9213) | Acc: (67.51%) (7864/11648)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 100 |  Loss: (0.9193) | Acc: (67.51%) (8728/12928)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 110 |  Loss: (0.9172) | Acc: (67.55%) (9598/14208)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 120 |  Loss: (0.9167) | Acc: (67.57%) (10466/15488)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 130 |  Loss: (0.9187) | Acc: (67.48%) (11315/16768)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 140 |  Loss: (0.9202) | Acc: (67.45%) (12173/18048)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 150 |  Loss: (0.9184) | Acc: (67.44%) (13035/19328)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 160 |  Loss: (0.9185) | Acc: (67.45%) (13901/20608)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 170 |  Loss: (0.9164) | Acc: (67.46%) (14766/21888)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 180 |  Loss: (0.9146) | Acc: (67.52%) (15643/23168)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 190 |  Loss: (0.9144) | Acc: (67.48%) (16497/24448)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 200 |  Loss: (0.9114) | Acc: (67.54%) (17377/25728)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 210 |  Loss: (0.9116) | Acc: (67.54%) (18240/27008)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 220 |  Loss: (0.9128) | Acc: (67.50%) (19095/28288)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 230 |  Loss: (0.9106) | Acc: (67.52%) (19963/29568)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 240 |  Loss: (0.9112) | Acc: (67.48%) (20816/30848)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 250 |  Loss: (0.9099) | Acc: (67.49%) (21684/32128)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 260 |  Loss: (0.9102) | Acc: (67.53%) (22560/33408)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 270 |  Loss: (0.9105) | Acc: (67.53%) (23426/34688)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 280 |  Loss: (0.9117) | Acc: (67.49%) (24274/35968)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 290 |  Loss: (0.9104) | Acc: (67.58%) (25172/37248)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 300 |  Loss: (0.9072) | Acc: (67.71%) (26088/38528)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 310 |  Loss: (0.9068) | Acc: (67.75%) (26970/39808)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 320 |  Loss: (0.9073) | Acc: (67.73%) (27828/41088)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 330 |  Loss: (0.9070) | Acc: (67.71%) (28688/42368)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 340 |  Loss: (0.9069) | Acc: (67.74%) (29568/43648)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 350 |  Loss: (0.9071) | Acc: (67.76%) (30442/44928)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 360 |  Loss: (0.9056) | Acc: (67.79%) (31326/46208)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 370 |  Loss: (0.9042) | Acc: (67.86%) (32227/47488)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 380 |  Loss: (0.9030) | Acc: (67.93%) (33130/48768)\n",
      "#TRAIN: Epoch: 4 | Batch_idx: 390 |  Loss: (0.9018) | Acc: (67.99%) (33993/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8214) | Acc: (70.52%) (7052/10000)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 0 |  Loss: (0.7205) | Acc: (74.22%) (95/128)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 10 |  Loss: (0.8146) | Acc: (71.16%) (1002/1408)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 20 |  Loss: (0.8284) | Acc: (71.54%) (1923/2688)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 30 |  Loss: (0.8378) | Acc: (70.99%) (2817/3968)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 40 |  Loss: (0.8357) | Acc: (71.19%) (3736/5248)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 50 |  Loss: (0.8329) | Acc: (70.83%) (4624/6528)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 60 |  Loss: (0.8361) | Acc: (70.54%) (5508/7808)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 70 |  Loss: (0.8269) | Acc: (70.70%) (6425/9088)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 80 |  Loss: (0.8343) | Acc: (70.50%) (7309/10368)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 90 |  Loss: (0.8320) | Acc: (70.76%) (8242/11648)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 100 |  Loss: (0.8368) | Acc: (70.46%) (9109/12928)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 110 |  Loss: (0.8307) | Acc: (70.68%) (10042/14208)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 120 |  Loss: (0.8311) | Acc: (70.82%) (10968/15488)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 130 |  Loss: (0.8274) | Acc: (70.94%) (11896/16768)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 140 |  Loss: (0.8254) | Acc: (71.03%) (12819/18048)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 150 |  Loss: (0.8272) | Acc: (71.00%) (13723/19328)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 160 |  Loss: (0.8264) | Acc: (70.91%) (14613/20608)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 170 |  Loss: (0.8297) | Acc: (70.82%) (15500/21888)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 180 |  Loss: (0.8303) | Acc: (70.81%) (16406/23168)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 190 |  Loss: (0.8300) | Acc: (70.76%) (17300/24448)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 200 |  Loss: (0.8315) | Acc: (70.75%) (18202/25728)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 210 |  Loss: (0.8319) | Acc: (70.74%) (19106/27008)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 220 |  Loss: (0.8316) | Acc: (70.81%) (20031/28288)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 230 |  Loss: (0.8313) | Acc: (70.82%) (20940/29568)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 240 |  Loss: (0.8314) | Acc: (70.84%) (21854/30848)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 250 |  Loss: (0.8330) | Acc: (70.82%) (22753/32128)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 260 |  Loss: (0.8325) | Acc: (70.84%) (23666/33408)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 270 |  Loss: (0.8322) | Acc: (70.81%) (24561/34688)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 280 |  Loss: (0.8317) | Acc: (70.82%) (25471/35968)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 290 |  Loss: (0.8309) | Acc: (70.88%) (26400/37248)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 300 |  Loss: (0.8297) | Acc: (70.92%) (27323/38528)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 310 |  Loss: (0.8291) | Acc: (70.94%) (28238/39808)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 320 |  Loss: (0.8284) | Acc: (70.99%) (29170/41088)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 330 |  Loss: (0.8288) | Acc: (71.00%) (30080/42368)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 340 |  Loss: (0.8290) | Acc: (71.02%) (30997/43648)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 350 |  Loss: (0.8287) | Acc: (71.01%) (31905/44928)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 360 |  Loss: (0.8286) | Acc: (71.04%) (32827/46208)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 370 |  Loss: (0.8271) | Acc: (71.10%) (33765/47488)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 380 |  Loss: (0.8252) | Acc: (71.13%) (34688/48768)\n",
      "#TRAIN: Epoch: 5 | Batch_idx: 390 |  Loss: (0.8251) | Acc: (71.13%) (35564/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.8002) | Acc: (72.22%) (7222/10000)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 0 |  Loss: (0.6401) | Acc: (79.69%) (102/128)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 10 |  Loss: (0.7520) | Acc: (74.08%) (1043/1408)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 20 |  Loss: (0.7767) | Acc: (73.47%) (1975/2688)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 30 |  Loss: (0.7793) | Acc: (73.01%) (2897/3968)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 40 |  Loss: (0.7667) | Acc: (73.76%) (3871/5248)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 50 |  Loss: (0.7725) | Acc: (73.47%) (4796/6528)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 60 |  Loss: (0.7732) | Acc: (73.28%) (5722/7808)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 70 |  Loss: (0.7703) | Acc: (73.33%) (6664/9088)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 80 |  Loss: (0.7701) | Acc: (73.31%) (7601/10368)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 90 |  Loss: (0.7708) | Acc: (73.14%) (8519/11648)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 100 |  Loss: (0.7701) | Acc: (73.16%) (9458/12928)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 110 |  Loss: (0.7680) | Acc: (73.16%) (10394/14208)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 120 |  Loss: (0.7652) | Acc: (73.33%) (11358/15488)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 130 |  Loss: (0.7653) | Acc: (73.34%) (12298/16768)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 140 |  Loss: (0.7667) | Acc: (73.25%) (13220/18048)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 150 |  Loss: (0.7677) | Acc: (73.19%) (14147/19328)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 160 |  Loss: (0.7689) | Acc: (73.14%) (15073/20608)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 170 |  Loss: (0.7677) | Acc: (73.14%) (16008/21888)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 180 |  Loss: (0.7678) | Acc: (73.19%) (16956/23168)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 190 |  Loss: (0.7682) | Acc: (73.20%) (17896/24448)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 200 |  Loss: (0.7676) | Acc: (73.20%) (18832/25728)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 210 |  Loss: (0.7674) | Acc: (73.25%) (19784/27008)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 220 |  Loss: (0.7685) | Acc: (73.26%) (20723/28288)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 230 |  Loss: (0.7679) | Acc: (73.34%) (21686/29568)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 240 |  Loss: (0.7670) | Acc: (73.39%) (22640/30848)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 250 |  Loss: (0.7660) | Acc: (73.43%) (23592/32128)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 260 |  Loss: (0.7646) | Acc: (73.46%) (24543/33408)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 270 |  Loss: (0.7643) | Acc: (73.47%) (25485/34688)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 280 |  Loss: (0.7652) | Acc: (73.45%) (26417/35968)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 290 |  Loss: (0.7640) | Acc: (73.48%) (27368/37248)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 300 |  Loss: (0.7660) | Acc: (73.45%) (28298/38528)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 310 |  Loss: (0.7653) | Acc: (73.47%) (29246/39808)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 320 |  Loss: (0.7652) | Acc: (73.47%) (30187/41088)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 330 |  Loss: (0.7645) | Acc: (73.48%) (31134/42368)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 340 |  Loss: (0.7639) | Acc: (73.50%) (32083/43648)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 350 |  Loss: (0.7629) | Acc: (73.57%) (33052/44928)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 360 |  Loss: (0.7622) | Acc: (73.56%) (33990/46208)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 370 |  Loss: (0.7631) | Acc: (73.53%) (34916/47488)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 380 |  Loss: (0.7632) | Acc: (73.53%) (35858/48768)\n",
      "#TRAIN: Epoch: 6 | Batch_idx: 390 |  Loss: (0.7614) | Acc: (73.61%) (36806/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6987) | Acc: (74.86%) (7486/10000)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 0 |  Loss: (0.8195) | Acc: (72.66%) (93/128)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 10 |  Loss: (0.6976) | Acc: (76.42%) (1076/1408)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 20 |  Loss: (0.7205) | Acc: (75.15%) (2020/2688)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 30 |  Loss: (0.7291) | Acc: (75.13%) (2981/3968)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 40 |  Loss: (0.7301) | Acc: (75.15%) (3944/5248)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 50 |  Loss: (0.7262) | Acc: (75.47%) (4927/6528)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 60 |  Loss: (0.7263) | Acc: (75.31%) (5880/7808)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 70 |  Loss: (0.7313) | Acc: (75.09%) (6824/9088)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 80 |  Loss: (0.7339) | Acc: (74.91%) (7767/10368)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 90 |  Loss: (0.7310) | Acc: (74.89%) (8723/11648)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 100 |  Loss: (0.7273) | Acc: (75.02%) (9699/12928)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 110 |  Loss: (0.7259) | Acc: (75.01%) (10657/14208)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 120 |  Loss: (0.7245) | Acc: (75.09%) (11630/15488)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 130 |  Loss: (0.7204) | Acc: (75.23%) (12614/16768)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 140 |  Loss: (0.7228) | Acc: (75.07%) (13548/18048)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 150 |  Loss: (0.7262) | Acc: (74.93%) (14482/19328)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 160 |  Loss: (0.7280) | Acc: (74.93%) (15441/20608)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 170 |  Loss: (0.7283) | Acc: (74.95%) (16405/21888)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 180 |  Loss: (0.7293) | Acc: (74.91%) (17355/23168)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 190 |  Loss: (0.7261) | Acc: (74.93%) (18320/24448)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 200 |  Loss: (0.7267) | Acc: (74.94%) (19280/25728)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 210 |  Loss: (0.7276) | Acc: (74.84%) (20214/27008)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 220 |  Loss: (0.7313) | Acc: (74.77%) (21150/28288)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 230 |  Loss: (0.7299) | Acc: (74.83%) (22125/29568)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 240 |  Loss: (0.7299) | Acc: (74.81%) (23076/30848)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 250 |  Loss: (0.7299) | Acc: (74.78%) (24024/32128)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 260 |  Loss: (0.7293) | Acc: (74.80%) (24989/33408)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 270 |  Loss: (0.7263) | Acc: (74.86%) (25967/34688)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 280 |  Loss: (0.7246) | Acc: (74.87%) (26930/35968)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 290 |  Loss: (0.7247) | Acc: (74.88%) (27891/37248)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 300 |  Loss: (0.7240) | Acc: (74.89%) (28855/38528)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 310 |  Loss: (0.7228) | Acc: (74.97%) (29844/39808)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 320 |  Loss: (0.7240) | Acc: (74.91%) (30777/41088)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 330 |  Loss: (0.7230) | Acc: (74.93%) (31748/42368)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 340 |  Loss: (0.7214) | Acc: (74.96%) (32720/43648)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 350 |  Loss: (0.7191) | Acc: (75.07%) (33727/44928)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 360 |  Loss: (0.7180) | Acc: (75.12%) (34711/46208)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 370 |  Loss: (0.7179) | Acc: (75.11%) (35670/47488)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 380 |  Loss: (0.7170) | Acc: (75.13%) (36640/48768)\n",
      "#TRAIN: Epoch: 7 | Batch_idx: 390 |  Loss: (0.7169) | Acc: (75.13%) (37567/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.7484) | Acc: (73.96%) (7396/10000)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 0 |  Loss: (0.6713) | Acc: (77.34%) (99/128)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 10 |  Loss: (0.7045) | Acc: (75.64%) (1065/1408)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 20 |  Loss: (0.7100) | Acc: (75.71%) (2035/2688)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 30 |  Loss: (0.6988) | Acc: (75.88%) (3011/3968)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 40 |  Loss: (0.6892) | Acc: (76.20%) (3999/5248)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 50 |  Loss: (0.6934) | Acc: (76.27%) (4979/6528)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 60 |  Loss: (0.6845) | Acc: (76.52%) (5975/7808)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 70 |  Loss: (0.6853) | Acc: (76.47%) (6950/9088)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 80 |  Loss: (0.6857) | Acc: (76.59%) (7941/10368)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 90 |  Loss: (0.6767) | Acc: (76.97%) (8966/11648)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 100 |  Loss: (0.6768) | Acc: (76.93%) (9945/12928)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 110 |  Loss: (0.6771) | Acc: (76.92%) (10929/14208)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 120 |  Loss: (0.6789) | Acc: (76.75%) (11887/15488)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 130 |  Loss: (0.6781) | Acc: (76.84%) (12884/16768)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 140 |  Loss: (0.6802) | Acc: (76.76%) (13853/18048)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 150 |  Loss: (0.6798) | Acc: (76.80%) (14844/19328)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 160 |  Loss: (0.6820) | Acc: (76.71%) (15809/20608)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 170 |  Loss: (0.6812) | Acc: (76.69%) (16787/21888)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 180 |  Loss: (0.6803) | Acc: (76.75%) (17781/23168)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 190 |  Loss: (0.6785) | Acc: (76.82%) (18780/24448)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 200 |  Loss: (0.6789) | Acc: (76.77%) (19752/25728)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 210 |  Loss: (0.6799) | Acc: (76.73%) (20723/27008)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 220 |  Loss: (0.6772) | Acc: (76.83%) (21734/28288)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 230 |  Loss: (0.6741) | Acc: (76.94%) (22750/29568)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 240 |  Loss: (0.6760) | Acc: (76.93%) (23730/30848)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 250 |  Loss: (0.6762) | Acc: (76.94%) (24720/32128)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 260 |  Loss: (0.6791) | Acc: (76.86%) (25676/33408)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 270 |  Loss: (0.6765) | Acc: (76.92%) (26683/34688)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 280 |  Loss: (0.6759) | Acc: (76.98%) (27687/35968)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 290 |  Loss: (0.6773) | Acc: (76.92%) (28652/37248)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 300 |  Loss: (0.6761) | Acc: (76.98%) (29658/38528)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 310 |  Loss: (0.6757) | Acc: (76.94%) (30630/39808)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 320 |  Loss: (0.6763) | Acc: (76.93%) (31609/41088)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 330 |  Loss: (0.6753) | Acc: (76.92%) (32589/42368)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 340 |  Loss: (0.6741) | Acc: (76.97%) (33594/43648)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 350 |  Loss: (0.6745) | Acc: (76.92%) (34557/44928)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 360 |  Loss: (0.6742) | Acc: (76.92%) (35541/46208)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 370 |  Loss: (0.6739) | Acc: (76.89%) (36514/47488)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 380 |  Loss: (0.6729) | Acc: (76.92%) (37512/48768)\n",
      "#TRAIN: Epoch: 8 | Batch_idx: 390 |  Loss: (0.6732) | Acc: (76.90%) (38451/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6765) | Acc: (76.82%) (7682/10000)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 0 |  Loss: (0.8316) | Acc: (75.00%) (96/128)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 10 |  Loss: (0.6158) | Acc: (79.26%) (1116/1408)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 20 |  Loss: (0.6409) | Acc: (78.24%) (2103/2688)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 30 |  Loss: (0.6315) | Acc: (78.40%) (3111/3968)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 40 |  Loss: (0.6328) | Acc: (78.45%) (4117/5248)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 50 |  Loss: (0.6415) | Acc: (78.00%) (5092/6528)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 60 |  Loss: (0.6373) | Acc: (78.19%) (6105/7808)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 70 |  Loss: (0.6406) | Acc: (77.98%) (7087/9088)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 80 |  Loss: (0.6366) | Acc: (78.25%) (8113/10368)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 90 |  Loss: (0.6380) | Acc: (78.11%) (9098/11648)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 100 |  Loss: (0.6349) | Acc: (78.29%) (10121/12928)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 110 |  Loss: (0.6369) | Acc: (78.17%) (11107/14208)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 120 |  Loss: (0.6407) | Acc: (78.07%) (12092/15488)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 130 |  Loss: (0.6405) | Acc: (78.11%) (13097/16768)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 140 |  Loss: (0.6385) | Acc: (78.15%) (14105/18048)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 150 |  Loss: (0.6371) | Acc: (78.26%) (15126/19328)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 160 |  Loss: (0.6406) | Acc: (78.03%) (16080/20608)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 170 |  Loss: (0.6380) | Acc: (78.08%) (17091/21888)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 180 |  Loss: (0.6379) | Acc: (78.08%) (18089/23168)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 190 |  Loss: (0.6415) | Acc: (77.95%) (19057/24448)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 200 |  Loss: (0.6419) | Acc: (77.97%) (20061/25728)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 210 |  Loss: (0.6399) | Acc: (78.07%) (21086/27008)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 220 |  Loss: (0.6399) | Acc: (78.04%) (22077/28288)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 230 |  Loss: (0.6392) | Acc: (78.05%) (23077/29568)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 240 |  Loss: (0.6387) | Acc: (78.03%) (24071/30848)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 250 |  Loss: (0.6394) | Acc: (77.96%) (25047/32128)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 260 |  Loss: (0.6400) | Acc: (77.94%) (26038/33408)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 270 |  Loss: (0.6401) | Acc: (77.92%) (27028/34688)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 280 |  Loss: (0.6391) | Acc: (77.96%) (28039/35968)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 290 |  Loss: (0.6386) | Acc: (77.96%) (29037/37248)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 300 |  Loss: (0.6397) | Acc: (77.93%) (30023/38528)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 310 |  Loss: (0.6385) | Acc: (77.94%) (31028/39808)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 320 |  Loss: (0.6378) | Acc: (77.97%) (32035/41088)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 330 |  Loss: (0.6378) | Acc: (77.93%) (33019/42368)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 340 |  Loss: (0.6385) | Acc: (77.89%) (33997/43648)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 350 |  Loss: (0.6400) | Acc: (77.85%) (34977/44928)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 360 |  Loss: (0.6392) | Acc: (77.88%) (35989/46208)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 370 |  Loss: (0.6393) | Acc: (77.87%) (36979/47488)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 380 |  Loss: (0.6386) | Acc: (77.88%) (37980/48768)\n",
      "#TRAIN: Epoch: 9 | Batch_idx: 390 |  Loss: (0.6381) | Acc: (77.92%) (38958/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6242) | Acc: (78.81%) (7881/10000)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 0 |  Loss: (0.6997) | Acc: (74.22%) (95/128)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 10 |  Loss: (0.6009) | Acc: (77.63%) (1093/1408)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 20 |  Loss: (0.6033) | Acc: (77.86%) (2093/2688)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 30 |  Loss: (0.6132) | Acc: (77.67%) (3082/3968)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 40 |  Loss: (0.6159) | Acc: (77.74%) (4080/5248)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 50 |  Loss: (0.6211) | Acc: (77.77%) (5077/6528)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 60 |  Loss: (0.6339) | Acc: (77.33%) (6038/7808)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 70 |  Loss: (0.6301) | Acc: (77.50%) (7043/9088)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 80 |  Loss: (0.6272) | Acc: (77.70%) (8056/10368)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 90 |  Loss: (0.6233) | Acc: (77.82%) (9065/11648)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 100 |  Loss: (0.6216) | Acc: (77.94%) (10076/12928)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 110 |  Loss: (0.6186) | Acc: (78.11%) (11098/14208)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 120 |  Loss: (0.6188) | Acc: (78.09%) (12095/15488)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 130 |  Loss: (0.6191) | Acc: (78.14%) (13103/16768)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 140 |  Loss: (0.6168) | Acc: (78.19%) (14112/18048)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 150 |  Loss: (0.6171) | Acc: (78.21%) (15117/19328)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 160 |  Loss: (0.6185) | Acc: (78.23%) (16122/20608)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 170 |  Loss: (0.6166) | Acc: (78.28%) (17134/21888)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 180 |  Loss: (0.6160) | Acc: (78.40%) (18164/23168)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 190 |  Loss: (0.6157) | Acc: (78.44%) (19177/24448)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 200 |  Loss: (0.6124) | Acc: (78.63%) (20229/25728)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 210 |  Loss: (0.6129) | Acc: (78.65%) (21242/27008)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 220 |  Loss: (0.6120) | Acc: (78.68%) (22258/28288)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 230 |  Loss: (0.6115) | Acc: (78.68%) (23263/29568)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 240 |  Loss: (0.6098) | Acc: (78.74%) (24291/30848)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 250 |  Loss: (0.6076) | Acc: (78.86%) (25336/32128)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 260 |  Loss: (0.6073) | Acc: (78.85%) (26341/33408)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 270 |  Loss: (0.6063) | Acc: (78.85%) (27350/34688)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 280 |  Loss: (0.6070) | Acc: (78.86%) (28364/35968)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 290 |  Loss: (0.6080) | Acc: (78.81%) (29356/37248)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 300 |  Loss: (0.6092) | Acc: (78.77%) (30348/38528)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 310 |  Loss: (0.6100) | Acc: (78.78%) (31360/39808)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 320 |  Loss: (0.6090) | Acc: (78.84%) (32394/41088)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 330 |  Loss: (0.6085) | Acc: (78.84%) (33403/42368)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 340 |  Loss: (0.6085) | Acc: (78.84%) (34412/43648)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 350 |  Loss: (0.6089) | Acc: (78.86%) (35430/44928)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 360 |  Loss: (0.6088) | Acc: (78.87%) (36444/46208)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 370 |  Loss: (0.6078) | Acc: (78.86%) (37451/47488)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 380 |  Loss: (0.6083) | Acc: (78.86%) (38459/48768)\n",
      "#TRAIN: Epoch: 10 | Batch_idx: 390 |  Loss: (0.6088) | Acc: (78.90%) (39448/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6247) | Acc: (78.73%) (7873/10000)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 0 |  Loss: (0.6118) | Acc: (80.47%) (103/128)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 10 |  Loss: (0.6059) | Acc: (79.05%) (1113/1408)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 20 |  Loss: (0.5949) | Acc: (80.10%) (2153/2688)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 30 |  Loss: (0.5979) | Acc: (79.86%) (3169/3968)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 40 |  Loss: (0.6032) | Acc: (79.67%) (4181/5248)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 50 |  Loss: (0.5954) | Acc: (80.02%) (5224/6528)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 60 |  Loss: (0.5968) | Acc: (79.87%) (6236/7808)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 70 |  Loss: (0.6002) | Acc: (79.94%) (7265/9088)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 80 |  Loss: (0.6024) | Acc: (79.75%) (8268/10368)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 90 |  Loss: (0.6013) | Acc: (79.64%) (9277/11648)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 100 |  Loss: (0.5993) | Acc: (79.77%) (10313/12928)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 110 |  Loss: (0.5958) | Acc: (79.79%) (11336/14208)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 120 |  Loss: (0.5936) | Acc: (79.86%) (12368/15488)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 130 |  Loss: (0.5904) | Acc: (79.96%) (13407/16768)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 140 |  Loss: (0.5901) | Acc: (79.97%) (14433/18048)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 150 |  Loss: (0.5857) | Acc: (80.13%) (15487/19328)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 160 |  Loss: (0.5838) | Acc: (80.23%) (16534/20608)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 170 |  Loss: (0.5836) | Acc: (80.22%) (17558/21888)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 180 |  Loss: (0.5829) | Acc: (80.26%) (18595/23168)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 190 |  Loss: (0.5832) | Acc: (80.24%) (19617/24448)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 200 |  Loss: (0.5818) | Acc: (80.29%) (20658/25728)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 210 |  Loss: (0.5824) | Acc: (80.24%) (21671/27008)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 220 |  Loss: (0.5818) | Acc: (80.32%) (22721/28288)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 230 |  Loss: (0.5816) | Acc: (80.26%) (23732/29568)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 240 |  Loss: (0.5815) | Acc: (80.20%) (24741/30848)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 250 |  Loss: (0.5798) | Acc: (80.25%) (25782/32128)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 260 |  Loss: (0.5815) | Acc: (80.18%) (26785/33408)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 270 |  Loss: (0.5804) | Acc: (80.22%) (27826/34688)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 280 |  Loss: (0.5795) | Acc: (80.26%) (28869/35968)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 290 |  Loss: (0.5793) | Acc: (80.25%) (29892/37248)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 300 |  Loss: (0.5813) | Acc: (80.15%) (30880/38528)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 310 |  Loss: (0.5832) | Acc: (80.09%) (31882/39808)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 320 |  Loss: (0.5830) | Acc: (80.06%) (32894/41088)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 330 |  Loss: (0.5815) | Acc: (80.13%) (33951/42368)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 340 |  Loss: (0.5810) | Acc: (80.17%) (34993/43648)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 350 |  Loss: (0.5817) | Acc: (80.15%) (36008/44928)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 360 |  Loss: (0.5819) | Acc: (80.13%) (37026/46208)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 370 |  Loss: (0.5812) | Acc: (80.13%) (38054/47488)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 380 |  Loss: (0.5814) | Acc: (80.11%) (39066/48768)\n",
      "#TRAIN: Epoch: 11 | Batch_idx: 390 |  Loss: (0.5807) | Acc: (80.10%) (40052/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6060) | Acc: (79.75%) (7975/10000)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 0 |  Loss: (0.4797) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 10 |  Loss: (0.5631) | Acc: (80.61%) (1135/1408)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 20 |  Loss: (0.5562) | Acc: (80.43%) (2162/2688)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 30 |  Loss: (0.5816) | Acc: (79.81%) (3167/3968)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 40 |  Loss: (0.5780) | Acc: (80.20%) (4209/5248)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 50 |  Loss: (0.5696) | Acc: (80.44%) (5251/6528)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 60 |  Loss: (0.5659) | Acc: (80.53%) (6288/7808)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 70 |  Loss: (0.5581) | Acc: (80.93%) (7355/9088)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 80 |  Loss: (0.5627) | Acc: (80.86%) (8384/10368)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 90 |  Loss: (0.5667) | Acc: (80.83%) (9415/11648)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 100 |  Loss: (0.5656) | Acc: (81.00%) (10472/12928)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 110 |  Loss: (0.5649) | Acc: (80.95%) (11501/14208)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 120 |  Loss: (0.5633) | Acc: (80.89%) (12529/15488)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 130 |  Loss: (0.5632) | Acc: (80.94%) (13572/16768)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 140 |  Loss: (0.5622) | Acc: (81.00%) (14619/18048)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 150 |  Loss: (0.5630) | Acc: (81.03%) (15662/19328)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 160 |  Loss: (0.5604) | Acc: (81.16%) (16726/20608)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 170 |  Loss: (0.5595) | Acc: (81.21%) (17775/21888)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 180 |  Loss: (0.5600) | Acc: (81.12%) (18794/23168)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 190 |  Loss: (0.5595) | Acc: (81.15%) (19840/24448)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 200 |  Loss: (0.5594) | Acc: (81.17%) (20883/25728)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 210 |  Loss: (0.5590) | Acc: (81.19%) (21927/27008)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 220 |  Loss: (0.5592) | Acc: (81.14%) (22952/28288)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 230 |  Loss: (0.5588) | Acc: (81.18%) (24002/29568)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 240 |  Loss: (0.5591) | Acc: (81.12%) (25024/30848)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 250 |  Loss: (0.5575) | Acc: (81.18%) (26080/32128)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 260 |  Loss: (0.5566) | Acc: (81.19%) (27125/33408)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 270 |  Loss: (0.5563) | Acc: (81.20%) (28167/34688)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 280 |  Loss: (0.5552) | Acc: (81.22%) (29212/35968)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 290 |  Loss: (0.5540) | Acc: (81.26%) (30268/37248)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 300 |  Loss: (0.5541) | Acc: (81.27%) (31310/38528)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 310 |  Loss: (0.5531) | Acc: (81.32%) (32370/39808)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 320 |  Loss: (0.5538) | Acc: (81.26%) (33388/41088)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 330 |  Loss: (0.5534) | Acc: (81.27%) (34433/42368)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 340 |  Loss: (0.5541) | Acc: (81.22%) (35451/43648)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 350 |  Loss: (0.5541) | Acc: (81.21%) (36485/44928)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 360 |  Loss: (0.5546) | Acc: (81.17%) (37509/46208)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 370 |  Loss: (0.5540) | Acc: (81.17%) (38547/47488)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 380 |  Loss: (0.5540) | Acc: (81.17%) (39586/48768)\n",
      "#TRAIN: Epoch: 12 | Batch_idx: 390 |  Loss: (0.5547) | Acc: (81.13%) (40566/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5723) | Acc: (80.81%) (8081/10000)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 0 |  Loss: (0.4569) | Acc: (84.38%) (108/128)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 10 |  Loss: (0.5140) | Acc: (82.03%) (1155/1408)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 20 |  Loss: (0.5170) | Acc: (81.92%) (2202/2688)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 30 |  Loss: (0.5226) | Acc: (81.63%) (3239/3968)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 40 |  Loss: (0.5290) | Acc: (81.54%) (4279/5248)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 50 |  Loss: (0.5335) | Acc: (81.40%) (5314/6528)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 60 |  Loss: (0.5367) | Acc: (81.53%) (6366/7808)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 70 |  Loss: (0.5368) | Acc: (81.58%) (7414/9088)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 80 |  Loss: (0.5352) | Acc: (81.64%) (8464/10368)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 90 |  Loss: (0.5365) | Acc: (81.70%) (9516/11648)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 100 |  Loss: (0.5330) | Acc: (81.75%) (10569/12928)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 110 |  Loss: (0.5314) | Acc: (81.78%) (11620/14208)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 120 |  Loss: (0.5283) | Acc: (81.91%) (12686/15488)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 130 |  Loss: (0.5281) | Acc: (81.93%) (13738/16768)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 140 |  Loss: (0.5273) | Acc: (81.99%) (14798/18048)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 150 |  Loss: (0.5264) | Acc: (82.05%) (15859/19328)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 160 |  Loss: (0.5278) | Acc: (81.98%) (16894/20608)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 170 |  Loss: (0.5290) | Acc: (81.92%) (17930/21888)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 180 |  Loss: (0.5298) | Acc: (81.92%) (18979/23168)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 190 |  Loss: (0.5314) | Acc: (81.84%) (20008/24448)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 200 |  Loss: (0.5309) | Acc: (81.84%) (21055/25728)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 210 |  Loss: (0.5303) | Acc: (81.86%) (22109/27008)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 220 |  Loss: (0.5324) | Acc: (81.81%) (23141/28288)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 230 |  Loss: (0.5337) | Acc: (81.77%) (24177/29568)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 240 |  Loss: (0.5323) | Acc: (81.87%) (25255/30848)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 250 |  Loss: (0.5320) | Acc: (81.88%) (26308/32128)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 260 |  Loss: (0.5305) | Acc: (81.91%) (27364/33408)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 270 |  Loss: (0.5290) | Acc: (81.98%) (28437/34688)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 280 |  Loss: (0.5272) | Acc: (82.05%) (29512/35968)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 290 |  Loss: (0.5280) | Acc: (82.04%) (30559/37248)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 300 |  Loss: (0.5264) | Acc: (82.07%) (31620/38528)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 310 |  Loss: (0.5260) | Acc: (82.08%) (32674/39808)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 320 |  Loss: (0.5269) | Acc: (82.12%) (33741/41088)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 330 |  Loss: (0.5265) | Acc: (82.12%) (34794/42368)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 340 |  Loss: (0.5259) | Acc: (82.16%) (35862/43648)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 350 |  Loss: (0.5257) | Acc: (82.16%) (36912/44928)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 360 |  Loss: (0.5260) | Acc: (82.15%) (37960/46208)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 370 |  Loss: (0.5255) | Acc: (82.15%) (39011/47488)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 380 |  Loss: (0.5250) | Acc: (82.18%) (40077/48768)\n",
      "#TRAIN: Epoch: 13 | Batch_idx: 390 |  Loss: (0.5248) | Acc: (82.19%) (41095/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5825) | Acc: (80.86%) (8086/10000)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 0 |  Loss: (0.5485) | Acc: (78.12%) (100/128)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 10 |  Loss: (0.5225) | Acc: (82.17%) (1157/1408)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 20 |  Loss: (0.5214) | Acc: (83.48%) (2244/2688)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 30 |  Loss: (0.5183) | Acc: (83.17%) (3300/3968)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 40 |  Loss: (0.5260) | Acc: (82.79%) (4345/5248)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 50 |  Loss: (0.5270) | Acc: (82.51%) (5386/6528)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 60 |  Loss: (0.5275) | Acc: (82.33%) (6428/7808)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 70 |  Loss: (0.5190) | Acc: (82.55%) (7502/9088)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 80 |  Loss: (0.5184) | Acc: (82.44%) (8547/10368)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 90 |  Loss: (0.5155) | Acc: (82.45%) (9604/11648)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 100 |  Loss: (0.5187) | Acc: (82.32%) (10642/12928)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 110 |  Loss: (0.5180) | Acc: (82.27%) (11689/14208)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 120 |  Loss: (0.5157) | Acc: (82.33%) (12751/15488)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 130 |  Loss: (0.5143) | Acc: (82.35%) (13809/16768)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 140 |  Loss: (0.5144) | Acc: (82.36%) (14865/18048)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 150 |  Loss: (0.5149) | Acc: (82.36%) (15918/19328)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 160 |  Loss: (0.5123) | Acc: (82.47%) (16995/20608)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 170 |  Loss: (0.5133) | Acc: (82.42%) (18039/21888)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 180 |  Loss: (0.5136) | Acc: (82.42%) (19094/23168)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 190 |  Loss: (0.5112) | Acc: (82.48%) (20165/24448)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 200 |  Loss: (0.5125) | Acc: (82.45%) (21214/25728)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 210 |  Loss: (0.5128) | Acc: (82.45%) (22267/27008)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 220 |  Loss: (0.5122) | Acc: (82.53%) (23345/28288)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 230 |  Loss: (0.5131) | Acc: (82.48%) (24388/29568)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 240 |  Loss: (0.5142) | Acc: (82.45%) (25434/30848)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 250 |  Loss: (0.5138) | Acc: (82.44%) (26487/32128)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 260 |  Loss: (0.5128) | Acc: (82.46%) (27549/33408)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 270 |  Loss: (0.5115) | Acc: (82.50%) (28617/34688)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 280 |  Loss: (0.5114) | Acc: (82.51%) (29676/35968)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 290 |  Loss: (0.5118) | Acc: (82.47%) (30718/37248)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 300 |  Loss: (0.5133) | Acc: (82.42%) (31754/38528)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 310 |  Loss: (0.5132) | Acc: (82.44%) (32816/39808)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 320 |  Loss: (0.5139) | Acc: (82.42%) (33864/41088)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 330 |  Loss: (0.5142) | Acc: (82.39%) (34908/42368)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 340 |  Loss: (0.5140) | Acc: (82.41%) (35971/43648)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 350 |  Loss: (0.5142) | Acc: (82.37%) (37008/44928)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 360 |  Loss: (0.5143) | Acc: (82.36%) (38056/46208)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 370 |  Loss: (0.5146) | Acc: (82.36%) (39109/47488)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 380 |  Loss: (0.5143) | Acc: (82.38%) (40177/48768)\n",
      "#TRAIN: Epoch: 14 | Batch_idx: 390 |  Loss: (0.5134) | Acc: (82.40%) (41201/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6181) | Acc: (79.37%) (7937/10000)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 0 |  Loss: (0.3023) | Acc: (89.84%) (115/128)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 10 |  Loss: (0.4997) | Acc: (82.67%) (1164/1408)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 20 |  Loss: (0.4855) | Acc: (83.56%) (2246/2688)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 30 |  Loss: (0.4896) | Acc: (83.39%) (3309/3968)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 40 |  Loss: (0.4815) | Acc: (83.75%) (4395/5248)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 50 |  Loss: (0.4810) | Acc: (83.87%) (5475/6528)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 60 |  Loss: (0.4938) | Acc: (83.56%) (6524/7808)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 70 |  Loss: (0.4879) | Acc: (83.64%) (7601/9088)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 80 |  Loss: (0.4804) | Acc: (83.85%) (8694/10368)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 90 |  Loss: (0.4838) | Acc: (83.73%) (9753/11648)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 100 |  Loss: (0.4871) | Acc: (83.66%) (10815/12928)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 110 |  Loss: (0.4864) | Acc: (83.61%) (11880/14208)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 120 |  Loss: (0.4881) | Acc: (83.63%) (12952/15488)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 130 |  Loss: (0.4897) | Acc: (83.47%) (13997/16768)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 140 |  Loss: (0.4912) | Acc: (83.43%) (15058/18048)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 150 |  Loss: (0.4936) | Acc: (83.38%) (16115/19328)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 160 |  Loss: (0.4931) | Acc: (83.39%) (17186/20608)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 170 |  Loss: (0.4912) | Acc: (83.48%) (18272/21888)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 180 |  Loss: (0.4913) | Acc: (83.49%) (19343/23168)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 190 |  Loss: (0.4908) | Acc: (83.48%) (20409/24448)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 200 |  Loss: (0.4919) | Acc: (83.42%) (21462/25728)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 210 |  Loss: (0.4912) | Acc: (83.43%) (22534/27008)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 220 |  Loss: (0.4900) | Acc: (83.49%) (23619/28288)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 230 |  Loss: (0.4908) | Acc: (83.48%) (24683/29568)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 240 |  Loss: (0.4915) | Acc: (83.48%) (25753/30848)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 250 |  Loss: (0.4922) | Acc: (83.43%) (26804/32128)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 260 |  Loss: (0.4940) | Acc: (83.37%) (27852/33408)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 270 |  Loss: (0.4943) | Acc: (83.33%) (28907/34688)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 280 |  Loss: (0.4946) | Acc: (83.38%) (29989/35968)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 290 |  Loss: (0.4948) | Acc: (83.32%) (31036/37248)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 300 |  Loss: (0.4936) | Acc: (83.38%) (32124/38528)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 310 |  Loss: (0.4939) | Acc: (83.35%) (33180/39808)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 320 |  Loss: (0.4950) | Acc: (83.30%) (34225/41088)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 330 |  Loss: (0.4936) | Acc: (83.34%) (35310/42368)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 340 |  Loss: (0.4946) | Acc: (83.31%) (36361/43648)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 350 |  Loss: (0.4951) | Acc: (83.27%) (37413/44928)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 360 |  Loss: (0.4955) | Acc: (83.25%) (38466/46208)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 370 |  Loss: (0.4956) | Acc: (83.25%) (39534/47488)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 380 |  Loss: (0.4955) | Acc: (83.26%) (40603/48768)\n",
      "#TRAIN: Epoch: 15 | Batch_idx: 390 |  Loss: (0.4959) | Acc: (83.22%) (41609/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5845) | Acc: (80.50%) (8050/10000)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 0 |  Loss: (0.3500) | Acc: (83.59%) (107/128)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 10 |  Loss: (0.4600) | Acc: (83.31%) (1173/1408)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 20 |  Loss: (0.4529) | Acc: (83.82%) (2253/2688)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 30 |  Loss: (0.4572) | Acc: (83.90%) (3329/3968)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 40 |  Loss: (0.4638) | Acc: (83.88%) (4402/5248)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 50 |  Loss: (0.4694) | Acc: (83.70%) (5464/6528)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 60 |  Loss: (0.4682) | Acc: (83.84%) (6546/7808)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 70 |  Loss: (0.4687) | Acc: (83.96%) (7630/9088)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 80 |  Loss: (0.4697) | Acc: (83.83%) (8691/10368)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 90 |  Loss: (0.4679) | Acc: (83.89%) (9771/11648)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 100 |  Loss: (0.4695) | Acc: (83.77%) (10830/12928)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 110 |  Loss: (0.4687) | Acc: (83.86%) (11915/14208)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 120 |  Loss: (0.4712) | Acc: (83.81%) (12981/15488)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 130 |  Loss: (0.4712) | Acc: (83.80%) (14052/16768)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 140 |  Loss: (0.4742) | Acc: (83.76%) (15117/18048)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 150 |  Loss: (0.4734) | Acc: (83.79%) (16195/19328)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 160 |  Loss: (0.4703) | Acc: (83.96%) (17302/20608)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 170 |  Loss: (0.4687) | Acc: (84.05%) (18396/21888)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 180 |  Loss: (0.4692) | Acc: (84.06%) (19476/23168)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 190 |  Loss: (0.4691) | Acc: (84.06%) (20551/24448)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 200 |  Loss: (0.4698) | Acc: (84.03%) (21620/25728)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 210 |  Loss: (0.4689) | Acc: (84.03%) (22695/27008)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 220 |  Loss: (0.4682) | Acc: (84.09%) (23787/28288)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 230 |  Loss: (0.4678) | Acc: (84.08%) (24860/29568)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 240 |  Loss: (0.4678) | Acc: (84.11%) (25945/30848)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 250 |  Loss: (0.4673) | Acc: (84.09%) (27015/32128)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 260 |  Loss: (0.4686) | Acc: (84.02%) (28070/33408)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 270 |  Loss: (0.4696) | Acc: (84.01%) (29143/34688)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 280 |  Loss: (0.4691) | Acc: (84.06%) (30236/35968)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 290 |  Loss: (0.4671) | Acc: (84.15%) (31345/37248)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 300 |  Loss: (0.4678) | Acc: (84.12%) (32409/38528)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 310 |  Loss: (0.4680) | Acc: (84.10%) (33480/39808)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 320 |  Loss: (0.4696) | Acc: (84.05%) (34534/41088)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 330 |  Loss: (0.4699) | Acc: (84.02%) (35598/42368)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 340 |  Loss: (0.4718) | Acc: (83.94%) (36640/43648)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 350 |  Loss: (0.4721) | Acc: (83.91%) (37698/44928)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 360 |  Loss: (0.4725) | Acc: (83.89%) (38766/46208)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 370 |  Loss: (0.4726) | Acc: (83.91%) (39846/47488)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 380 |  Loss: (0.4719) | Acc: (83.93%) (40929/48768)\n",
      "#TRAIN: Epoch: 16 | Batch_idx: 390 |  Loss: (0.4726) | Acc: (83.89%) (41944/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5345) | Acc: (81.90%) (8190/10000)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 0 |  Loss: (0.4677) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 10 |  Loss: (0.4176) | Acc: (85.94%) (1210/1408)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 20 |  Loss: (0.4261) | Acc: (85.31%) (2293/2688)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 30 |  Loss: (0.4318) | Acc: (85.53%) (3394/3968)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 40 |  Loss: (0.4345) | Acc: (85.35%) (4479/5248)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 50 |  Loss: (0.4297) | Acc: (85.23%) (5564/6528)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 60 |  Loss: (0.4274) | Acc: (85.25%) (6656/7808)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 70 |  Loss: (0.4297) | Acc: (85.23%) (7746/9088)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 80 |  Loss: (0.4304) | Acc: (85.18%) (8831/10368)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 90 |  Loss: (0.4291) | Acc: (85.28%) (9933/11648)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 100 |  Loss: (0.4318) | Acc: (85.28%) (11025/12928)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 110 |  Loss: (0.4283) | Acc: (85.42%) (12136/14208)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 120 |  Loss: (0.4328) | Acc: (85.28%) (13208/15488)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 130 |  Loss: (0.4339) | Acc: (85.17%) (14281/16768)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 140 |  Loss: (0.4353) | Acc: (85.16%) (15370/18048)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 150 |  Loss: (0.4362) | Acc: (85.11%) (16450/19328)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 160 |  Loss: (0.4384) | Acc: (85.01%) (17519/20608)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 170 |  Loss: (0.4391) | Acc: (84.98%) (18601/21888)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 180 |  Loss: (0.4441) | Acc: (84.80%) (19646/23168)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 190 |  Loss: (0.4454) | Acc: (84.76%) (20722/24448)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 200 |  Loss: (0.4455) | Acc: (84.75%) (21805/25728)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 210 |  Loss: (0.4458) | Acc: (84.78%) (22897/27008)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 220 |  Loss: (0.4482) | Acc: (84.68%) (23953/28288)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 230 |  Loss: (0.4492) | Acc: (84.62%) (25019/29568)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 240 |  Loss: (0.4497) | Acc: (84.61%) (26100/30848)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 250 |  Loss: (0.4505) | Acc: (84.55%) (27164/32128)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 260 |  Loss: (0.4519) | Acc: (84.52%) (28237/33408)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 270 |  Loss: (0.4530) | Acc: (84.49%) (29307/34688)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 280 |  Loss: (0.4532) | Acc: (84.47%) (30382/35968)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 290 |  Loss: (0.4543) | Acc: (84.46%) (31459/37248)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 300 |  Loss: (0.4542) | Acc: (84.47%) (32543/38528)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 310 |  Loss: (0.4545) | Acc: (84.45%) (33618/39808)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 320 |  Loss: (0.4549) | Acc: (84.45%) (34700/41088)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 330 |  Loss: (0.4570) | Acc: (84.42%) (35767/42368)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 340 |  Loss: (0.4571) | Acc: (84.43%) (36850/43648)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 350 |  Loss: (0.4567) | Acc: (84.44%) (37937/44928)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 360 |  Loss: (0.4572) | Acc: (84.45%) (39022/46208)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 370 |  Loss: (0.4570) | Acc: (84.46%) (40107/47488)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 380 |  Loss: (0.4563) | Acc: (84.47%) (41196/48768)\n",
      "#TRAIN: Epoch: 17 | Batch_idx: 390 |  Loss: (0.4559) | Acc: (84.48%) (42242/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5579) | Acc: (81.98%) (8198/10000)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 0 |  Loss: (0.4497) | Acc: (86.72%) (111/128)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 10 |  Loss: (0.4457) | Acc: (85.23%) (1200/1408)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 20 |  Loss: (0.4306) | Acc: (85.45%) (2297/2688)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 30 |  Loss: (0.4294) | Acc: (85.58%) (3396/3968)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 40 |  Loss: (0.4324) | Acc: (85.29%) (4476/5248)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 50 |  Loss: (0.4315) | Acc: (85.19%) (5561/6528)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 60 |  Loss: (0.4337) | Acc: (85.22%) (6654/7808)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 70 |  Loss: (0.4347) | Acc: (85.08%) (7732/9088)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 80 |  Loss: (0.4377) | Acc: (84.94%) (8807/10368)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 90 |  Loss: (0.4416) | Acc: (84.94%) (9894/11648)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 100 |  Loss: (0.4409) | Acc: (84.97%) (10985/12928)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 110 |  Loss: (0.4385) | Acc: (85.06%) (12086/14208)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 120 |  Loss: (0.4356) | Acc: (85.26%) (13205/15488)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 130 |  Loss: (0.4389) | Acc: (85.17%) (14281/16768)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 140 |  Loss: (0.4388) | Acc: (85.15%) (15368/18048)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 150 |  Loss: (0.4378) | Acc: (85.14%) (16456/19328)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 160 |  Loss: (0.4385) | Acc: (85.19%) (17555/20608)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 170 |  Loss: (0.4402) | Acc: (85.13%) (18633/21888)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 180 |  Loss: (0.4376) | Acc: (85.23%) (19746/23168)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 190 |  Loss: (0.4362) | Acc: (85.24%) (20840/24448)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 200 |  Loss: (0.4392) | Acc: (85.17%) (21913/25728)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 210 |  Loss: (0.4383) | Acc: (85.18%) (23006/27008)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 220 |  Loss: (0.4391) | Acc: (85.18%) (24097/28288)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 230 |  Loss: (0.4388) | Acc: (85.22%) (25198/29568)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 240 |  Loss: (0.4394) | Acc: (85.22%) (26288/30848)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 250 |  Loss: (0.4394) | Acc: (85.24%) (27387/32128)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 260 |  Loss: (0.4389) | Acc: (85.27%) (28486/33408)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 270 |  Loss: (0.4389) | Acc: (85.21%) (29558/34688)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 280 |  Loss: (0.4395) | Acc: (85.24%) (30660/35968)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 290 |  Loss: (0.4411) | Acc: (85.14%) (31713/37248)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 300 |  Loss: (0.4417) | Acc: (85.10%) (32789/38528)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 310 |  Loss: (0.4422) | Acc: (85.08%) (33868/39808)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 320 |  Loss: (0.4428) | Acc: (85.07%) (34955/41088)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 330 |  Loss: (0.4434) | Acc: (85.03%) (36027/42368)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 340 |  Loss: (0.4437) | Acc: (85.01%) (37107/43648)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 350 |  Loss: (0.4436) | Acc: (85.00%) (38187/44928)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 360 |  Loss: (0.4439) | Acc: (85.01%) (39283/46208)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 370 |  Loss: (0.4438) | Acc: (85.04%) (40384/47488)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 380 |  Loss: (0.4442) | Acc: (85.01%) (41459/48768)\n",
      "#TRAIN: Epoch: 18 | Batch_idx: 390 |  Loss: (0.4435) | Acc: (85.04%) (42518/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5256) | Acc: (82.60%) (8260/10000)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 0 |  Loss: (0.4966) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 10 |  Loss: (0.4632) | Acc: (84.73%) (1193/1408)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 20 |  Loss: (0.4363) | Acc: (85.57%) (2300/2688)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 30 |  Loss: (0.4184) | Acc: (86.04%) (3414/3968)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 40 |  Loss: (0.4136) | Acc: (85.94%) (4510/5248)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 50 |  Loss: (0.4150) | Acc: (86.00%) (5614/6528)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 60 |  Loss: (0.4156) | Acc: (86.07%) (6720/7808)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 70 |  Loss: (0.4141) | Acc: (86.12%) (7827/9088)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 80 |  Loss: (0.4174) | Acc: (86.10%) (8927/10368)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 90 |  Loss: (0.4158) | Acc: (86.03%) (10021/11648)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 100 |  Loss: (0.4144) | Acc: (86.16%) (11139/12928)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 110 |  Loss: (0.4138) | Acc: (86.13%) (12237/14208)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 120 |  Loss: (0.4124) | Acc: (86.18%) (13347/15488)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 130 |  Loss: (0.4132) | Acc: (86.09%) (14436/16768)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 140 |  Loss: (0.4143) | Acc: (86.06%) (15533/18048)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 150 |  Loss: (0.4131) | Acc: (86.11%) (16644/19328)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 160 |  Loss: (0.4147) | Acc: (86.04%) (17731/20608)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 170 |  Loss: (0.4176) | Acc: (85.89%) (18799/21888)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 180 |  Loss: (0.4201) | Acc: (85.81%) (19880/23168)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 190 |  Loss: (0.4184) | Acc: (85.84%) (20986/24448)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 200 |  Loss: (0.4170) | Acc: (85.85%) (22088/25728)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 210 |  Loss: (0.4179) | Acc: (85.88%) (23194/27008)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 220 |  Loss: (0.4170) | Acc: (85.93%) (24307/28288)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 230 |  Loss: (0.4178) | Acc: (85.89%) (25397/29568)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 240 |  Loss: (0.4195) | Acc: (85.85%) (26482/30848)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 250 |  Loss: (0.4196) | Acc: (85.84%) (27579/32128)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 260 |  Loss: (0.4210) | Acc: (85.81%) (28668/33408)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 270 |  Loss: (0.4221) | Acc: (85.82%) (29768/34688)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 280 |  Loss: (0.4224) | Acc: (85.82%) (30866/35968)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 290 |  Loss: (0.4234) | Acc: (85.75%) (31942/37248)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 300 |  Loss: (0.4239) | Acc: (85.70%) (33017/38528)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 310 |  Loss: (0.4235) | Acc: (85.70%) (34114/39808)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 320 |  Loss: (0.4242) | Acc: (85.66%) (35194/41088)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 330 |  Loss: (0.4235) | Acc: (85.68%) (36301/42368)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 340 |  Loss: (0.4227) | Acc: (85.69%) (37403/43648)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 350 |  Loss: (0.4233) | Acc: (85.68%) (38494/44928)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 360 |  Loss: (0.4237) | Acc: (85.67%) (39585/46208)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 370 |  Loss: (0.4234) | Acc: (85.67%) (40685/47488)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 380 |  Loss: (0.4236) | Acc: (85.67%) (41779/48768)\n",
      "#TRAIN: Epoch: 19 | Batch_idx: 390 |  Loss: (0.4236) | Acc: (85.67%) (42834/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5094) | Acc: (83.22%) (8322/10000)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 0 |  Loss: (0.4329) | Acc: (82.81%) (106/128)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 10 |  Loss: (0.3867) | Acc: (86.58%) (1219/1408)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 20 |  Loss: (0.3853) | Acc: (86.57%) (2327/2688)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 30 |  Loss: (0.3950) | Acc: (86.24%) (3422/3968)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 40 |  Loss: (0.3910) | Acc: (86.59%) (4544/5248)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 50 |  Loss: (0.3903) | Acc: (86.55%) (5650/6528)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 60 |  Loss: (0.3948) | Acc: (86.41%) (6747/7808)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 70 |  Loss: (0.3990) | Acc: (86.23%) (7837/9088)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 80 |  Loss: (0.3960) | Acc: (86.28%) (8945/10368)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 90 |  Loss: (0.3949) | Acc: (86.32%) (10054/11648)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 100 |  Loss: (0.3982) | Acc: (86.16%) (11139/12928)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 110 |  Loss: (0.3982) | Acc: (86.14%) (12239/14208)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 120 |  Loss: (0.3992) | Acc: (86.14%) (13342/15488)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 130 |  Loss: (0.4000) | Acc: (86.10%) (14438/16768)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 140 |  Loss: (0.3996) | Acc: (86.08%) (15536/18048)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 150 |  Loss: (0.3987) | Acc: (86.20%) (16661/19328)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 160 |  Loss: (0.4014) | Acc: (86.13%) (17749/20608)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 170 |  Loss: (0.4016) | Acc: (86.14%) (18854/21888)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 180 |  Loss: (0.4007) | Acc: (86.16%) (19962/23168)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 190 |  Loss: (0.4023) | Acc: (86.15%) (21061/24448)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 200 |  Loss: (0.4031) | Acc: (86.14%) (22161/25728)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 210 |  Loss: (0.4020) | Acc: (86.23%) (23290/27008)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 220 |  Loss: (0.4022) | Acc: (86.23%) (24392/28288)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 230 |  Loss: (0.4036) | Acc: (86.21%) (25492/29568)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 240 |  Loss: (0.4050) | Acc: (86.20%) (26590/30848)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 250 |  Loss: (0.4045) | Acc: (86.22%) (27702/32128)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 260 |  Loss: (0.4050) | Acc: (86.22%) (28804/33408)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 270 |  Loss: (0.4043) | Acc: (86.23%) (29912/34688)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 280 |  Loss: (0.4047) | Acc: (86.23%) (31016/35968)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 290 |  Loss: (0.4059) | Acc: (86.19%) (32104/37248)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 300 |  Loss: (0.4062) | Acc: (86.18%) (33203/38528)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 310 |  Loss: (0.4087) | Acc: (86.10%) (34275/39808)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 320 |  Loss: (0.4101) | Acc: (86.03%) (35346/41088)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 330 |  Loss: (0.4099) | Acc: (86.05%) (36458/42368)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 340 |  Loss: (0.4112) | Acc: (86.00%) (37539/43648)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 350 |  Loss: (0.4102) | Acc: (86.05%) (38659/44928)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 360 |  Loss: (0.4101) | Acc: (86.09%) (39781/46208)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 370 |  Loss: (0.4100) | Acc: (86.07%) (40875/47488)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 380 |  Loss: (0.4094) | Acc: (86.09%) (41985/48768)\n",
      "#TRAIN: Epoch: 20 | Batch_idx: 390 |  Loss: (0.4098) | Acc: (86.06%) (43029/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4995) | Acc: (83.79%) (8379/10000)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 0 |  Loss: (0.3485) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 10 |  Loss: (0.3788) | Acc: (85.23%) (1200/1408)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 20 |  Loss: (0.3827) | Acc: (85.94%) (2310/2688)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 30 |  Loss: (0.3840) | Acc: (85.96%) (3411/3968)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 40 |  Loss: (0.3851) | Acc: (85.99%) (4513/5248)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 50 |  Loss: (0.3869) | Acc: (85.92%) (5609/6528)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 60 |  Loss: (0.3890) | Acc: (86.12%) (6724/7808)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 70 |  Loss: (0.3902) | Acc: (86.15%) (7829/9088)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 80 |  Loss: (0.3895) | Acc: (86.23%) (8940/10368)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 90 |  Loss: (0.3898) | Acc: (86.25%) (10046/11648)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 100 |  Loss: (0.3919) | Acc: (86.27%) (11153/12928)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 110 |  Loss: (0.3854) | Acc: (86.54%) (12295/14208)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 120 |  Loss: (0.3878) | Acc: (86.50%) (13397/15488)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 130 |  Loss: (0.3900) | Acc: (86.43%) (14493/16768)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 140 |  Loss: (0.3893) | Acc: (86.45%) (15603/18048)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 150 |  Loss: (0.3890) | Acc: (86.50%) (16718/19328)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 160 |  Loss: (0.3930) | Acc: (86.38%) (17801/20608)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 170 |  Loss: (0.3920) | Acc: (86.43%) (18917/21888)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 180 |  Loss: (0.3922) | Acc: (86.47%) (20034/23168)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 190 |  Loss: (0.3931) | Acc: (86.43%) (21130/24448)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 200 |  Loss: (0.3923) | Acc: (86.48%) (22250/25728)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 210 |  Loss: (0.3927) | Acc: (86.47%) (23354/27008)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 220 |  Loss: (0.3930) | Acc: (86.45%) (24455/28288)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 230 |  Loss: (0.3929) | Acc: (86.45%) (25561/29568)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 240 |  Loss: (0.3932) | Acc: (86.45%) (26669/30848)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 250 |  Loss: (0.3939) | Acc: (86.46%) (27778/32128)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 260 |  Loss: (0.3946) | Acc: (86.41%) (28867/33408)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 270 |  Loss: (0.3962) | Acc: (86.34%) (29948/34688)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 280 |  Loss: (0.3978) | Acc: (86.32%) (31046/35968)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 290 |  Loss: (0.3980) | Acc: (86.31%) (32148/37248)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 300 |  Loss: (0.3981) | Acc: (86.31%) (33255/38528)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 310 |  Loss: (0.3964) | Acc: (86.34%) (34370/39808)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 320 |  Loss: (0.3966) | Acc: (86.31%) (35464/41088)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 330 |  Loss: (0.3962) | Acc: (86.33%) (36576/42368)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 340 |  Loss: (0.3960) | Acc: (86.32%) (37679/43648)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 350 |  Loss: (0.3974) | Acc: (86.28%) (38763/44928)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 360 |  Loss: (0.3965) | Acc: (86.31%) (39882/46208)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 370 |  Loss: (0.3978) | Acc: (86.30%) (40983/47488)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 380 |  Loss: (0.3972) | Acc: (86.34%) (42106/48768)\n",
      "#TRAIN: Epoch: 21 | Batch_idx: 390 |  Loss: (0.3977) | Acc: (86.31%) (43157/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5803) | Acc: (81.71%) (8171/10000)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 0 |  Loss: (0.4413) | Acc: (82.81%) (106/128)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 10 |  Loss: (0.4235) | Acc: (85.16%) (1199/1408)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 20 |  Loss: (0.4123) | Acc: (85.57%) (2300/2688)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 30 |  Loss: (0.3972) | Acc: (86.21%) (3421/3968)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 40 |  Loss: (0.3860) | Acc: (86.64%) (4547/5248)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 50 |  Loss: (0.3893) | Acc: (86.55%) (5650/6528)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 60 |  Loss: (0.3866) | Acc: (86.76%) (6774/7808)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 70 |  Loss: (0.3856) | Acc: (86.94%) (7901/9088)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 80 |  Loss: (0.3865) | Acc: (86.98%) (9018/10368)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 90 |  Loss: (0.3878) | Acc: (86.90%) (10122/11648)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 100 |  Loss: (0.3845) | Acc: (86.93%) (11238/12928)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 110 |  Loss: (0.3846) | Acc: (87.00%) (12361/14208)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 120 |  Loss: (0.3872) | Acc: (86.95%) (13467/15488)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 130 |  Loss: (0.3886) | Acc: (86.88%) (14568/16768)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 140 |  Loss: (0.3885) | Acc: (86.90%) (15683/18048)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 150 |  Loss: (0.3886) | Acc: (86.98%) (16811/19328)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 160 |  Loss: (0.3881) | Acc: (86.93%) (17915/20608)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 170 |  Loss: (0.3874) | Acc: (86.95%) (19032/21888)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 180 |  Loss: (0.3895) | Acc: (86.84%) (20120/23168)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 190 |  Loss: (0.3871) | Acc: (86.90%) (21246/24448)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 200 |  Loss: (0.3881) | Acc: (86.83%) (22340/25728)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 210 |  Loss: (0.3879) | Acc: (86.81%) (23445/27008)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 220 |  Loss: (0.3876) | Acc: (86.87%) (24574/28288)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 230 |  Loss: (0.3877) | Acc: (86.84%) (25676/29568)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 240 |  Loss: (0.3880) | Acc: (86.82%) (26783/30848)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 250 |  Loss: (0.3887) | Acc: (86.80%) (27886/32128)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 260 |  Loss: (0.3898) | Acc: (86.75%) (28981/33408)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 270 |  Loss: (0.3883) | Acc: (86.76%) (30097/34688)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 280 |  Loss: (0.3892) | Acc: (86.76%) (31205/35968)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 290 |  Loss: (0.3879) | Acc: (86.81%) (32335/37248)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 300 |  Loss: (0.3886) | Acc: (86.78%) (33436/38528)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 310 |  Loss: (0.3883) | Acc: (86.79%) (34548/39808)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 320 |  Loss: (0.3887) | Acc: (86.75%) (35643/41088)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 330 |  Loss: (0.3885) | Acc: (86.76%) (36758/42368)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 340 |  Loss: (0.3888) | Acc: (86.74%) (37861/43648)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 350 |  Loss: (0.3888) | Acc: (86.73%) (38966/44928)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 360 |  Loss: (0.3894) | Acc: (86.73%) (40076/46208)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 370 |  Loss: (0.3887) | Acc: (86.75%) (41195/47488)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 380 |  Loss: (0.3890) | Acc: (86.77%) (42316/48768)\n",
      "#TRAIN: Epoch: 22 | Batch_idx: 390 |  Loss: (0.3892) | Acc: (86.74%) (43370/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5345) | Acc: (82.68%) (8268/10000)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 0 |  Loss: (0.3330) | Acc: (88.28%) (113/128)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 10 |  Loss: (0.3364) | Acc: (88.21%) (1242/1408)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 20 |  Loss: (0.3360) | Acc: (88.58%) (2381/2688)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 30 |  Loss: (0.3424) | Acc: (88.10%) (3496/3968)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 40 |  Loss: (0.3445) | Acc: (88.00%) (4618/5248)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 50 |  Loss: (0.3446) | Acc: (88.04%) (5747/6528)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 60 |  Loss: (0.3557) | Acc: (87.67%) (6845/7808)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 70 |  Loss: (0.3577) | Acc: (87.68%) (7968/9088)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 80 |  Loss: (0.3600) | Acc: (87.47%) (9069/10368)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 90 |  Loss: (0.3602) | Acc: (87.49%) (10191/11648)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 100 |  Loss: (0.3606) | Acc: (87.40%) (11299/12928)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 110 |  Loss: (0.3604) | Acc: (87.44%) (12423/14208)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 120 |  Loss: (0.3602) | Acc: (87.44%) (13542/15488)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 130 |  Loss: (0.3642) | Acc: (87.40%) (14655/16768)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 140 |  Loss: (0.3659) | Acc: (87.36%) (15767/18048)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 150 |  Loss: (0.3673) | Acc: (87.33%) (16879/19328)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 160 |  Loss: (0.3657) | Acc: (87.45%) (18021/20608)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 170 |  Loss: (0.3689) | Acc: (87.34%) (19117/21888)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 180 |  Loss: (0.3709) | Acc: (87.27%) (20218/23168)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 190 |  Loss: (0.3702) | Acc: (87.26%) (21333/24448)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 200 |  Loss: (0.3712) | Acc: (87.24%) (22446/25728)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 210 |  Loss: (0.3717) | Acc: (87.21%) (23553/27008)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 220 |  Loss: (0.3731) | Acc: (87.25%) (24681/28288)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 230 |  Loss: (0.3739) | Acc: (87.27%) (25803/29568)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 240 |  Loss: (0.3734) | Acc: (87.25%) (26915/30848)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 250 |  Loss: (0.3729) | Acc: (87.25%) (28031/32128)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 260 |  Loss: (0.3726) | Acc: (87.26%) (29153/33408)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 270 |  Loss: (0.3727) | Acc: (87.29%) (30280/34688)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 280 |  Loss: (0.3733) | Acc: (87.26%) (31386/35968)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 290 |  Loss: (0.3724) | Acc: (87.29%) (32512/37248)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 300 |  Loss: (0.3733) | Acc: (87.27%) (33622/38528)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 310 |  Loss: (0.3734) | Acc: (87.28%) (34743/39808)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 320 |  Loss: (0.3731) | Acc: (87.28%) (35860/41088)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 330 |  Loss: (0.3734) | Acc: (87.28%) (36978/42368)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 340 |  Loss: (0.3721) | Acc: (87.33%) (38118/43648)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 350 |  Loss: (0.3733) | Acc: (87.31%) (39226/44928)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 360 |  Loss: (0.3732) | Acc: (87.31%) (40346/46208)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 370 |  Loss: (0.3717) | Acc: (87.37%) (41488/47488)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 380 |  Loss: (0.3714) | Acc: (87.39%) (42617/48768)\n",
      "#TRAIN: Epoch: 23 | Batch_idx: 390 |  Loss: (0.3716) | Acc: (87.38%) (43689/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4968) | Acc: (84.06%) (8406/10000)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 0 |  Loss: (0.3723) | Acc: (86.72%) (111/128)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 10 |  Loss: (0.3578) | Acc: (88.35%) (1244/1408)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 20 |  Loss: (0.3521) | Acc: (88.39%) (2376/2688)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 30 |  Loss: (0.3496) | Acc: (88.56%) (3514/3968)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 40 |  Loss: (0.3484) | Acc: (88.38%) (4638/5248)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 50 |  Loss: (0.3559) | Acc: (88.19%) (5757/6528)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 60 |  Loss: (0.3537) | Acc: (88.24%) (6890/7808)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 70 |  Loss: (0.3558) | Acc: (87.98%) (7996/9088)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 80 |  Loss: (0.3518) | Acc: (88.20%) (9145/10368)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 90 |  Loss: (0.3542) | Acc: (88.02%) (10252/11648)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 100 |  Loss: (0.3565) | Acc: (87.97%) (11373/12928)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 110 |  Loss: (0.3549) | Acc: (88.01%) (12505/14208)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 120 |  Loss: (0.3545) | Acc: (88.10%) (13645/15488)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 130 |  Loss: (0.3548) | Acc: (88.08%) (14769/16768)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 140 |  Loss: (0.3558) | Acc: (88.05%) (15892/18048)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 150 |  Loss: (0.3584) | Acc: (87.96%) (17000/19328)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 160 |  Loss: (0.3607) | Acc: (87.84%) (18103/20608)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 170 |  Loss: (0.3578) | Acc: (87.92%) (19245/21888)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 180 |  Loss: (0.3613) | Acc: (87.80%) (20341/23168)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 190 |  Loss: (0.3618) | Acc: (87.79%) (21463/24448)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 200 |  Loss: (0.3623) | Acc: (87.73%) (22571/25728)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 210 |  Loss: (0.3631) | Acc: (87.74%) (23697/27008)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 220 |  Loss: (0.3629) | Acc: (87.76%) (24826/28288)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 230 |  Loss: (0.3645) | Acc: (87.66%) (25920/29568)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 240 |  Loss: (0.3636) | Acc: (87.69%) (27052/30848)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 250 |  Loss: (0.3638) | Acc: (87.66%) (28165/32128)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 260 |  Loss: (0.3629) | Acc: (87.71%) (29302/33408)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 270 |  Loss: (0.3642) | Acc: (87.66%) (30407/34688)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 280 |  Loss: (0.3648) | Acc: (87.63%) (31519/35968)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 290 |  Loss: (0.3645) | Acc: (87.63%) (32640/37248)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 300 |  Loss: (0.3641) | Acc: (87.65%) (33768/38528)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 310 |  Loss: (0.3638) | Acc: (87.66%) (34895/39808)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 320 |  Loss: (0.3635) | Acc: (87.68%) (36024/41088)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 330 |  Loss: (0.3641) | Acc: (87.66%) (37141/42368)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 340 |  Loss: (0.3640) | Acc: (87.66%) (38263/43648)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 350 |  Loss: (0.3648) | Acc: (87.64%) (39377/44928)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 360 |  Loss: (0.3661) | Acc: (87.63%) (40493/46208)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 370 |  Loss: (0.3664) | Acc: (87.63%) (41612/47488)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 380 |  Loss: (0.3655) | Acc: (87.66%) (42750/48768)\n",
      "#TRAIN: Epoch: 24 | Batch_idx: 390 |  Loss: (0.3654) | Acc: (87.65%) (43825/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4733) | Acc: (84.27%) (8427/10000)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 0 |  Loss: (0.4096) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 10 |  Loss: (0.3580) | Acc: (87.93%) (1238/1408)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 20 |  Loss: (0.3727) | Acc: (87.61%) (2355/2688)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 30 |  Loss: (0.3717) | Acc: (87.47%) (3471/3968)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 40 |  Loss: (0.3582) | Acc: (88.00%) (4618/5248)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 50 |  Loss: (0.3548) | Acc: (88.16%) (5755/6528)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 60 |  Loss: (0.3519) | Acc: (88.26%) (6891/7808)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 70 |  Loss: (0.3589) | Acc: (88.01%) (7998/9088)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 80 |  Loss: (0.3567) | Acc: (88.03%) (9127/10368)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 90 |  Loss: (0.3542) | Acc: (88.02%) (10253/11648)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 100 |  Loss: (0.3561) | Acc: (88.03%) (11381/12928)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 110 |  Loss: (0.3569) | Acc: (88.02%) (12506/14208)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 120 |  Loss: (0.3585) | Acc: (87.98%) (13626/15488)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 130 |  Loss: (0.3591) | Acc: (87.95%) (14747/16768)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 140 |  Loss: (0.3578) | Acc: (88.00%) (15882/18048)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 150 |  Loss: (0.3547) | Acc: (88.09%) (17027/19328)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 160 |  Loss: (0.3536) | Acc: (88.13%) (18161/20608)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 170 |  Loss: (0.3538) | Acc: (88.12%) (19288/21888)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 180 |  Loss: (0.3518) | Acc: (88.20%) (20434/23168)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 190 |  Loss: (0.3535) | Acc: (88.23%) (21570/24448)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 200 |  Loss: (0.3546) | Acc: (88.20%) (22693/25728)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 210 |  Loss: (0.3541) | Acc: (88.22%) (23826/27008)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 220 |  Loss: (0.3572) | Acc: (88.10%) (24922/28288)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 230 |  Loss: (0.3560) | Acc: (88.13%) (26057/29568)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 240 |  Loss: (0.3568) | Acc: (88.12%) (27182/30848)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 250 |  Loss: (0.3557) | Acc: (88.14%) (28319/32128)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 260 |  Loss: (0.3552) | Acc: (88.15%) (29450/33408)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 270 |  Loss: (0.3542) | Acc: (88.16%) (30581/34688)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 280 |  Loss: (0.3528) | Acc: (88.19%) (31720/35968)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 290 |  Loss: (0.3522) | Acc: (88.20%) (32851/37248)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 300 |  Loss: (0.3534) | Acc: (88.13%) (33956/38528)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 310 |  Loss: (0.3532) | Acc: (88.16%) (35095/39808)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 320 |  Loss: (0.3531) | Acc: (88.16%) (36224/41088)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 330 |  Loss: (0.3539) | Acc: (88.12%) (37333/42368)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 340 |  Loss: (0.3550) | Acc: (88.05%) (38433/43648)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 350 |  Loss: (0.3547) | Acc: (88.05%) (39559/44928)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 360 |  Loss: (0.3537) | Acc: (88.08%) (40701/46208)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 370 |  Loss: (0.3545) | Acc: (88.04%) (41809/47488)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 380 |  Loss: (0.3543) | Acc: (88.02%) (42924/48768)\n",
      "#TRAIN: Epoch: 25 | Batch_idx: 390 |  Loss: (0.3540) | Acc: (88.01%) (44005/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4772) | Acc: (84.67%) (8467/10000)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 0 |  Loss: (0.4354) | Acc: (85.16%) (109/128)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 10 |  Loss: (0.3085) | Acc: (89.84%) (1265/1408)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 20 |  Loss: (0.3273) | Acc: (88.99%) (2392/2688)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 30 |  Loss: (0.3226) | Acc: (89.11%) (3536/3968)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 40 |  Loss: (0.3191) | Acc: (89.23%) (4683/5248)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 50 |  Loss: (0.3229) | Acc: (89.29%) (5829/6528)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 60 |  Loss: (0.3239) | Acc: (89.16%) (6962/7808)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 70 |  Loss: (0.3198) | Acc: (89.35%) (8120/9088)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 80 |  Loss: (0.3218) | Acc: (89.36%) (9265/10368)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 90 |  Loss: (0.3253) | Acc: (89.23%) (10393/11648)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 100 |  Loss: (0.3250) | Acc: (89.19%) (11531/12928)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 110 |  Loss: (0.3270) | Acc: (89.07%) (12655/14208)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 120 |  Loss: (0.3280) | Acc: (88.98%) (13781/15488)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 130 |  Loss: (0.3276) | Acc: (89.01%) (14925/16768)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 140 |  Loss: (0.3298) | Acc: (88.96%) (16056/18048)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 150 |  Loss: (0.3319) | Acc: (88.94%) (17190/19328)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 160 |  Loss: (0.3308) | Acc: (88.98%) (18336/20608)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 170 |  Loss: (0.3343) | Acc: (88.82%) (19441/21888)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 180 |  Loss: (0.3373) | Acc: (88.74%) (20559/23168)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 190 |  Loss: (0.3391) | Acc: (88.67%) (21677/24448)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 200 |  Loss: (0.3389) | Acc: (88.67%) (22814/25728)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 210 |  Loss: (0.3385) | Acc: (88.66%) (23945/27008)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 220 |  Loss: (0.3376) | Acc: (88.70%) (25092/28288)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 230 |  Loss: (0.3367) | Acc: (88.73%) (26235/29568)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 240 |  Loss: (0.3367) | Acc: (88.71%) (27365/30848)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 250 |  Loss: (0.3348) | Acc: (88.78%) (28523/32128)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 260 |  Loss: (0.3375) | Acc: (88.67%) (29622/33408)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 270 |  Loss: (0.3383) | Acc: (88.64%) (30746/34688)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 280 |  Loss: (0.3391) | Acc: (88.63%) (31879/35968)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 290 |  Loss: (0.3397) | Acc: (88.61%) (33007/37248)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 300 |  Loss: (0.3401) | Acc: (88.61%) (34141/38528)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 310 |  Loss: (0.3406) | Acc: (88.60%) (35269/39808)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 320 |  Loss: (0.3412) | Acc: (88.60%) (36405/41088)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 330 |  Loss: (0.3415) | Acc: (88.60%) (37537/42368)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 340 |  Loss: (0.3411) | Acc: (88.60%) (38672/43648)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 350 |  Loss: (0.3418) | Acc: (88.56%) (39788/44928)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 360 |  Loss: (0.3423) | Acc: (88.54%) (40912/46208)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 370 |  Loss: (0.3437) | Acc: (88.49%) (42020/47488)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 380 |  Loss: (0.3435) | Acc: (88.51%) (43164/48768)\n",
      "#TRAIN: Epoch: 26 | Batch_idx: 390 |  Loss: (0.3432) | Acc: (88.52%) (44261/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4981) | Acc: (83.87%) (8387/10000)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 0 |  Loss: (0.3118) | Acc: (87.50%) (112/128)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 10 |  Loss: (0.3117) | Acc: (89.13%) (1255/1408)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 20 |  Loss: (0.3372) | Acc: (89.14%) (2396/2688)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 30 |  Loss: (0.3369) | Acc: (88.94%) (3529/3968)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 40 |  Loss: (0.3371) | Acc: (88.85%) (4663/5248)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 50 |  Loss: (0.3379) | Acc: (88.77%) (5795/6528)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 60 |  Loss: (0.3353) | Acc: (89.04%) (6952/7808)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 70 |  Loss: (0.3320) | Acc: (89.19%) (8106/9088)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 80 |  Loss: (0.3293) | Acc: (89.06%) (9234/10368)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 90 |  Loss: (0.3303) | Acc: (89.06%) (10374/11648)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 100 |  Loss: (0.3290) | Acc: (89.04%) (11511/12928)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 110 |  Loss: (0.3309) | Acc: (88.99%) (12643/14208)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 120 |  Loss: (0.3323) | Acc: (88.91%) (13771/15488)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 130 |  Loss: (0.3319) | Acc: (88.87%) (14902/16768)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 140 |  Loss: (0.3305) | Acc: (88.92%) (16048/18048)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 150 |  Loss: (0.3289) | Acc: (88.99%) (17200/19328)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 160 |  Loss: (0.3286) | Acc: (88.97%) (18335/20608)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 170 |  Loss: (0.3265) | Acc: (89.03%) (19486/21888)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 180 |  Loss: (0.3261) | Acc: (89.05%) (20631/23168)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 190 |  Loss: (0.3244) | Acc: (89.11%) (21785/24448)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 200 |  Loss: (0.3253) | Acc: (89.10%) (22923/25728)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 210 |  Loss: (0.3234) | Acc: (89.14%) (24076/27008)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 220 |  Loss: (0.3261) | Acc: (89.08%) (25199/28288)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 230 |  Loss: (0.3266) | Acc: (89.02%) (26321/29568)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 240 |  Loss: (0.3271) | Acc: (89.04%) (27466/30848)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 250 |  Loss: (0.3289) | Acc: (88.99%) (28592/32128)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 260 |  Loss: (0.3291) | Acc: (88.99%) (29730/33408)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 270 |  Loss: (0.3280) | Acc: (89.04%) (30885/34688)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 280 |  Loss: (0.3294) | Acc: (88.97%) (31999/35968)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 290 |  Loss: (0.3296) | Acc: (88.97%) (33138/37248)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 300 |  Loss: (0.3288) | Acc: (88.96%) (34275/38528)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 310 |  Loss: (0.3292) | Acc: (88.92%) (35399/39808)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 320 |  Loss: (0.3286) | Acc: (88.94%) (36545/41088)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 330 |  Loss: (0.3294) | Acc: (88.91%) (37671/42368)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 340 |  Loss: (0.3291) | Acc: (88.93%) (38815/43648)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 350 |  Loss: (0.3300) | Acc: (88.91%) (39947/44928)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 360 |  Loss: (0.3314) | Acc: (88.84%) (41051/46208)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 370 |  Loss: (0.3321) | Acc: (88.84%) (42188/47488)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 380 |  Loss: (0.3317) | Acc: (88.82%) (43315/48768)\n",
      "#TRAIN: Epoch: 27 | Batch_idx: 390 |  Loss: (0.3327) | Acc: (88.77%) (44386/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5004) | Acc: (84.25%) (8425/10000)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 0 |  Loss: (0.2909) | Acc: (89.06%) (114/128)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 10 |  Loss: (0.2603) | Acc: (91.69%) (1291/1408)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 20 |  Loss: (0.2892) | Acc: (90.22%) (2425/2688)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 30 |  Loss: (0.2917) | Acc: (90.20%) (3579/3968)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 40 |  Loss: (0.2899) | Acc: (90.15%) (4731/5248)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 50 |  Loss: (0.2958) | Acc: (90.01%) (5876/6528)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 60 |  Loss: (0.3044) | Acc: (89.70%) (7004/7808)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 70 |  Loss: (0.3064) | Acc: (89.74%) (8156/9088)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 80 |  Loss: (0.3097) | Acc: (89.59%) (9289/10368)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 90 |  Loss: (0.3107) | Acc: (89.56%) (10432/11648)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 100 |  Loss: (0.3115) | Acc: (89.44%) (11563/12928)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 110 |  Loss: (0.3154) | Acc: (89.23%) (12678/14208)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 120 |  Loss: (0.3126) | Acc: (89.32%) (13834/15488)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 130 |  Loss: (0.3134) | Acc: (89.35%) (14983/16768)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 140 |  Loss: (0.3131) | Acc: (89.35%) (16125/18048)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 150 |  Loss: (0.3141) | Acc: (89.33%) (17265/19328)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 160 |  Loss: (0.3147) | Acc: (89.28%) (18399/20608)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 170 |  Loss: (0.3142) | Acc: (89.31%) (19549/21888)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 180 |  Loss: (0.3147) | Acc: (89.29%) (20686/23168)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 190 |  Loss: (0.3155) | Acc: (89.27%) (21825/24448)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 200 |  Loss: (0.3155) | Acc: (89.28%) (22970/25728)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 210 |  Loss: (0.3177) | Acc: (89.21%) (24093/27008)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 220 |  Loss: (0.3178) | Acc: (89.20%) (25234/28288)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 230 |  Loss: (0.3182) | Acc: (89.16%) (26362/29568)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 240 |  Loss: (0.3182) | Acc: (89.16%) (27505/30848)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 250 |  Loss: (0.3174) | Acc: (89.21%) (28661/32128)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 260 |  Loss: (0.3189) | Acc: (89.18%) (29792/33408)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 270 |  Loss: (0.3199) | Acc: (89.15%) (30923/34688)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 280 |  Loss: (0.3193) | Acc: (89.19%) (32079/35968)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 290 |  Loss: (0.3197) | Acc: (89.18%) (33218/37248)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 300 |  Loss: (0.3207) | Acc: (89.16%) (34350/38528)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 310 |  Loss: (0.3206) | Acc: (89.16%) (35493/39808)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 320 |  Loss: (0.3210) | Acc: (89.15%) (36629/41088)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 330 |  Loss: (0.3209) | Acc: (89.15%) (37770/42368)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 340 |  Loss: (0.3205) | Acc: (89.14%) (38908/43648)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 350 |  Loss: (0.3207) | Acc: (89.14%) (40050/44928)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 360 |  Loss: (0.3213) | Acc: (89.13%) (41185/46208)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 370 |  Loss: (0.3209) | Acc: (89.12%) (42322/47488)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 380 |  Loss: (0.3200) | Acc: (89.15%) (43477/48768)\n",
      "#TRAIN: Epoch: 28 | Batch_idx: 390 |  Loss: (0.3202) | Acc: (89.14%) (44568/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4925) | Acc: (85.00%) (8500/10000)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 0 |  Loss: (0.4232) | Acc: (84.38%) (108/128)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 10 |  Loss: (0.3344) | Acc: (87.57%) (1233/1408)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 20 |  Loss: (0.3333) | Acc: (88.06%) (2367/2688)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 30 |  Loss: (0.3322) | Acc: (88.26%) (3502/3968)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 40 |  Loss: (0.3153) | Acc: (88.95%) (4668/5248)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 50 |  Loss: (0.3148) | Acc: (89.00%) (5810/6528)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 60 |  Loss: (0.3090) | Acc: (89.19%) (6964/7808)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 70 |  Loss: (0.3078) | Acc: (89.34%) (8119/9088)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 80 |  Loss: (0.3059) | Acc: (89.41%) (9270/10368)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 90 |  Loss: (0.3047) | Acc: (89.46%) (10420/11648)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 100 |  Loss: (0.3043) | Acc: (89.47%) (11567/12928)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 110 |  Loss: (0.3016) | Acc: (89.66%) (12739/14208)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 120 |  Loss: (0.3022) | Acc: (89.65%) (13885/15488)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 130 |  Loss: (0.3030) | Acc: (89.55%) (15016/16768)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 140 |  Loss: (0.3064) | Acc: (89.50%) (16153/18048)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 150 |  Loss: (0.3066) | Acc: (89.49%) (17297/19328)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 160 |  Loss: (0.3058) | Acc: (89.50%) (18444/20608)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 170 |  Loss: (0.3073) | Acc: (89.42%) (19572/21888)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 180 |  Loss: (0.3064) | Acc: (89.51%) (20737/23168)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 190 |  Loss: (0.3063) | Acc: (89.52%) (21886/24448)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 200 |  Loss: (0.3069) | Acc: (89.50%) (23026/25728)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 210 |  Loss: (0.3060) | Acc: (89.51%) (24175/27008)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 220 |  Loss: (0.3065) | Acc: (89.45%) (25304/28288)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 230 |  Loss: (0.3051) | Acc: (89.49%) (26461/29568)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 240 |  Loss: (0.3055) | Acc: (89.50%) (27608/30848)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 250 |  Loss: (0.3070) | Acc: (89.46%) (28743/32128)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 260 |  Loss: (0.3076) | Acc: (89.45%) (29885/33408)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 270 |  Loss: (0.3085) | Acc: (89.41%) (31014/34688)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 280 |  Loss: (0.3079) | Acc: (89.46%) (32176/35968)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 290 |  Loss: (0.3089) | Acc: (89.41%) (33305/37248)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 300 |  Loss: (0.3085) | Acc: (89.42%) (34453/38528)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 310 |  Loss: (0.3085) | Acc: (89.41%) (35594/39808)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 320 |  Loss: (0.3089) | Acc: (89.42%) (36740/41088)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 330 |  Loss: (0.3095) | Acc: (89.40%) (37876/42368)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 340 |  Loss: (0.3089) | Acc: (89.44%) (39037/43648)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 350 |  Loss: (0.3085) | Acc: (89.44%) (40184/44928)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 360 |  Loss: (0.3092) | Acc: (89.44%) (41327/46208)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 370 |  Loss: (0.3094) | Acc: (89.44%) (42474/47488)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 380 |  Loss: (0.3097) | Acc: (89.43%) (43611/48768)\n",
      "#TRAIN: Epoch: 29 | Batch_idx: 390 |  Loss: (0.3089) | Acc: (89.46%) (44732/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4946) | Acc: (84.56%) (8456/10000)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 0 |  Loss: (0.3623) | Acc: (85.94%) (110/128)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 10 |  Loss: (0.2999) | Acc: (89.84%) (1265/1408)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 20 |  Loss: (0.2859) | Acc: (90.40%) (2430/2688)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 30 |  Loss: (0.2911) | Acc: (90.07%) (3574/3968)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 40 |  Loss: (0.2877) | Acc: (90.28%) (4738/5248)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 50 |  Loss: (0.2806) | Acc: (90.61%) (5915/6528)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 60 |  Loss: (0.2820) | Acc: (90.55%) (7070/7808)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 70 |  Loss: (0.2841) | Acc: (90.43%) (8218/9088)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 80 |  Loss: (0.2884) | Acc: (90.30%) (9362/10368)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 90 |  Loss: (0.2898) | Acc: (90.31%) (10519/11648)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 100 |  Loss: (0.2887) | Acc: (90.33%) (11678/12928)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 110 |  Loss: (0.2905) | Acc: (90.22%) (12819/14208)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 120 |  Loss: (0.2936) | Acc: (90.08%) (13951/15488)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 130 |  Loss: (0.2948) | Acc: (90.06%) (15101/16768)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 140 |  Loss: (0.2972) | Acc: (89.95%) (16234/18048)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 150 |  Loss: (0.2990) | Acc: (89.91%) (17377/19328)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 160 |  Loss: (0.3006) | Acc: (89.85%) (18516/20608)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 170 |  Loss: (0.3027) | Acc: (89.84%) (19665/21888)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 180 |  Loss: (0.3012) | Acc: (89.90%) (20828/23168)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 190 |  Loss: (0.3019) | Acc: (89.86%) (21969/24448)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 200 |  Loss: (0.3013) | Acc: (89.88%) (23124/25728)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 210 |  Loss: (0.3032) | Acc: (89.81%) (24255/27008)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 220 |  Loss: (0.3032) | Acc: (89.79%) (25401/28288)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 230 |  Loss: (0.3027) | Acc: (89.76%) (26539/29568)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 240 |  Loss: (0.3016) | Acc: (89.80%) (27703/30848)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 250 |  Loss: (0.3015) | Acc: (89.81%) (28854/32128)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 260 |  Loss: (0.3027) | Acc: (89.77%) (29989/33408)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 270 |  Loss: (0.3026) | Acc: (89.76%) (31135/34688)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 280 |  Loss: (0.3021) | Acc: (89.78%) (32293/35968)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 290 |  Loss: (0.3014) | Acc: (89.78%) (33443/37248)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 300 |  Loss: (0.3020) | Acc: (89.77%) (34588/38528)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 310 |  Loss: (0.3016) | Acc: (89.79%) (35744/39808)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 320 |  Loss: (0.3015) | Acc: (89.80%) (36896/41088)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 330 |  Loss: (0.3012) | Acc: (89.80%) (38045/42368)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 340 |  Loss: (0.3026) | Acc: (89.76%) (39180/43648)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 350 |  Loss: (0.3031) | Acc: (89.75%) (40321/44928)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 360 |  Loss: (0.3030) | Acc: (89.74%) (41467/46208)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 370 |  Loss: (0.3040) | Acc: (89.70%) (42595/47488)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 380 |  Loss: (0.3041) | Acc: (89.68%) (43735/48768)\n",
      "#TRAIN: Epoch: 30 | Batch_idx: 390 |  Loss: (0.3040) | Acc: (89.69%) (44845/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5011) | Acc: (84.47%) (8447/10000)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 0 |  Loss: (0.2960) | Acc: (89.84%) (115/128)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 10 |  Loss: (0.3255) | Acc: (89.13%) (1255/1408)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 20 |  Loss: (0.3130) | Acc: (89.36%) (2402/2688)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 30 |  Loss: (0.2955) | Acc: (90.10%) (3575/3968)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 40 |  Loss: (0.2956) | Acc: (90.03%) (4725/5248)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 50 |  Loss: (0.2925) | Acc: (90.10%) (5882/6528)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 60 |  Loss: (0.2969) | Acc: (89.96%) (7024/7808)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 70 |  Loss: (0.2895) | Acc: (90.28%) (8205/9088)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 80 |  Loss: (0.2866) | Acc: (90.38%) (9371/10368)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 90 |  Loss: (0.2863) | Acc: (90.32%) (10520/11648)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 100 |  Loss: (0.2870) | Acc: (90.30%) (11674/12928)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 110 |  Loss: (0.2871) | Acc: (90.30%) (12830/14208)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 120 |  Loss: (0.2893) | Acc: (90.26%) (13979/15488)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 130 |  Loss: (0.2930) | Acc: (90.15%) (15117/16768)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 140 |  Loss: (0.2945) | Acc: (90.09%) (16260/18048)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 150 |  Loss: (0.2959) | Acc: (90.05%) (17404/19328)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 160 |  Loss: (0.2938) | Acc: (90.12%) (18571/20608)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 170 |  Loss: (0.2942) | Acc: (90.15%) (19732/21888)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 180 |  Loss: (0.2928) | Acc: (90.18%) (20892/23168)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 190 |  Loss: (0.2933) | Acc: (90.17%) (22044/24448)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 200 |  Loss: (0.2949) | Acc: (90.13%) (23189/25728)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 210 |  Loss: (0.2949) | Acc: (90.09%) (24331/27008)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 220 |  Loss: (0.2953) | Acc: (90.07%) (25479/28288)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 230 |  Loss: (0.2940) | Acc: (90.10%) (26642/29568)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 240 |  Loss: (0.2943) | Acc: (90.12%) (27800/30848)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 250 |  Loss: (0.2946) | Acc: (90.13%) (28958/32128)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 260 |  Loss: (0.2945) | Acc: (90.15%) (30116/33408)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 270 |  Loss: (0.2959) | Acc: (90.07%) (31242/34688)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 280 |  Loss: (0.2964) | Acc: (90.07%) (32397/35968)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 290 |  Loss: (0.2959) | Acc: (90.08%) (33554/37248)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 300 |  Loss: (0.2967) | Acc: (90.03%) (34686/38528)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 310 |  Loss: (0.2967) | Acc: (90.05%) (35849/39808)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 320 |  Loss: (0.2968) | Acc: (90.07%) (37006/41088)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 330 |  Loss: (0.2962) | Acc: (90.06%) (38157/42368)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 340 |  Loss: (0.2975) | Acc: (90.02%) (39290/43648)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 350 |  Loss: (0.2981) | Acc: (90.00%) (40433/44928)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 360 |  Loss: (0.2986) | Acc: (89.97%) (41575/46208)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 370 |  Loss: (0.2986) | Acc: (89.99%) (42735/47488)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 380 |  Loss: (0.2986) | Acc: (89.99%) (43888/48768)\n",
      "#TRAIN: Epoch: 31 | Batch_idx: 390 |  Loss: (0.2987) | Acc: (89.98%) (44991/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5165) | Acc: (83.98%) (8398/10000)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 0 |  Loss: (0.2098) | Acc: (92.19%) (118/128)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 10 |  Loss: (0.2469) | Acc: (92.05%) (1296/1408)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 20 |  Loss: (0.2528) | Acc: (91.37%) (2456/2688)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 30 |  Loss: (0.2568) | Acc: (91.26%) (3621/3968)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 40 |  Loss: (0.2608) | Acc: (91.27%) (4790/5248)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 50 |  Loss: (0.2607) | Acc: (91.33%) (5962/6528)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 60 |  Loss: (0.2592) | Acc: (91.37%) (7134/7808)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 70 |  Loss: (0.2617) | Acc: (91.19%) (8287/9088)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 80 |  Loss: (0.2672) | Acc: (90.95%) (9430/10368)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 90 |  Loss: (0.2672) | Acc: (91.05%) (10606/11648)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 100 |  Loss: (0.2671) | Acc: (91.01%) (11766/12928)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 110 |  Loss: (0.2670) | Acc: (90.98%) (12926/14208)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 120 |  Loss: (0.2643) | Acc: (91.08%) (14106/15488)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 130 |  Loss: (0.2647) | Acc: (91.03%) (15264/16768)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 140 |  Loss: (0.2659) | Acc: (90.99%) (16421/18048)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 150 |  Loss: (0.2688) | Acc: (90.86%) (17561/19328)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 160 |  Loss: (0.2707) | Acc: (90.79%) (18711/20608)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 170 |  Loss: (0.2689) | Acc: (90.86%) (19888/21888)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 180 |  Loss: (0.2693) | Acc: (90.84%) (21045/23168)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 190 |  Loss: (0.2696) | Acc: (90.85%) (22211/24448)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 200 |  Loss: (0.2721) | Acc: (90.75%) (23347/25728)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 210 |  Loss: (0.2726) | Acc: (90.75%) (24510/27008)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 220 |  Loss: (0.2732) | Acc: (90.74%) (25668/28288)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 230 |  Loss: (0.2726) | Acc: (90.76%) (26835/29568)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 240 |  Loss: (0.2739) | Acc: (90.73%) (27988/30848)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 250 |  Loss: (0.2756) | Acc: (90.70%) (29139/32128)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 260 |  Loss: (0.2771) | Acc: (90.62%) (30276/33408)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 270 |  Loss: (0.2771) | Acc: (90.61%) (31430/34688)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 280 |  Loss: (0.2779) | Acc: (90.62%) (32593/35968)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 290 |  Loss: (0.2782) | Acc: (90.58%) (33738/37248)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 300 |  Loss: (0.2801) | Acc: (90.51%) (34871/38528)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 310 |  Loss: (0.2799) | Acc: (90.52%) (36036/39808)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 320 |  Loss: (0.2799) | Acc: (90.52%) (37194/41088)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 330 |  Loss: (0.2810) | Acc: (90.49%) (38338/42368)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 340 |  Loss: (0.2820) | Acc: (90.41%) (39462/43648)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 350 |  Loss: (0.2822) | Acc: (90.38%) (40606/44928)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 360 |  Loss: (0.2835) | Acc: (90.33%) (41741/46208)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 370 |  Loss: (0.2829) | Acc: (90.35%) (42904/47488)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 380 |  Loss: (0.2832) | Acc: (90.34%) (44058/48768)\n",
      "#TRAIN: Epoch: 32 | Batch_idx: 390 |  Loss: (0.2830) | Acc: (90.34%) (45170/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4739) | Acc: (85.43%) (8543/10000)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 0 |  Loss: (0.1906) | Acc: (91.41%) (117/128)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 10 |  Loss: (0.2544) | Acc: (91.55%) (1289/1408)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 20 |  Loss: (0.2503) | Acc: (91.63%) (2463/2688)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 30 |  Loss: (0.2604) | Acc: (91.28%) (3622/3968)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 40 |  Loss: (0.2616) | Acc: (91.29%) (4791/5248)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 50 |  Loss: (0.2707) | Acc: (90.90%) (5934/6528)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 60 |  Loss: (0.2701) | Acc: (90.88%) (7096/7808)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 70 |  Loss: (0.2766) | Acc: (90.61%) (8235/9088)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 80 |  Loss: (0.2816) | Acc: (90.48%) (9381/10368)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 90 |  Loss: (0.2788) | Acc: (90.56%) (10548/11648)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 100 |  Loss: (0.2792) | Acc: (90.54%) (11705/12928)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 110 |  Loss: (0.2807) | Acc: (90.58%) (12869/14208)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 120 |  Loss: (0.2798) | Acc: (90.59%) (14031/15488)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 130 |  Loss: (0.2783) | Acc: (90.71%) (15210/16768)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 140 |  Loss: (0.2808) | Acc: (90.62%) (16356/18048)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 150 |  Loss: (0.2799) | Acc: (90.67%) (17525/19328)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 160 |  Loss: (0.2780) | Acc: (90.74%) (18699/20608)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 170 |  Loss: (0.2767) | Acc: (90.75%) (19864/21888)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 180 |  Loss: (0.2785) | Acc: (90.67%) (21007/23168)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 190 |  Loss: (0.2797) | Acc: (90.62%) (22154/24448)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 200 |  Loss: (0.2797) | Acc: (90.59%) (23307/25728)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 210 |  Loss: (0.2788) | Acc: (90.61%) (24472/27008)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 220 |  Loss: (0.2778) | Acc: (90.65%) (25643/28288)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 230 |  Loss: (0.2782) | Acc: (90.66%) (26805/29568)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 240 |  Loss: (0.2760) | Acc: (90.69%) (27975/30848)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 250 |  Loss: (0.2749) | Acc: (90.72%) (29145/32128)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 260 |  Loss: (0.2738) | Acc: (90.76%) (30322/33408)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 270 |  Loss: (0.2732) | Acc: (90.78%) (31490/34688)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 280 |  Loss: (0.2731) | Acc: (90.81%) (32661/35968)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 290 |  Loss: (0.2736) | Acc: (90.77%) (33810/37248)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 300 |  Loss: (0.2733) | Acc: (90.77%) (34973/38528)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 310 |  Loss: (0.2741) | Acc: (90.76%) (36128/39808)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 320 |  Loss: (0.2753) | Acc: (90.70%) (37267/41088)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 330 |  Loss: (0.2751) | Acc: (90.69%) (38423/42368)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 340 |  Loss: (0.2755) | Acc: (90.68%) (39582/43648)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 350 |  Loss: (0.2755) | Acc: (90.66%) (40733/44928)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 360 |  Loss: (0.2743) | Acc: (90.71%) (41914/46208)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 370 |  Loss: (0.2739) | Acc: (90.73%) (43086/47488)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 380 |  Loss: (0.2736) | Acc: (90.73%) (44247/48768)\n",
      "#TRAIN: Epoch: 33 | Batch_idx: 390 |  Loss: (0.2734) | Acc: (90.72%) (45360/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4662) | Acc: (85.88%) (8588/10000)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 0 |  Loss: (0.2230) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 10 |  Loss: (0.2879) | Acc: (90.20%) (1270/1408)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 20 |  Loss: (0.2757) | Acc: (90.66%) (2437/2688)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 30 |  Loss: (0.2656) | Acc: (90.83%) (3604/3968)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 40 |  Loss: (0.2581) | Acc: (91.14%) (4783/5248)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 50 |  Loss: (0.2609) | Acc: (91.19%) (5953/6528)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 60 |  Loss: (0.2609) | Acc: (91.21%) (7122/7808)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 70 |  Loss: (0.2624) | Acc: (91.14%) (8283/9088)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 80 |  Loss: (0.2646) | Acc: (91.06%) (9441/10368)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 90 |  Loss: (0.2616) | Acc: (91.19%) (10622/11648)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 100 |  Loss: (0.2581) | Acc: (91.31%) (11805/12928)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 110 |  Loss: (0.2587) | Acc: (91.25%) (12965/14208)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 120 |  Loss: (0.2570) | Acc: (91.32%) (14144/15488)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 130 |  Loss: (0.2552) | Acc: (91.38%) (15322/16768)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 140 |  Loss: (0.2542) | Acc: (91.36%) (16488/18048)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 150 |  Loss: (0.2561) | Acc: (91.30%) (17647/19328)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 160 |  Loss: (0.2575) | Acc: (91.23%) (18801/20608)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 170 |  Loss: (0.2586) | Acc: (91.22%) (19967/21888)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 180 |  Loss: (0.2601) | Acc: (91.14%) (21116/23168)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 190 |  Loss: (0.2595) | Acc: (91.16%) (22288/24448)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 200 |  Loss: (0.2626) | Acc: (91.06%) (23429/25728)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 210 |  Loss: (0.2634) | Acc: (91.07%) (24597/27008)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 220 |  Loss: (0.2645) | Acc: (91.01%) (25746/28288)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 230 |  Loss: (0.2668) | Acc: (90.93%) (26885/29568)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 240 |  Loss: (0.2670) | Acc: (90.90%) (28042/30848)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 250 |  Loss: (0.2680) | Acc: (90.86%) (29193/32128)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 260 |  Loss: (0.2682) | Acc: (90.86%) (30355/33408)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 270 |  Loss: (0.2683) | Acc: (90.86%) (31519/34688)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 280 |  Loss: (0.2683) | Acc: (90.84%) (32675/35968)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 290 |  Loss: (0.2673) | Acc: (90.90%) (33858/37248)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 300 |  Loss: (0.2688) | Acc: (90.83%) (34995/38528)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 310 |  Loss: (0.2692) | Acc: (90.82%) (36152/39808)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 320 |  Loss: (0.2691) | Acc: (90.81%) (37310/41088)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 330 |  Loss: (0.2687) | Acc: (90.82%) (38477/42368)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 340 |  Loss: (0.2684) | Acc: (90.84%) (39651/43648)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 350 |  Loss: (0.2688) | Acc: (90.83%) (40807/44928)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 360 |  Loss: (0.2682) | Acc: (90.84%) (41976/46208)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 370 |  Loss: (0.2679) | Acc: (90.87%) (43151/47488)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 380 |  Loss: (0.2677) | Acc: (90.86%) (44312/48768)\n",
      "#TRAIN: Epoch: 34 | Batch_idx: 390 |  Loss: (0.2690) | Acc: (90.82%) (45409/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5178) | Acc: (84.39%) (8439/10000)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 0 |  Loss: (0.2443) | Acc: (91.41%) (117/128)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 10 |  Loss: (0.2915) | Acc: (90.34%) (1272/1408)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 20 |  Loss: (0.2868) | Acc: (90.18%) (2424/2688)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 30 |  Loss: (0.2771) | Acc: (90.78%) (3602/3968)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 40 |  Loss: (0.2703) | Acc: (90.95%) (4773/5248)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 50 |  Loss: (0.2683) | Acc: (91.04%) (5943/6528)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 60 |  Loss: (0.2646) | Acc: (91.21%) (7122/7808)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 70 |  Loss: (0.2630) | Acc: (91.21%) (8289/9088)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 80 |  Loss: (0.2619) | Acc: (91.28%) (9464/10368)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 90 |  Loss: (0.2611) | Acc: (91.29%) (10633/11648)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 100 |  Loss: (0.2585) | Acc: (91.31%) (11804/12928)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 110 |  Loss: (0.2575) | Acc: (91.33%) (12976/14208)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 120 |  Loss: (0.2582) | Acc: (91.35%) (14149/15488)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 130 |  Loss: (0.2600) | Acc: (91.22%) (15295/16768)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 140 |  Loss: (0.2597) | Acc: (91.23%) (16466/18048)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 150 |  Loss: (0.2581) | Acc: (91.24%) (17635/19328)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 160 |  Loss: (0.2594) | Acc: (91.20%) (18794/20608)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 170 |  Loss: (0.2606) | Acc: (91.23%) (19969/21888)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 180 |  Loss: (0.2600) | Acc: (91.26%) (21144/23168)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 190 |  Loss: (0.2606) | Acc: (91.24%) (22306/24448)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 200 |  Loss: (0.2603) | Acc: (91.26%) (23480/25728)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 210 |  Loss: (0.2615) | Acc: (91.20%) (24631/27008)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 220 |  Loss: (0.2619) | Acc: (91.20%) (25800/28288)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 230 |  Loss: (0.2623) | Acc: (91.19%) (26964/29568)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 240 |  Loss: (0.2636) | Acc: (91.16%) (28122/30848)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 250 |  Loss: (0.2654) | Acc: (91.06%) (29255/32128)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 260 |  Loss: (0.2655) | Acc: (91.02%) (30409/33408)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 270 |  Loss: (0.2659) | Acc: (91.02%) (31572/34688)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 280 |  Loss: (0.2657) | Acc: (91.02%) (32739/35968)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 290 |  Loss: (0.2649) | Acc: (91.04%) (33909/37248)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 300 |  Loss: (0.2649) | Acc: (91.03%) (35072/38528)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 310 |  Loss: (0.2646) | Acc: (91.03%) (36237/39808)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 320 |  Loss: (0.2652) | Acc: (91.03%) (37403/41088)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 330 |  Loss: (0.2654) | Acc: (91.03%) (38567/42368)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 340 |  Loss: (0.2662) | Acc: (91.01%) (39723/43648)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 350 |  Loss: (0.2660) | Acc: (91.01%) (40889/44928)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 360 |  Loss: (0.2660) | Acc: (91.03%) (42061/46208)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 370 |  Loss: (0.2665) | Acc: (91.01%) (43221/47488)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 380 |  Loss: (0.2663) | Acc: (91.03%) (44394/48768)\n",
      "#TRAIN: Epoch: 35 | Batch_idx: 390 |  Loss: (0.2666) | Acc: (91.02%) (45511/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4562) | Acc: (86.11%) (8611/10000)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 0 |  Loss: (0.1806) | Acc: (91.41%) (117/128)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 10 |  Loss: (0.2392) | Acc: (91.26%) (1285/1408)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 20 |  Loss: (0.2287) | Acc: (91.67%) (2464/2688)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 30 |  Loss: (0.2264) | Acc: (92.11%) (3655/3968)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 40 |  Loss: (0.2298) | Acc: (92.09%) (4833/5248)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 50 |  Loss: (0.2235) | Acc: (92.22%) (6020/6528)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 60 |  Loss: (0.2253) | Acc: (92.12%) (7193/7808)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 70 |  Loss: (0.2243) | Acc: (92.19%) (8378/9088)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 80 |  Loss: (0.2268) | Acc: (92.16%) (9555/10368)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 90 |  Loss: (0.2291) | Acc: (92.10%) (10728/11648)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 100 |  Loss: (0.2349) | Acc: (91.89%) (11879/12928)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 110 |  Loss: (0.2359) | Acc: (91.93%) (13061/14208)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 120 |  Loss: (0.2365) | Acc: (91.90%) (14234/15488)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 130 |  Loss: (0.2397) | Acc: (91.84%) (15399/16768)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 140 |  Loss: (0.2420) | Acc: (91.76%) (16560/18048)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 150 |  Loss: (0.2445) | Acc: (91.71%) (17725/19328)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 160 |  Loss: (0.2460) | Acc: (91.66%) (18890/20608)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 170 |  Loss: (0.2477) | Acc: (91.64%) (20058/21888)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 180 |  Loss: (0.2470) | Acc: (91.67%) (21238/23168)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 190 |  Loss: (0.2475) | Acc: (91.63%) (22402/24448)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 200 |  Loss: (0.2482) | Acc: (91.62%) (23571/25728)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 210 |  Loss: (0.2497) | Acc: (91.59%) (24737/27008)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 220 |  Loss: (0.2503) | Acc: (91.56%) (25900/28288)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 230 |  Loss: (0.2504) | Acc: (91.55%) (27070/29568)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 240 |  Loss: (0.2506) | Acc: (91.52%) (28231/30848)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 250 |  Loss: (0.2509) | Acc: (91.55%) (29412/32128)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 260 |  Loss: (0.2518) | Acc: (91.52%) (30575/33408)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 270 |  Loss: (0.2524) | Acc: (91.47%) (31729/34688)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 280 |  Loss: (0.2524) | Acc: (91.45%) (32891/35968)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 290 |  Loss: (0.2533) | Acc: (91.41%) (34048/37248)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 300 |  Loss: (0.2548) | Acc: (91.35%) (35197/38528)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 310 |  Loss: (0.2555) | Acc: (91.33%) (36358/39808)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 320 |  Loss: (0.2551) | Acc: (91.32%) (37523/41088)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 330 |  Loss: (0.2545) | Acc: (91.34%) (38699/42368)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 340 |  Loss: (0.2541) | Acc: (91.37%) (39879/43648)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 350 |  Loss: (0.2530) | Acc: (91.41%) (41069/44928)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 360 |  Loss: (0.2531) | Acc: (91.39%) (42231/46208)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 370 |  Loss: (0.2533) | Acc: (91.39%) (43401/47488)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 380 |  Loss: (0.2535) | Acc: (91.38%) (44563/48768)\n",
      "#TRAIN: Epoch: 36 | Batch_idx: 390 |  Loss: (0.2553) | Acc: (91.33%) (45666/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5270) | Acc: (84.76%) (8476/10000)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 0 |  Loss: (0.3498) | Acc: (84.38%) (108/128)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 10 |  Loss: (0.2394) | Acc: (91.19%) (1284/1408)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 20 |  Loss: (0.2373) | Acc: (91.18%) (2451/2688)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 30 |  Loss: (0.2428) | Acc: (91.56%) (3633/3968)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 40 |  Loss: (0.2421) | Acc: (91.54%) (4804/5248)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 50 |  Loss: (0.2397) | Acc: (91.87%) (5997/6528)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 60 |  Loss: (0.2459) | Acc: (91.65%) (7156/7808)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 70 |  Loss: (0.2487) | Acc: (91.44%) (8310/9088)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 80 |  Loss: (0.2463) | Acc: (91.57%) (9494/10368)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 90 |  Loss: (0.2440) | Acc: (91.66%) (10677/11648)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 100 |  Loss: (0.2436) | Acc: (91.70%) (11855/12928)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 110 |  Loss: (0.2458) | Acc: (91.73%) (13033/14208)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 120 |  Loss: (0.2435) | Acc: (91.83%) (14222/15488)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 130 |  Loss: (0.2433) | Acc: (91.82%) (15397/16768)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 140 |  Loss: (0.2439) | Acc: (91.73%) (16555/18048)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 150 |  Loss: (0.2438) | Acc: (91.78%) (17740/19328)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 160 |  Loss: (0.2427) | Acc: (91.83%) (18925/20608)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 170 |  Loss: (0.2438) | Acc: (91.81%) (20095/21888)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 180 |  Loss: (0.2435) | Acc: (91.81%) (21271/23168)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 190 |  Loss: (0.2440) | Acc: (91.82%) (22447/24448)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 200 |  Loss: (0.2453) | Acc: (91.76%) (23608/25728)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 210 |  Loss: (0.2453) | Acc: (91.78%) (24788/27008)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 220 |  Loss: (0.2455) | Acc: (91.74%) (25951/28288)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 230 |  Loss: (0.2453) | Acc: (91.74%) (27127/29568)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 240 |  Loss: (0.2453) | Acc: (91.75%) (28304/30848)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 250 |  Loss: (0.2459) | Acc: (91.73%) (29472/32128)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 260 |  Loss: (0.2464) | Acc: (91.71%) (30639/33408)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 270 |  Loss: (0.2480) | Acc: (91.66%) (31795/34688)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 280 |  Loss: (0.2488) | Acc: (91.62%) (32955/35968)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 290 |  Loss: (0.2493) | Acc: (91.60%) (34120/37248)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 300 |  Loss: (0.2489) | Acc: (91.61%) (35294/38528)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 310 |  Loss: (0.2481) | Acc: (91.64%) (36482/39808)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 320 |  Loss: (0.2481) | Acc: (91.67%) (37664/41088)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 330 |  Loss: (0.2465) | Acc: (91.73%) (38863/42368)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 340 |  Loss: (0.2474) | Acc: (91.68%) (40016/43648)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 350 |  Loss: (0.2475) | Acc: (91.68%) (41188/44928)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 360 |  Loss: (0.2468) | Acc: (91.71%) (42377/46208)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 370 |  Loss: (0.2483) | Acc: (91.67%) (43532/47488)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 380 |  Loss: (0.2483) | Acc: (91.67%) (44705/48768)\n",
      "#TRAIN: Epoch: 37 | Batch_idx: 390 |  Loss: (0.2483) | Acc: (91.68%) (45840/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4803) | Acc: (85.52%) (8552/10000)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 0 |  Loss: (0.2204) | Acc: (90.62%) (116/128)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 10 |  Loss: (0.2307) | Acc: (91.97%) (1295/1408)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 20 |  Loss: (0.2295) | Acc: (92.19%) (2478/2688)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 30 |  Loss: (0.2441) | Acc: (91.96%) (3649/3968)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 40 |  Loss: (0.2415) | Acc: (92.09%) (4833/5248)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 50 |  Loss: (0.2401) | Acc: (92.14%) (6015/6528)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 60 |  Loss: (0.2376) | Acc: (92.14%) (7194/7808)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 70 |  Loss: (0.2350) | Acc: (92.09%) (8369/9088)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 80 |  Loss: (0.2303) | Acc: (92.28%) (9568/10368)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 90 |  Loss: (0.2299) | Acc: (92.26%) (10746/11648)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 100 |  Loss: (0.2294) | Acc: (92.27%) (11929/12928)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 110 |  Loss: (0.2311) | Acc: (92.22%) (13102/14208)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 120 |  Loss: (0.2308) | Acc: (92.19%) (14278/15488)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 130 |  Loss: (0.2319) | Acc: (92.13%) (15448/16768)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 140 |  Loss: (0.2313) | Acc: (92.20%) (16640/18048)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 150 |  Loss: (0.2315) | Acc: (92.20%) (17820/19328)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 160 |  Loss: (0.2305) | Acc: (92.24%) (19009/20608)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 170 |  Loss: (0.2305) | Acc: (92.24%) (20189/21888)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 180 |  Loss: (0.2326) | Acc: (92.15%) (21349/23168)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 190 |  Loss: (0.2331) | Acc: (92.13%) (22525/24448)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 200 |  Loss: (0.2333) | Acc: (92.14%) (23707/25728)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 210 |  Loss: (0.2328) | Acc: (92.14%) (24886/27008)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 220 |  Loss: (0.2338) | Acc: (92.10%) (26053/28288)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 230 |  Loss: (0.2352) | Acc: (92.03%) (27210/29568)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 240 |  Loss: (0.2360) | Acc: (91.97%) (28372/30848)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 250 |  Loss: (0.2358) | Acc: (91.99%) (29554/32128)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 260 |  Loss: (0.2363) | Acc: (91.99%) (30732/33408)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 270 |  Loss: (0.2366) | Acc: (91.97%) (31901/34688)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 280 |  Loss: (0.2362) | Acc: (91.98%) (33084/35968)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 290 |  Loss: (0.2376) | Acc: (91.94%) (34247/37248)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 300 |  Loss: (0.2390) | Acc: (91.89%) (35405/38528)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 310 |  Loss: (0.2394) | Acc: (91.88%) (36574/39808)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 320 |  Loss: (0.2401) | Acc: (91.83%) (37731/41088)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 330 |  Loss: (0.2409) | Acc: (91.82%) (38901/42368)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 340 |  Loss: (0.2405) | Acc: (91.83%) (40083/43648)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 350 |  Loss: (0.2401) | Acc: (91.84%) (41264/44928)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 360 |  Loss: (0.2407) | Acc: (91.83%) (42432/46208)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 370 |  Loss: (0.2412) | Acc: (91.78%) (43583/47488)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 380 |  Loss: (0.2416) | Acc: (91.77%) (44753/48768)\n",
      "#TRAIN: Epoch: 38 | Batch_idx: 390 |  Loss: (0.2417) | Acc: (91.76%) (45879/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4970) | Acc: (85.19%) (8519/10000)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 0 |  Loss: (0.2207) | Acc: (91.41%) (117/128)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 10 |  Loss: (0.2304) | Acc: (92.47%) (1302/1408)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 20 |  Loss: (0.2145) | Acc: (92.52%) (2487/2688)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 30 |  Loss: (0.2257) | Acc: (92.21%) (3659/3968)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 40 |  Loss: (0.2352) | Acc: (91.75%) (4815/5248)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 50 |  Loss: (0.2335) | Acc: (91.85%) (5996/6528)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 60 |  Loss: (0.2304) | Acc: (91.97%) (7181/7808)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 70 |  Loss: (0.2302) | Acc: (91.97%) (8358/9088)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 80 |  Loss: (0.2278) | Acc: (92.01%) (9540/10368)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 90 |  Loss: (0.2278) | Acc: (91.99%) (10715/11648)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 100 |  Loss: (0.2255) | Acc: (92.06%) (11901/12928)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 110 |  Loss: (0.2286) | Acc: (92.00%) (13071/14208)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 120 |  Loss: (0.2283) | Acc: (92.01%) (14250/15488)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 130 |  Loss: (0.2304) | Acc: (91.99%) (15425/16768)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 140 |  Loss: (0.2308) | Acc: (91.99%) (16602/18048)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 150 |  Loss: (0.2310) | Acc: (91.98%) (17778/19328)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 160 |  Loss: (0.2319) | Acc: (91.94%) (18946/20608)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 170 |  Loss: (0.2320) | Acc: (91.92%) (20120/21888)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 180 |  Loss: (0.2322) | Acc: (91.91%) (21293/23168)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 190 |  Loss: (0.2320) | Acc: (91.94%) (22478/24448)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 200 |  Loss: (0.2321) | Acc: (91.93%) (23653/25728)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 210 |  Loss: (0.2311) | Acc: (91.98%) (24842/27008)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 220 |  Loss: (0.2305) | Acc: (92.02%) (26031/28288)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 230 |  Loss: (0.2310) | Acc: (91.99%) (27200/29568)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 240 |  Loss: (0.2311) | Acc: (91.98%) (28375/30848)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 250 |  Loss: (0.2308) | Acc: (91.99%) (29555/32128)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 260 |  Loss: (0.2310) | Acc: (91.99%) (30731/33408)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 270 |  Loss: (0.2334) | Acc: (91.88%) (31870/34688)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 280 |  Loss: (0.2330) | Acc: (91.88%) (33049/35968)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 290 |  Loss: (0.2333) | Acc: (91.88%) (34224/37248)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 300 |  Loss: (0.2330) | Acc: (91.90%) (35409/38528)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 310 |  Loss: (0.2339) | Acc: (91.86%) (36567/39808)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 320 |  Loss: (0.2344) | Acc: (91.86%) (37743/41088)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 330 |  Loss: (0.2348) | Acc: (91.86%) (38920/42368)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 340 |  Loss: (0.2349) | Acc: (91.86%) (40094/43648)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 350 |  Loss: (0.2350) | Acc: (91.88%) (41279/44928)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 360 |  Loss: (0.2353) | Acc: (91.87%) (42449/46208)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 370 |  Loss: (0.2358) | Acc: (91.87%) (43625/47488)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 380 |  Loss: (0.2350) | Acc: (91.88%) (44809/48768)\n",
      "#TRAIN: Epoch: 39 | Batch_idx: 390 |  Loss: (0.2347) | Acc: (91.90%) (45951/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4714) | Acc: (85.97%) (8597/10000)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 0 |  Loss: (0.1735) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 10 |  Loss: (0.2043) | Acc: (93.39%) (1315/1408)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 20 |  Loss: (0.2100) | Acc: (93.01%) (2500/2688)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 30 |  Loss: (0.2106) | Acc: (92.84%) (3684/3968)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 40 |  Loss: (0.2181) | Acc: (92.68%) (4864/5248)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 50 |  Loss: (0.2239) | Acc: (92.33%) (6027/6528)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 60 |  Loss: (0.2202) | Acc: (92.47%) (7220/7808)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 70 |  Loss: (0.2210) | Acc: (92.42%) (8399/9088)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 80 |  Loss: (0.2226) | Acc: (92.39%) (9579/10368)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 90 |  Loss: (0.2213) | Acc: (92.45%) (10768/11648)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 100 |  Loss: (0.2246) | Acc: (92.31%) (11934/12928)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 110 |  Loss: (0.2263) | Acc: (92.24%) (13105/14208)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 120 |  Loss: (0.2254) | Acc: (92.33%) (14300/15488)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 130 |  Loss: (0.2262) | Acc: (92.24%) (15466/16768)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 140 |  Loss: (0.2255) | Acc: (92.29%) (16656/18048)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 150 |  Loss: (0.2239) | Acc: (92.31%) (17842/19328)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 160 |  Loss: (0.2232) | Acc: (92.28%) (19017/20608)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 170 |  Loss: (0.2221) | Acc: (92.32%) (20208/21888)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 180 |  Loss: (0.2214) | Acc: (92.34%) (21393/23168)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 190 |  Loss: (0.2209) | Acc: (92.37%) (22582/24448)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 200 |  Loss: (0.2203) | Acc: (92.40%) (23772/25728)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 210 |  Loss: (0.2187) | Acc: (92.49%) (24979/27008)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 220 |  Loss: (0.2193) | Acc: (92.48%) (26162/28288)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 230 |  Loss: (0.2218) | Acc: (92.43%) (27329/29568)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 240 |  Loss: (0.2226) | Acc: (92.41%) (28507/30848)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 250 |  Loss: (0.2233) | Acc: (92.39%) (29682/32128)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 260 |  Loss: (0.2246) | Acc: (92.36%) (30855/33408)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 270 |  Loss: (0.2238) | Acc: (92.40%) (32053/34688)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 280 |  Loss: (0.2260) | Acc: (92.33%) (33209/35968)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 290 |  Loss: (0.2256) | Acc: (92.36%) (34404/37248)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 300 |  Loss: (0.2262) | Acc: (92.35%) (35581/38528)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 310 |  Loss: (0.2262) | Acc: (92.36%) (36766/39808)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 320 |  Loss: (0.2265) | Acc: (92.37%) (37951/41088)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 330 |  Loss: (0.2269) | Acc: (92.36%) (39132/42368)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 340 |  Loss: (0.2271) | Acc: (92.34%) (40306/43648)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 350 |  Loss: (0.2271) | Acc: (92.35%) (41490/44928)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 360 |  Loss: (0.2272) | Acc: (92.33%) (42666/46208)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 370 |  Loss: (0.2280) | Acc: (92.32%) (43843/47488)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 380 |  Loss: (0.2279) | Acc: (92.34%) (45033/48768)\n",
      "#TRAIN: Epoch: 40 | Batch_idx: 390 |  Loss: (0.2289) | Acc: (92.33%) (46163/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4661) | Acc: (86.53%) (8653/10000)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 0 |  Loss: (0.2058) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 10 |  Loss: (0.2509) | Acc: (91.62%) (1290/1408)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 20 |  Loss: (0.2234) | Acc: (92.56%) (2488/2688)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 30 |  Loss: (0.2087) | Acc: (93.02%) (3691/3968)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 40 |  Loss: (0.2130) | Acc: (92.91%) (4876/5248)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 50 |  Loss: (0.2186) | Acc: (92.66%) (6049/6528)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 60 |  Loss: (0.2158) | Acc: (92.75%) (7242/7808)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 70 |  Loss: (0.2139) | Acc: (92.79%) (8433/9088)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 80 |  Loss: (0.2132) | Acc: (92.73%) (9614/10368)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 90 |  Loss: (0.2126) | Acc: (92.78%) (10807/11648)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 100 |  Loss: (0.2133) | Acc: (92.78%) (11994/12928)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 110 |  Loss: (0.2144) | Acc: (92.69%) (13169/14208)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 120 |  Loss: (0.2153) | Acc: (92.61%) (14344/15488)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 130 |  Loss: (0.2155) | Acc: (92.65%) (15535/16768)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 140 |  Loss: (0.2150) | Acc: (92.72%) (16734/18048)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 150 |  Loss: (0.2147) | Acc: (92.70%) (17918/19328)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 160 |  Loss: (0.2153) | Acc: (92.71%) (19106/20608)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 170 |  Loss: (0.2159) | Acc: (92.66%) (20282/21888)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 180 |  Loss: (0.2171) | Acc: (92.64%) (21463/23168)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 190 |  Loss: (0.2159) | Acc: (92.70%) (22663/24448)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 200 |  Loss: (0.2160) | Acc: (92.67%) (23841/25728)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 210 |  Loss: (0.2172) | Acc: (92.64%) (25020/27008)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 220 |  Loss: (0.2179) | Acc: (92.64%) (26205/28288)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 230 |  Loss: (0.2177) | Acc: (92.65%) (27394/29568)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 240 |  Loss: (0.2180) | Acc: (92.62%) (28570/30848)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 250 |  Loss: (0.2178) | Acc: (92.63%) (29760/32128)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 260 |  Loss: (0.2181) | Acc: (92.62%) (30942/33408)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 270 |  Loss: (0.2182) | Acc: (92.60%) (32122/34688)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 280 |  Loss: (0.2181) | Acc: (92.61%) (33310/35968)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 290 |  Loss: (0.2198) | Acc: (92.55%) (34472/37248)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 300 |  Loss: (0.2194) | Acc: (92.54%) (35655/38528)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 310 |  Loss: (0.2196) | Acc: (92.53%) (36834/39808)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 320 |  Loss: (0.2189) | Acc: (92.56%) (38033/41088)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 330 |  Loss: (0.2205) | Acc: (92.53%) (39201/42368)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 340 |  Loss: (0.2204) | Acc: (92.52%) (40381/43648)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 350 |  Loss: (0.2202) | Acc: (92.54%) (41578/44928)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 360 |  Loss: (0.2199) | Acc: (92.56%) (42771/46208)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 370 |  Loss: (0.2202) | Acc: (92.55%) (43950/47488)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 380 |  Loss: (0.2210) | Acc: (92.54%) (45131/48768)\n",
      "#TRAIN: Epoch: 41 | Batch_idx: 390 |  Loss: (0.2211) | Acc: (92.53%) (46267/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5114) | Acc: (85.47%) (8547/10000)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 0 |  Loss: (0.2511) | Acc: (92.97%) (119/128)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 10 |  Loss: (0.2087) | Acc: (92.90%) (1308/1408)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 20 |  Loss: (0.2103) | Acc: (93.04%) (2501/2688)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 30 |  Loss: (0.1969) | Acc: (93.32%) (3703/3968)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 40 |  Loss: (0.1926) | Acc: (93.58%) (4911/5248)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 50 |  Loss: (0.1936) | Acc: (93.44%) (6100/6528)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 60 |  Loss: (0.1982) | Acc: (93.29%) (7284/7808)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 70 |  Loss: (0.1979) | Acc: (93.38%) (8486/9088)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 80 |  Loss: (0.2012) | Acc: (93.25%) (9668/10368)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 90 |  Loss: (0.2024) | Acc: (93.19%) (10855/11648)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 100 |  Loss: (0.2022) | Acc: (93.21%) (12050/12928)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 110 |  Loss: (0.2028) | Acc: (93.21%) (13243/14208)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 120 |  Loss: (0.2044) | Acc: (93.14%) (14426/15488)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 130 |  Loss: (0.2049) | Acc: (93.13%) (15616/16768)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 140 |  Loss: (0.2063) | Acc: (93.07%) (16798/18048)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 150 |  Loss: (0.2076) | Acc: (93.02%) (17978/19328)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 160 |  Loss: (0.2095) | Acc: (92.93%) (19151/20608)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 170 |  Loss: (0.2099) | Acc: (92.92%) (20339/21888)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 180 |  Loss: (0.2106) | Acc: (92.87%) (21516/23168)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 190 |  Loss: (0.2108) | Acc: (92.85%) (22699/24448)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 200 |  Loss: (0.2120) | Acc: (92.79%) (23873/25728)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 210 |  Loss: (0.2138) | Acc: (92.72%) (25043/27008)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 220 |  Loss: (0.2149) | Acc: (92.66%) (26212/28288)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 230 |  Loss: (0.2162) | Acc: (92.62%) (27387/29568)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 240 |  Loss: (0.2166) | Acc: (92.58%) (28558/30848)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 250 |  Loss: (0.2162) | Acc: (92.56%) (29738/32128)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 260 |  Loss: (0.2154) | Acc: (92.57%) (30925/33408)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 270 |  Loss: (0.2154) | Acc: (92.57%) (32111/34688)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 280 |  Loss: (0.2140) | Acc: (92.64%) (33319/35968)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 290 |  Loss: (0.2140) | Acc: (92.67%) (34516/37248)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 300 |  Loss: (0.2149) | Acc: (92.62%) (35685/38528)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 310 |  Loss: (0.2158) | Acc: (92.58%) (36855/39808)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 320 |  Loss: (0.2159) | Acc: (92.58%) (38040/41088)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 330 |  Loss: (0.2173) | Acc: (92.53%) (39201/42368)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 340 |  Loss: (0.2169) | Acc: (92.54%) (40393/43648)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 350 |  Loss: (0.2168) | Acc: (92.54%) (41577/44928)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 360 |  Loss: (0.2169) | Acc: (92.50%) (42744/46208)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 370 |  Loss: (0.2170) | Acc: (92.49%) (43921/47488)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 380 |  Loss: (0.2173) | Acc: (92.48%) (45103/48768)\n",
      "#TRAIN: Epoch: 42 | Batch_idx: 390 |  Loss: (0.2173) | Acc: (92.49%) (46245/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4821) | Acc: (86.45%) (8645/10000)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 0 |  Loss: (0.1854) | Acc: (92.19%) (118/128)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 10 |  Loss: (0.1951) | Acc: (93.11%) (1311/1408)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 20 |  Loss: (0.2046) | Acc: (92.89%) (2497/2688)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 30 |  Loss: (0.2109) | Acc: (92.67%) (3677/3968)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 40 |  Loss: (0.2077) | Acc: (92.64%) (4862/5248)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 50 |  Loss: (0.2052) | Acc: (92.72%) (6053/6528)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 60 |  Loss: (0.2037) | Acc: (92.87%) (7251/7808)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 70 |  Loss: (0.1995) | Acc: (93.01%) (8453/9088)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 80 |  Loss: (0.2025) | Acc: (92.91%) (9633/10368)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 90 |  Loss: (0.2022) | Acc: (92.94%) (10826/11648)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 100 |  Loss: (0.1985) | Acc: (93.05%) (12029/12928)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 110 |  Loss: (0.2028) | Acc: (92.88%) (13197/14208)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 120 |  Loss: (0.2033) | Acc: (92.90%) (14389/15488)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 130 |  Loss: (0.2023) | Acc: (92.89%) (15576/16768)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 140 |  Loss: (0.2033) | Acc: (92.85%) (16757/18048)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 150 |  Loss: (0.2043) | Acc: (92.84%) (17944/19328)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 160 |  Loss: (0.2041) | Acc: (92.87%) (19139/20608)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 170 |  Loss: (0.2051) | Acc: (92.84%) (20321/21888)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 180 |  Loss: (0.2056) | Acc: (92.83%) (21507/23168)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 190 |  Loss: (0.2047) | Acc: (92.87%) (22706/24448)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 200 |  Loss: (0.2045) | Acc: (92.88%) (23897/25728)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 210 |  Loss: (0.2056) | Acc: (92.82%) (25070/27008)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 220 |  Loss: (0.2055) | Acc: (92.84%) (26263/28288)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 230 |  Loss: (0.2066) | Acc: (92.83%) (27447/29568)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 240 |  Loss: (0.2070) | Acc: (92.81%) (28631/30848)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 250 |  Loss: (0.2079) | Acc: (92.80%) (29816/32128)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 260 |  Loss: (0.2084) | Acc: (92.80%) (31003/33408)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 270 |  Loss: (0.2084) | Acc: (92.81%) (32194/34688)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 280 |  Loss: (0.2087) | Acc: (92.79%) (33375/35968)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 290 |  Loss: (0.2087) | Acc: (92.80%) (34565/37248)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 300 |  Loss: (0.2083) | Acc: (92.81%) (35757/38528)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 310 |  Loss: (0.2090) | Acc: (92.78%) (36934/39808)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 320 |  Loss: (0.2104) | Acc: (92.73%) (38102/41088)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 330 |  Loss: (0.2107) | Acc: (92.72%) (39285/42368)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 340 |  Loss: (0.2107) | Acc: (92.72%) (40472/43648)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 350 |  Loss: (0.2112) | Acc: (92.70%) (41650/44928)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 360 |  Loss: (0.2115) | Acc: (92.69%) (42832/46208)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 370 |  Loss: (0.2122) | Acc: (92.69%) (44015/47488)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 380 |  Loss: (0.2130) | Acc: (92.67%) (45192/48768)\n",
      "#TRAIN: Epoch: 43 | Batch_idx: 390 |  Loss: (0.2125) | Acc: (92.69%) (46344/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4717) | Acc: (86.56%) (8656/10000)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 0 |  Loss: (0.1702) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 10 |  Loss: (0.1736) | Acc: (94.18%) (1326/1408)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 20 |  Loss: (0.1821) | Acc: (93.79%) (2521/2688)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 30 |  Loss: (0.1887) | Acc: (93.57%) (3713/3968)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 40 |  Loss: (0.1894) | Acc: (93.58%) (4911/5248)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 50 |  Loss: (0.1886) | Acc: (93.69%) (6116/6528)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 60 |  Loss: (0.1911) | Acc: (93.51%) (7301/7808)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 70 |  Loss: (0.1908) | Acc: (93.58%) (8505/9088)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 80 |  Loss: (0.1917) | Acc: (93.66%) (9711/10368)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 90 |  Loss: (0.1920) | Acc: (93.57%) (10899/11648)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 100 |  Loss: (0.1931) | Acc: (93.46%) (12083/12928)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 110 |  Loss: (0.1959) | Acc: (93.39%) (13269/14208)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 120 |  Loss: (0.1969) | Acc: (93.29%) (14449/15488)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 130 |  Loss: (0.1971) | Acc: (93.25%) (15636/16768)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 140 |  Loss: (0.1990) | Acc: (93.25%) (16830/18048)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 150 |  Loss: (0.1996) | Acc: (93.25%) (18023/19328)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 160 |  Loss: (0.1989) | Acc: (93.22%) (19210/20608)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 170 |  Loss: (0.1975) | Acc: (93.29%) (20420/21888)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 180 |  Loss: (0.1948) | Acc: (93.38%) (21634/23168)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 190 |  Loss: (0.1950) | Acc: (93.38%) (22829/24448)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 200 |  Loss: (0.1942) | Acc: (93.39%) (24028/25728)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 210 |  Loss: (0.1950) | Acc: (93.40%) (25226/27008)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 220 |  Loss: (0.1964) | Acc: (93.34%) (26404/28288)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 230 |  Loss: (0.1971) | Acc: (93.33%) (27597/29568)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 240 |  Loss: (0.1981) | Acc: (93.32%) (28786/30848)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 250 |  Loss: (0.1978) | Acc: (93.35%) (29991/32128)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 260 |  Loss: (0.1974) | Acc: (93.37%) (31192/33408)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 270 |  Loss: (0.1983) | Acc: (93.35%) (32380/34688)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 280 |  Loss: (0.1998) | Acc: (93.28%) (33551/35968)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 290 |  Loss: (0.2006) | Acc: (93.25%) (34734/37248)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 300 |  Loss: (0.2008) | Acc: (93.23%) (35920/38528)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 310 |  Loss: (0.2009) | Acc: (93.22%) (37110/39808)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 320 |  Loss: (0.2003) | Acc: (93.21%) (38299/41088)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 330 |  Loss: (0.2000) | Acc: (93.22%) (39495/42368)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 340 |  Loss: (0.2007) | Acc: (93.21%) (40686/43648)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 350 |  Loss: (0.2014) | Acc: (93.19%) (41868/44928)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 360 |  Loss: (0.2019) | Acc: (93.16%) (43049/46208)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 370 |  Loss: (0.2020) | Acc: (93.16%) (44238/47488)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 380 |  Loss: (0.2029) | Acc: (93.12%) (45411/48768)\n",
      "#TRAIN: Epoch: 44 | Batch_idx: 390 |  Loss: (0.2033) | Acc: (93.11%) (46555/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4991) | Acc: (86.00%) (8600/10000)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 0 |  Loss: (0.1035) | Acc: (96.88%) (124/128)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 10 |  Loss: (0.1737) | Acc: (94.67%) (1333/1408)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 20 |  Loss: (0.1808) | Acc: (94.08%) (2529/2688)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 30 |  Loss: (0.1934) | Acc: (93.72%) (3719/3968)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 40 |  Loss: (0.1892) | Acc: (94.04%) (4935/5248)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 50 |  Loss: (0.1885) | Acc: (94.00%) (6136/6528)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 60 |  Loss: (0.1838) | Acc: (93.99%) (7339/7808)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 70 |  Loss: (0.1824) | Acc: (94.00%) (8543/9088)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 80 |  Loss: (0.1815) | Acc: (94.02%) (9748/10368)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 90 |  Loss: (0.1840) | Acc: (93.87%) (10934/11648)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 100 |  Loss: (0.1864) | Acc: (93.84%) (12132/12928)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 110 |  Loss: (0.1870) | Acc: (93.74%) (13318/14208)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 120 |  Loss: (0.1897) | Acc: (93.65%) (14505/15488)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 130 |  Loss: (0.1916) | Acc: (93.59%) (15693/16768)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 140 |  Loss: (0.1926) | Acc: (93.51%) (16876/18048)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 150 |  Loss: (0.1929) | Acc: (93.54%) (18080/19328)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 160 |  Loss: (0.1946) | Acc: (93.42%) (19252/20608)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 170 |  Loss: (0.1948) | Acc: (93.43%) (20450/21888)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 180 |  Loss: (0.1946) | Acc: (93.46%) (21653/23168)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 190 |  Loss: (0.1963) | Acc: (93.40%) (22835/24448)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 200 |  Loss: (0.1954) | Acc: (93.44%) (24039/25728)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 210 |  Loss: (0.1946) | Acc: (93.46%) (25241/27008)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 220 |  Loss: (0.1954) | Acc: (93.42%) (26428/28288)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 230 |  Loss: (0.1949) | Acc: (93.45%) (27632/29568)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 240 |  Loss: (0.1955) | Acc: (93.42%) (28819/30848)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 250 |  Loss: (0.1969) | Acc: (93.36%) (29994/32128)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 260 |  Loss: (0.1965) | Acc: (93.37%) (31194/33408)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 270 |  Loss: (0.1958) | Acc: (93.39%) (32395/34688)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 280 |  Loss: (0.1959) | Acc: (93.39%) (33590/35968)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 290 |  Loss: (0.1962) | Acc: (93.37%) (34779/37248)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 300 |  Loss: (0.1963) | Acc: (93.37%) (35974/38528)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 310 |  Loss: (0.1963) | Acc: (93.38%) (37174/39808)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 320 |  Loss: (0.1969) | Acc: (93.37%) (38364/41088)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 330 |  Loss: (0.1982) | Acc: (93.32%) (39537/42368)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 340 |  Loss: (0.1983) | Acc: (93.31%) (40727/43648)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 350 |  Loss: (0.1980) | Acc: (93.32%) (41927/44928)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 360 |  Loss: (0.1994) | Acc: (93.28%) (43105/46208)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 370 |  Loss: (0.2003) | Acc: (93.27%) (44293/47488)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 380 |  Loss: (0.2005) | Acc: (93.26%) (45482/48768)\n",
      "#TRAIN: Epoch: 45 | Batch_idx: 390 |  Loss: (0.2017) | Acc: (93.20%) (46602/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4686) | Acc: (86.93%) (8693/10000)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 0 |  Loss: (0.2050) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 10 |  Loss: (0.2065) | Acc: (93.18%) (1312/1408)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 20 |  Loss: (0.1914) | Acc: (93.64%) (2517/2688)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 30 |  Loss: (0.1919) | Acc: (93.67%) (3717/3968)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 40 |  Loss: (0.1934) | Acc: (93.73%) (4919/5248)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 50 |  Loss: (0.1954) | Acc: (93.58%) (6109/6528)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 60 |  Loss: (0.1949) | Acc: (93.52%) (7302/7808)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 70 |  Loss: (0.1963) | Acc: (93.43%) (8491/9088)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 80 |  Loss: (0.1951) | Acc: (93.42%) (9686/10368)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 90 |  Loss: (0.1991) | Acc: (93.23%) (10860/11648)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 100 |  Loss: (0.1967) | Acc: (93.27%) (12058/12928)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 110 |  Loss: (0.1960) | Acc: (93.31%) (13257/14208)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 120 |  Loss: (0.1928) | Acc: (93.41%) (14468/15488)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 130 |  Loss: (0.1918) | Acc: (93.46%) (15671/16768)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 140 |  Loss: (0.1920) | Acc: (93.47%) (16870/18048)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 150 |  Loss: (0.1892) | Acc: (93.56%) (18083/19328)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 160 |  Loss: (0.1912) | Acc: (93.48%) (19264/20608)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 170 |  Loss: (0.1923) | Acc: (93.47%) (20459/21888)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 180 |  Loss: (0.1938) | Acc: (93.40%) (21640/23168)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 190 |  Loss: (0.1944) | Acc: (93.38%) (22830/24448)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 200 |  Loss: (0.1946) | Acc: (93.35%) (24018/25728)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 210 |  Loss: (0.1938) | Acc: (93.39%) (25224/27008)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 220 |  Loss: (0.1943) | Acc: (93.38%) (26416/28288)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 230 |  Loss: (0.1940) | Acc: (93.40%) (27616/29568)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 240 |  Loss: (0.1945) | Acc: (93.39%) (28809/30848)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 250 |  Loss: (0.1944) | Acc: (93.40%) (30009/32128)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 260 |  Loss: (0.1938) | Acc: (93.41%) (31207/33408)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 270 |  Loss: (0.1942) | Acc: (93.37%) (32387/34688)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 280 |  Loss: (0.1950) | Acc: (93.33%) (33568/35968)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 290 |  Loss: (0.1960) | Acc: (93.28%) (34745/37248)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 300 |  Loss: (0.1963) | Acc: (93.25%) (35928/38528)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 310 |  Loss: (0.1953) | Acc: (93.29%) (37138/39808)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 320 |  Loss: (0.1948) | Acc: (93.30%) (38334/41088)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 330 |  Loss: (0.1947) | Acc: (93.30%) (39531/42368)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 340 |  Loss: (0.1955) | Acc: (93.26%) (40707/43648)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 350 |  Loss: (0.1967) | Acc: (93.23%) (41887/44928)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 360 |  Loss: (0.1972) | Acc: (93.21%) (43071/46208)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 370 |  Loss: (0.1969) | Acc: (93.22%) (44266/47488)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 380 |  Loss: (0.1969) | Acc: (93.22%) (45460/48768)\n",
      "#TRAIN: Epoch: 46 | Batch_idx: 390 |  Loss: (0.1971) | Acc: (93.20%) (46602/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4844) | Acc: (86.29%) (8629/10000)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 0 |  Loss: (0.1295) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 10 |  Loss: (0.1407) | Acc: (94.82%) (1335/1408)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 20 |  Loss: (0.1697) | Acc: (94.27%) (2534/2688)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 30 |  Loss: (0.1777) | Acc: (93.95%) (3728/3968)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 40 |  Loss: (0.1818) | Acc: (93.94%) (4930/5248)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 50 |  Loss: (0.1842) | Acc: (93.83%) (6125/6528)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 60 |  Loss: (0.1821) | Acc: (93.85%) (7328/7808)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 70 |  Loss: (0.1840) | Acc: (93.73%) (8518/9088)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 80 |  Loss: (0.1814) | Acc: (93.80%) (9725/10368)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 90 |  Loss: (0.1830) | Acc: (93.69%) (10913/11648)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 100 |  Loss: (0.1800) | Acc: (93.73%) (12117/12928)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 110 |  Loss: (0.1810) | Acc: (93.65%) (13306/14208)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 120 |  Loss: (0.1816) | Acc: (93.64%) (14503/15488)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 130 |  Loss: (0.1820) | Acc: (93.65%) (15704/16768)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 140 |  Loss: (0.1834) | Acc: (93.64%) (16900/18048)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 150 |  Loss: (0.1864) | Acc: (93.56%) (18083/19328)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 160 |  Loss: (0.1875) | Acc: (93.56%) (19281/20608)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 170 |  Loss: (0.1874) | Acc: (93.59%) (20485/21888)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 180 |  Loss: (0.1879) | Acc: (93.62%) (21690/23168)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 190 |  Loss: (0.1872) | Acc: (93.63%) (22890/24448)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 200 |  Loss: (0.1873) | Acc: (93.61%) (24084/25728)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 210 |  Loss: (0.1851) | Acc: (93.71%) (25309/27008)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 220 |  Loss: (0.1860) | Acc: (93.67%) (26498/28288)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 230 |  Loss: (0.1871) | Acc: (93.66%) (27694/29568)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 240 |  Loss: (0.1870) | Acc: (93.64%) (28886/30848)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 250 |  Loss: (0.1873) | Acc: (93.62%) (30077/32128)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 260 |  Loss: (0.1869) | Acc: (93.61%) (31273/33408)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 270 |  Loss: (0.1874) | Acc: (93.60%) (32467/34688)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 280 |  Loss: (0.1874) | Acc: (93.59%) (33664/35968)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 290 |  Loss: (0.1874) | Acc: (93.61%) (34868/37248)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 300 |  Loss: (0.1883) | Acc: (93.59%) (36059/38528)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 310 |  Loss: (0.1875) | Acc: (93.62%) (37267/39808)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 320 |  Loss: (0.1869) | Acc: (93.62%) (38468/41088)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 330 |  Loss: (0.1861) | Acc: (93.65%) (39679/42368)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 340 |  Loss: (0.1863) | Acc: (93.65%) (40876/43648)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 350 |  Loss: (0.1862) | Acc: (93.65%) (42076/44928)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 360 |  Loss: (0.1876) | Acc: (93.61%) (43253/46208)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 370 |  Loss: (0.1882) | Acc: (93.60%) (44449/47488)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 380 |  Loss: (0.1884) | Acc: (93.60%) (45645/48768)\n",
      "#TRAIN: Epoch: 47 | Batch_idx: 390 |  Loss: (0.1893) | Acc: (93.57%) (46785/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4980) | Acc: (86.08%) (8608/10000)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 0 |  Loss: (0.1679) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 10 |  Loss: (0.1849) | Acc: (93.47%) (1316/1408)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 20 |  Loss: (0.1771) | Acc: (93.60%) (2516/2688)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 30 |  Loss: (0.1814) | Acc: (93.55%) (3712/3968)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 40 |  Loss: (0.1840) | Acc: (93.58%) (4911/5248)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 50 |  Loss: (0.1858) | Acc: (93.41%) (6098/6528)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 60 |  Loss: (0.1812) | Acc: (93.43%) (7295/7808)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 70 |  Loss: (0.1788) | Acc: (93.56%) (8503/9088)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 80 |  Loss: (0.1799) | Acc: (93.51%) (9695/10368)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 90 |  Loss: (0.1790) | Acc: (93.58%) (10900/11648)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 100 |  Loss: (0.1813) | Acc: (93.55%) (12094/12928)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 110 |  Loss: (0.1811) | Acc: (93.59%) (13297/14208)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 120 |  Loss: (0.1819) | Acc: (93.52%) (14485/15488)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 130 |  Loss: (0.1813) | Acc: (93.57%) (15689/16768)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 140 |  Loss: (0.1826) | Acc: (93.49%) (16873/18048)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 150 |  Loss: (0.1829) | Acc: (93.49%) (18070/19328)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 160 |  Loss: (0.1836) | Acc: (93.48%) (19264/20608)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 170 |  Loss: (0.1842) | Acc: (93.43%) (20450/21888)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 180 |  Loss: (0.1847) | Acc: (93.44%) (21649/23168)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 190 |  Loss: (0.1852) | Acc: (93.43%) (22841/24448)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 200 |  Loss: (0.1866) | Acc: (93.41%) (24033/25728)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 210 |  Loss: (0.1865) | Acc: (93.44%) (25236/27008)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 220 |  Loss: (0.1875) | Acc: (93.41%) (26423/28288)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 230 |  Loss: (0.1876) | Acc: (93.44%) (27627/29568)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 240 |  Loss: (0.1888) | Acc: (93.39%) (28809/30848)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 250 |  Loss: (0.1883) | Acc: (93.40%) (30007/32128)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 260 |  Loss: (0.1890) | Acc: (93.38%) (31198/33408)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 270 |  Loss: (0.1882) | Acc: (93.41%) (32403/34688)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 280 |  Loss: (0.1882) | Acc: (93.41%) (33598/35968)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 290 |  Loss: (0.1878) | Acc: (93.43%) (34799/37248)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 300 |  Loss: (0.1878) | Acc: (93.43%) (35995/38528)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 310 |  Loss: (0.1869) | Acc: (93.45%) (37201/39808)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 320 |  Loss: (0.1873) | Acc: (93.46%) (38400/41088)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 330 |  Loss: (0.1883) | Acc: (93.43%) (39585/42368)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 340 |  Loss: (0.1879) | Acc: (93.46%) (40793/43648)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 350 |  Loss: (0.1883) | Acc: (93.43%) (41977/44928)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 360 |  Loss: (0.1883) | Acc: (93.43%) (43172/46208)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 370 |  Loss: (0.1872) | Acc: (93.48%) (44390/47488)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 380 |  Loss: (0.1870) | Acc: (93.48%) (45590/48768)\n",
      "#TRAIN: Epoch: 48 | Batch_idx: 390 |  Loss: (0.1870) | Acc: (93.48%) (46740/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5045) | Acc: (85.95%) (8595/10000)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 0 |  Loss: (0.1572) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 10 |  Loss: (0.1686) | Acc: (94.32%) (1328/1408)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 20 |  Loss: (0.1792) | Acc: (93.94%) (2525/2688)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 30 |  Loss: (0.1782) | Acc: (94.08%) (3733/3968)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 40 |  Loss: (0.1887) | Acc: (93.67%) (4916/5248)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 50 |  Loss: (0.1839) | Acc: (93.75%) (6120/6528)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 60 |  Loss: (0.1830) | Acc: (93.80%) (7324/7808)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 70 |  Loss: (0.1830) | Acc: (93.84%) (8528/9088)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 80 |  Loss: (0.1828) | Acc: (93.85%) (9730/10368)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 90 |  Loss: (0.1833) | Acc: (93.83%) (10929/11648)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 100 |  Loss: (0.1803) | Acc: (93.87%) (12136/12928)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 110 |  Loss: (0.1771) | Acc: (94.02%) (13358/14208)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 120 |  Loss: (0.1765) | Acc: (94.02%) (14562/15488)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 130 |  Loss: (0.1777) | Acc: (94.01%) (15763/16768)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 140 |  Loss: (0.1787) | Acc: (93.98%) (16962/18048)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 150 |  Loss: (0.1783) | Acc: (93.99%) (18167/19328)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 160 |  Loss: (0.1764) | Acc: (94.05%) (19381/20608)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 170 |  Loss: (0.1768) | Acc: (94.03%) (20581/21888)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 180 |  Loss: (0.1773) | Acc: (94.01%) (21780/23168)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 190 |  Loss: (0.1756) | Acc: (94.09%) (23002/24448)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 200 |  Loss: (0.1754) | Acc: (94.08%) (24206/25728)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 210 |  Loss: (0.1756) | Acc: (94.05%) (25400/27008)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 220 |  Loss: (0.1762) | Acc: (94.03%) (26600/28288)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 230 |  Loss: (0.1759) | Acc: (94.02%) (27801/29568)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 240 |  Loss: (0.1760) | Acc: (94.03%) (29006/30848)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 250 |  Loss: (0.1755) | Acc: (94.05%) (30216/32128)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 260 |  Loss: (0.1762) | Acc: (94.03%) (31414/33408)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 270 |  Loss: (0.1765) | Acc: (94.05%) (32624/34688)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 280 |  Loss: (0.1762) | Acc: (94.05%) (33829/35968)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 290 |  Loss: (0.1762) | Acc: (94.04%) (35029/37248)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 300 |  Loss: (0.1772) | Acc: (94.02%) (36224/38528)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 310 |  Loss: (0.1777) | Acc: (94.02%) (37426/39808)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 320 |  Loss: (0.1773) | Acc: (94.03%) (38634/41088)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 330 |  Loss: (0.1767) | Acc: (94.04%) (39844/42368)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 340 |  Loss: (0.1773) | Acc: (94.02%) (41037/43648)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 350 |  Loss: (0.1775) | Acc: (94.01%) (42237/44928)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 360 |  Loss: (0.1778) | Acc: (93.99%) (43432/46208)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 370 |  Loss: (0.1775) | Acc: (93.99%) (44633/47488)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 380 |  Loss: (0.1786) | Acc: (93.95%) (45816/48768)\n",
      "#TRAIN: Epoch: 49 | Batch_idx: 390 |  Loss: (0.1787) | Acc: (93.96%) (46980/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4963) | Acc: (86.32%) (8632/10000)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 0 |  Loss: (0.2286) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 10 |  Loss: (0.1749) | Acc: (93.75%) (1320/1408)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 20 |  Loss: (0.1683) | Acc: (93.90%) (2524/2688)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 30 |  Loss: (0.1719) | Acc: (94.08%) (3733/3968)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 40 |  Loss: (0.1706) | Acc: (94.30%) (4949/5248)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 50 |  Loss: (0.1640) | Acc: (94.50%) (6169/6528)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 60 |  Loss: (0.1657) | Acc: (94.35%) (7367/7808)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 70 |  Loss: (0.1653) | Acc: (94.41%) (8580/9088)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 80 |  Loss: (0.1670) | Acc: (94.33%) (9780/10368)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 90 |  Loss: (0.1719) | Acc: (94.17%) (10969/11648)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 100 |  Loss: (0.1708) | Acc: (94.20%) (12178/12928)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 110 |  Loss: (0.1723) | Acc: (94.21%) (13385/14208)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 120 |  Loss: (0.1706) | Acc: (94.27%) (14601/15488)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 130 |  Loss: (0.1698) | Acc: (94.30%) (15813/16768)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 140 |  Loss: (0.1718) | Acc: (94.24%) (17008/18048)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 150 |  Loss: (0.1721) | Acc: (94.22%) (18210/19328)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 160 |  Loss: (0.1735) | Acc: (94.17%) (19407/20608)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 170 |  Loss: (0.1737) | Acc: (94.16%) (20609/21888)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 180 |  Loss: (0.1725) | Acc: (94.19%) (21822/23168)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 190 |  Loss: (0.1732) | Acc: (94.15%) (23017/24448)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 200 |  Loss: (0.1728) | Acc: (94.15%) (24223/25728)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 210 |  Loss: (0.1735) | Acc: (94.09%) (25413/27008)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 220 |  Loss: (0.1730) | Acc: (94.11%) (26622/28288)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 230 |  Loss: (0.1736) | Acc: (94.11%) (27826/29568)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 240 |  Loss: (0.1729) | Acc: (94.14%) (29040/30848)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 250 |  Loss: (0.1726) | Acc: (94.13%) (30242/32128)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 260 |  Loss: (0.1717) | Acc: (94.18%) (31463/33408)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 270 |  Loss: (0.1719) | Acc: (94.18%) (32670/34688)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 280 |  Loss: (0.1718) | Acc: (94.18%) (33873/35968)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 290 |  Loss: (0.1728) | Acc: (94.14%) (35064/37248)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 300 |  Loss: (0.1724) | Acc: (94.13%) (36267/38528)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 310 |  Loss: (0.1735) | Acc: (94.12%) (37468/39808)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 320 |  Loss: (0.1733) | Acc: (94.13%) (38677/41088)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 330 |  Loss: (0.1735) | Acc: (94.13%) (39879/42368)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 340 |  Loss: (0.1734) | Acc: (94.12%) (41082/43648)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 350 |  Loss: (0.1740) | Acc: (94.10%) (42278/44928)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 360 |  Loss: (0.1746) | Acc: (94.09%) (43478/46208)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 370 |  Loss: (0.1751) | Acc: (94.07%) (44673/47488)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 380 |  Loss: (0.1750) | Acc: (94.07%) (45878/48768)\n",
      "#TRAIN: Epoch: 50 | Batch_idx: 390 |  Loss: (0.1756) | Acc: (94.04%) (47019/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5211) | Acc: (86.02%) (8602/10000)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 0 |  Loss: (0.1181) | Acc: (95.31%) (122/128)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 10 |  Loss: (0.1586) | Acc: (94.46%) (1330/1408)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 20 |  Loss: (0.1649) | Acc: (94.20%) (2532/2688)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 30 |  Loss: (0.1668) | Acc: (94.23%) (3739/3968)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 40 |  Loss: (0.1727) | Acc: (94.07%) (4937/5248)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 50 |  Loss: (0.1674) | Acc: (94.29%) (6155/6528)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 60 |  Loss: (0.1673) | Acc: (94.34%) (7366/7808)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 70 |  Loss: (0.1687) | Acc: (94.32%) (8572/9088)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 80 |  Loss: (0.1658) | Acc: (94.45%) (9793/10368)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 90 |  Loss: (0.1627) | Acc: (94.60%) (11019/11648)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 100 |  Loss: (0.1632) | Acc: (94.58%) (12227/12928)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 110 |  Loss: (0.1652) | Acc: (94.55%) (13433/14208)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 120 |  Loss: (0.1659) | Acc: (94.52%) (14640/15488)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 130 |  Loss: (0.1655) | Acc: (94.57%) (15857/16768)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 140 |  Loss: (0.1651) | Acc: (94.59%) (17071/18048)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 150 |  Loss: (0.1663) | Acc: (94.55%) (18275/19328)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 160 |  Loss: (0.1671) | Acc: (94.56%) (19487/20608)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 170 |  Loss: (0.1665) | Acc: (94.58%) (20702/21888)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 180 |  Loss: (0.1655) | Acc: (94.62%) (21922/23168)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 190 |  Loss: (0.1647) | Acc: (94.62%) (23132/24448)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 200 |  Loss: (0.1654) | Acc: (94.57%) (24330/25728)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 210 |  Loss: (0.1668) | Acc: (94.50%) (25523/27008)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 220 |  Loss: (0.1676) | Acc: (94.47%) (26724/28288)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 230 |  Loss: (0.1679) | Acc: (94.44%) (27925/29568)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 240 |  Loss: (0.1673) | Acc: (94.47%) (29141/30848)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 250 |  Loss: (0.1660) | Acc: (94.48%) (30354/32128)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 260 |  Loss: (0.1660) | Acc: (94.47%) (31559/33408)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 270 |  Loss: (0.1656) | Acc: (94.50%) (32781/34688)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 280 |  Loss: (0.1660) | Acc: (94.50%) (33988/35968)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 290 |  Loss: (0.1659) | Acc: (94.49%) (35197/37248)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 300 |  Loss: (0.1660) | Acc: (94.47%) (36397/38528)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 310 |  Loss: (0.1665) | Acc: (94.45%) (37600/39808)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 320 |  Loss: (0.1673) | Acc: (94.42%) (38797/41088)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 330 |  Loss: (0.1675) | Acc: (94.41%) (39999/42368)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 340 |  Loss: (0.1670) | Acc: (94.43%) (41216/43648)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 350 |  Loss: (0.1679) | Acc: (94.39%) (42406/44928)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 360 |  Loss: (0.1679) | Acc: (94.39%) (43617/46208)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 370 |  Loss: (0.1682) | Acc: (94.38%) (44819/47488)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 380 |  Loss: (0.1683) | Acc: (94.36%) (46016/48768)\n",
      "#TRAIN: Epoch: 51 | Batch_idx: 390 |  Loss: (0.1689) | Acc: (94.34%) (47170/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5139) | Acc: (86.44%) (8644/10000)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 0 |  Loss: (0.1991) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 10 |  Loss: (0.1447) | Acc: (95.03%) (1338/1408)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 20 |  Loss: (0.1358) | Acc: (95.65%) (2571/2688)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 30 |  Loss: (0.1494) | Acc: (94.98%) (3769/3968)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 40 |  Loss: (0.1534) | Acc: (94.89%) (4980/5248)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 50 |  Loss: (0.1573) | Acc: (94.72%) (6183/6528)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 60 |  Loss: (0.1616) | Acc: (94.65%) (7390/7808)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 70 |  Loss: (0.1616) | Acc: (94.64%) (8601/9088)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 80 |  Loss: (0.1614) | Acc: (94.63%) (9811/10368)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 90 |  Loss: (0.1618) | Acc: (94.57%) (11015/11648)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 100 |  Loss: (0.1637) | Acc: (94.52%) (12220/12928)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 110 |  Loss: (0.1606) | Acc: (94.62%) (13443/14208)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 120 |  Loss: (0.1609) | Acc: (94.62%) (14655/15488)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 130 |  Loss: (0.1609) | Acc: (94.61%) (15865/16768)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 140 |  Loss: (0.1649) | Acc: (94.49%) (17053/18048)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 150 |  Loss: (0.1653) | Acc: (94.48%) (18261/19328)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 160 |  Loss: (0.1651) | Acc: (94.53%) (19481/20608)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 170 |  Loss: (0.1653) | Acc: (94.51%) (20686/21888)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 180 |  Loss: (0.1664) | Acc: (94.44%) (21880/23168)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 190 |  Loss: (0.1664) | Acc: (94.45%) (23090/24448)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 200 |  Loss: (0.1674) | Acc: (94.39%) (24284/25728)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 210 |  Loss: (0.1675) | Acc: (94.38%) (25490/27008)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 220 |  Loss: (0.1679) | Acc: (94.37%) (26696/28288)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 230 |  Loss: (0.1674) | Acc: (94.38%) (27907/29568)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 240 |  Loss: (0.1676) | Acc: (94.38%) (29114/30848)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 250 |  Loss: (0.1671) | Acc: (94.38%) (30324/32128)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 260 |  Loss: (0.1667) | Acc: (94.40%) (31536/33408)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 270 |  Loss: (0.1679) | Acc: (94.35%) (32728/34688)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 280 |  Loss: (0.1687) | Acc: (94.33%) (33930/35968)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 290 |  Loss: (0.1688) | Acc: (94.34%) (35139/37248)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 300 |  Loss: (0.1688) | Acc: (94.35%) (36350/38528)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 310 |  Loss: (0.1695) | Acc: (94.33%) (37549/39808)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 320 |  Loss: (0.1698) | Acc: (94.30%) (38745/41088)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 330 |  Loss: (0.1703) | Acc: (94.29%) (39950/42368)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 340 |  Loss: (0.1709) | Acc: (94.27%) (41146/43648)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 350 |  Loss: (0.1711) | Acc: (94.24%) (42341/44928)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 360 |  Loss: (0.1709) | Acc: (94.24%) (43547/46208)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 370 |  Loss: (0.1705) | Acc: (94.26%) (44760/47488)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 380 |  Loss: (0.1704) | Acc: (94.27%) (45972/48768)\n",
      "#TRAIN: Epoch: 52 | Batch_idx: 390 |  Loss: (0.1704) | Acc: (94.27%) (47133/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4916) | Acc: (86.69%) (8669/10000)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 0 |  Loss: (0.1672) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 10 |  Loss: (0.1429) | Acc: (95.45%) (1344/1408)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 20 |  Loss: (0.1592) | Acc: (95.01%) (2554/2688)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 30 |  Loss: (0.1570) | Acc: (95.11%) (3774/3968)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 40 |  Loss: (0.1602) | Acc: (94.89%) (4980/5248)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 50 |  Loss: (0.1628) | Acc: (94.81%) (6189/6528)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 60 |  Loss: (0.1629) | Acc: (94.74%) (7397/7808)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 70 |  Loss: (0.1618) | Acc: (94.70%) (8606/9088)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 80 |  Loss: (0.1621) | Acc: (94.62%) (9810/10368)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 90 |  Loss: (0.1635) | Acc: (94.59%) (11018/11648)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 100 |  Loss: (0.1650) | Acc: (94.57%) (12226/12928)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 110 |  Loss: (0.1658) | Acc: (94.55%) (13433/14208)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 120 |  Loss: (0.1638) | Acc: (94.62%) (14655/15488)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 130 |  Loss: (0.1637) | Acc: (94.66%) (15872/16768)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 140 |  Loss: (0.1635) | Acc: (94.61%) (17076/18048)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 150 |  Loss: (0.1636) | Acc: (94.62%) (18288/19328)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 160 |  Loss: (0.1639) | Acc: (94.64%) (19504/20608)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 170 |  Loss: (0.1632) | Acc: (94.68%) (20724/21888)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 180 |  Loss: (0.1639) | Acc: (94.68%) (21935/23168)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 190 |  Loss: (0.1625) | Acc: (94.71%) (23155/24448)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 200 |  Loss: (0.1627) | Acc: (94.68%) (24360/25728)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 210 |  Loss: (0.1617) | Acc: (94.68%) (25570/27008)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 220 |  Loss: (0.1626) | Acc: (94.67%) (26779/28288)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 230 |  Loss: (0.1631) | Acc: (94.63%) (27980/29568)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 240 |  Loss: (0.1626) | Acc: (94.60%) (29183/30848)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 250 |  Loss: (0.1619) | Acc: (94.61%) (30397/32128)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 260 |  Loss: (0.1621) | Acc: (94.60%) (31603/33408)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 270 |  Loss: (0.1616) | Acc: (94.61%) (32819/34688)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 280 |  Loss: (0.1613) | Acc: (94.61%) (34030/35968)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 290 |  Loss: (0.1615) | Acc: (94.62%) (35243/37248)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 300 |  Loss: (0.1613) | Acc: (94.60%) (36448/38528)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 310 |  Loss: (0.1619) | Acc: (94.55%) (37640/39808)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 320 |  Loss: (0.1618) | Acc: (94.56%) (38852/41088)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 330 |  Loss: (0.1611) | Acc: (94.58%) (40073/42368)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 340 |  Loss: (0.1619) | Acc: (94.54%) (41267/43648)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 350 |  Loss: (0.1617) | Acc: (94.56%) (42484/44928)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 360 |  Loss: (0.1610) | Acc: (94.58%) (43703/46208)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 370 |  Loss: (0.1615) | Acc: (94.57%) (44911/47488)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 380 |  Loss: (0.1606) | Acc: (94.61%) (46140/48768)\n",
      "#TRAIN: Epoch: 53 | Batch_idx: 390 |  Loss: (0.1606) | Acc: (94.59%) (47297/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5113) | Acc: (86.46%) (8646/10000)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 0 |  Loss: (0.1336) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 10 |  Loss: (0.1240) | Acc: (95.88%) (1350/1408)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 20 |  Loss: (0.1543) | Acc: (94.83%) (2549/2688)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 30 |  Loss: (0.1608) | Acc: (94.58%) (3753/3968)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 40 |  Loss: (0.1610) | Acc: (94.59%) (4964/5248)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 50 |  Loss: (0.1615) | Acc: (94.53%) (6171/6528)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 60 |  Loss: (0.1599) | Acc: (94.57%) (7384/7808)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 70 |  Loss: (0.1615) | Acc: (94.54%) (8592/9088)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 80 |  Loss: (0.1572) | Acc: (94.66%) (9814/10368)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 90 |  Loss: (0.1557) | Acc: (94.69%) (11030/11648)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 100 |  Loss: (0.1574) | Acc: (94.68%) (12240/12928)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 110 |  Loss: (0.1565) | Acc: (94.71%) (13456/14208)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 120 |  Loss: (0.1563) | Acc: (94.71%) (14668/15488)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 130 |  Loss: (0.1559) | Acc: (94.72%) (15883/16768)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 140 |  Loss: (0.1566) | Acc: (94.70%) (17091/18048)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 150 |  Loss: (0.1571) | Acc: (94.65%) (18293/19328)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 160 |  Loss: (0.1577) | Acc: (94.65%) (19505/20608)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 170 |  Loss: (0.1577) | Acc: (94.66%) (20719/21888)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 180 |  Loss: (0.1575) | Acc: (94.62%) (21921/23168)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 190 |  Loss: (0.1573) | Acc: (94.61%) (23131/24448)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 200 |  Loss: (0.1574) | Acc: (94.58%) (24334/25728)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 210 |  Loss: (0.1573) | Acc: (94.56%) (25539/27008)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 220 |  Loss: (0.1574) | Acc: (94.58%) (26754/28288)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 230 |  Loss: (0.1574) | Acc: (94.59%) (27968/29568)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 240 |  Loss: (0.1570) | Acc: (94.61%) (29186/30848)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 250 |  Loss: (0.1569) | Acc: (94.59%) (30390/32128)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 260 |  Loss: (0.1573) | Acc: (94.59%) (31599/33408)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 270 |  Loss: (0.1568) | Acc: (94.58%) (32808/34688)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 280 |  Loss: (0.1579) | Acc: (94.53%) (34002/35968)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 290 |  Loss: (0.1580) | Acc: (94.53%) (35209/37248)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 300 |  Loss: (0.1583) | Acc: (94.53%) (36422/38528)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 310 |  Loss: (0.1575) | Acc: (94.56%) (37644/39808)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 320 |  Loss: (0.1580) | Acc: (94.55%) (38849/41088)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 330 |  Loss: (0.1589) | Acc: (94.52%) (40048/42368)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 340 |  Loss: (0.1582) | Acc: (94.55%) (41269/43648)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 350 |  Loss: (0.1585) | Acc: (94.55%) (42480/44928)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 360 |  Loss: (0.1589) | Acc: (94.53%) (43680/46208)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 370 |  Loss: (0.1587) | Acc: (94.53%) (44892/47488)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 380 |  Loss: (0.1597) | Acc: (94.51%) (46093/48768)\n",
      "#TRAIN: Epoch: 54 | Batch_idx: 390 |  Loss: (0.1604) | Acc: (94.50%) (47250/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4929) | Acc: (86.68%) (8668/10000)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 0 |  Loss: (0.1455) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 10 |  Loss: (0.1505) | Acc: (94.89%) (1336/1408)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 20 |  Loss: (0.1487) | Acc: (95.01%) (2554/2688)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 30 |  Loss: (0.1562) | Acc: (94.83%) (3763/3968)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 40 |  Loss: (0.1565) | Acc: (94.74%) (4972/5248)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 50 |  Loss: (0.1510) | Acc: (94.98%) (6200/6528)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 60 |  Loss: (0.1487) | Acc: (95.13%) (7428/7808)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 70 |  Loss: (0.1501) | Acc: (95.04%) (8637/9088)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 80 |  Loss: (0.1508) | Acc: (95.00%) (9850/10368)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 90 |  Loss: (0.1532) | Acc: (94.98%) (11063/11648)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 100 |  Loss: (0.1520) | Acc: (95.02%) (12284/12928)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 110 |  Loss: (0.1518) | Acc: (95.00%) (13498/14208)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 120 |  Loss: (0.1507) | Acc: (95.07%) (14725/15488)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 130 |  Loss: (0.1505) | Acc: (95.09%) (15944/16768)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 140 |  Loss: (0.1506) | Acc: (95.07%) (17159/18048)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 150 |  Loss: (0.1510) | Acc: (94.98%) (18358/19328)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 160 |  Loss: (0.1512) | Acc: (94.98%) (19574/20608)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 170 |  Loss: (0.1516) | Acc: (94.94%) (20780/21888)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 180 |  Loss: (0.1537) | Acc: (94.85%) (21975/23168)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 190 |  Loss: (0.1527) | Acc: (94.90%) (23202/24448)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 200 |  Loss: (0.1524) | Acc: (94.90%) (24417/25728)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 210 |  Loss: (0.1528) | Acc: (94.89%) (25628/27008)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 220 |  Loss: (0.1540) | Acc: (94.81%) (26819/28288)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 230 |  Loss: (0.1534) | Acc: (94.82%) (28036/29568)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 240 |  Loss: (0.1525) | Acc: (94.86%) (29263/30848)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 250 |  Loss: (0.1530) | Acc: (94.85%) (30474/32128)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 260 |  Loss: (0.1539) | Acc: (94.79%) (31668/33408)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 270 |  Loss: (0.1546) | Acc: (94.78%) (32878/34688)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 280 |  Loss: (0.1551) | Acc: (94.76%) (34083/35968)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 290 |  Loss: (0.1567) | Acc: (94.70%) (35273/37248)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 300 |  Loss: (0.1567) | Acc: (94.71%) (36488/38528)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 310 |  Loss: (0.1566) | Acc: (94.70%) (37699/39808)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 320 |  Loss: (0.1580) | Acc: (94.65%) (38888/41088)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 330 |  Loss: (0.1582) | Acc: (94.63%) (40093/42368)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 340 |  Loss: (0.1580) | Acc: (94.64%) (41308/43648)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 350 |  Loss: (0.1578) | Acc: (94.64%) (42518/44928)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 360 |  Loss: (0.1579) | Acc: (94.64%) (43730/46208)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 370 |  Loss: (0.1576) | Acc: (94.64%) (44941/47488)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 380 |  Loss: (0.1579) | Acc: (94.63%) (46151/48768)\n",
      "#TRAIN: Epoch: 55 | Batch_idx: 390 |  Loss: (0.1576) | Acc: (94.64%) (47322/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4941) | Acc: (86.62%) (8662/10000)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 0 |  Loss: (0.1499) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 10 |  Loss: (0.1635) | Acc: (94.39%) (1329/1408)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 20 |  Loss: (0.1463) | Acc: (94.72%) (2546/2688)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 30 |  Loss: (0.1498) | Acc: (94.83%) (3763/3968)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 40 |  Loss: (0.1470) | Acc: (95.01%) (4986/5248)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 50 |  Loss: (0.1464) | Acc: (94.93%) (6197/6528)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 60 |  Loss: (0.1492) | Acc: (94.71%) (7395/7808)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 70 |  Loss: (0.1463) | Acc: (94.94%) (8628/9088)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 80 |  Loss: (0.1466) | Acc: (94.92%) (9841/10368)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 90 |  Loss: (0.1480) | Acc: (94.82%) (11045/11648)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 100 |  Loss: (0.1480) | Acc: (94.83%) (12259/12928)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 110 |  Loss: (0.1472) | Acc: (94.84%) (13475/14208)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 120 |  Loss: (0.1490) | Acc: (94.82%) (14685/15488)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 130 |  Loss: (0.1489) | Acc: (94.79%) (15894/16768)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 140 |  Loss: (0.1496) | Acc: (94.74%) (17098/18048)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 150 |  Loss: (0.1494) | Acc: (94.72%) (18308/19328)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 160 |  Loss: (0.1495) | Acc: (94.70%) (19515/20608)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 170 |  Loss: (0.1505) | Acc: (94.68%) (20724/21888)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 180 |  Loss: (0.1494) | Acc: (94.70%) (21940/23168)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 190 |  Loss: (0.1477) | Acc: (94.78%) (23171/24448)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 200 |  Loss: (0.1488) | Acc: (94.71%) (24368/25728)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 210 |  Loss: (0.1495) | Acc: (94.72%) (25582/27008)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 220 |  Loss: (0.1487) | Acc: (94.75%) (26803/28288)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 230 |  Loss: (0.1498) | Acc: (94.74%) (28013/29568)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 240 |  Loss: (0.1503) | Acc: (94.74%) (29224/30848)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 250 |  Loss: (0.1500) | Acc: (94.75%) (30442/32128)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 260 |  Loss: (0.1503) | Acc: (94.75%) (31653/33408)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 270 |  Loss: (0.1511) | Acc: (94.75%) (32868/34688)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 280 |  Loss: (0.1516) | Acc: (94.73%) (34073/35968)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 290 |  Loss: (0.1513) | Acc: (94.75%) (35294/37248)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 300 |  Loss: (0.1514) | Acc: (94.76%) (36509/38528)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 310 |  Loss: (0.1512) | Acc: (94.75%) (37717/39808)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 320 |  Loss: (0.1516) | Acc: (94.73%) (38922/41088)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 330 |  Loss: (0.1521) | Acc: (94.71%) (40126/42368)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 340 |  Loss: (0.1531) | Acc: (94.68%) (41327/43648)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 350 |  Loss: (0.1529) | Acc: (94.70%) (42545/44928)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 360 |  Loss: (0.1533) | Acc: (94.68%) (43752/46208)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 370 |  Loss: (0.1536) | Acc: (94.69%) (44968/47488)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 380 |  Loss: (0.1540) | Acc: (94.70%) (46181/48768)\n",
      "#TRAIN: Epoch: 56 | Batch_idx: 390 |  Loss: (0.1543) | Acc: (94.69%) (47345/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5620) | Acc: (85.67%) (8567/10000)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 0 |  Loss: (0.1407) | Acc: (96.88%) (124/128)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 10 |  Loss: (0.1511) | Acc: (94.89%) (1336/1408)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 20 |  Loss: (0.1434) | Acc: (95.09%) (2556/2688)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 30 |  Loss: (0.1487) | Acc: (94.61%) (3754/3968)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 40 |  Loss: (0.1426) | Acc: (94.99%) (4985/5248)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 50 |  Loss: (0.1411) | Acc: (95.16%) (6212/6528)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 60 |  Loss: (0.1410) | Acc: (95.07%) (7423/7808)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 70 |  Loss: (0.1375) | Acc: (95.22%) (8654/9088)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 80 |  Loss: (0.1348) | Acc: (95.35%) (9886/10368)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 90 |  Loss: (0.1341) | Acc: (95.40%) (11112/11648)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 100 |  Loss: (0.1352) | Acc: (95.41%) (12334/12928)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 110 |  Loss: (0.1385) | Acc: (95.28%) (13538/14208)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 120 |  Loss: (0.1384) | Acc: (95.25%) (14753/15488)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 130 |  Loss: (0.1383) | Acc: (95.27%) (15975/16768)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 140 |  Loss: (0.1399) | Acc: (95.25%) (17190/18048)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 150 |  Loss: (0.1412) | Acc: (95.20%) (18400/19328)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 160 |  Loss: (0.1428) | Acc: (95.13%) (19605/20608)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 170 |  Loss: (0.1424) | Acc: (95.12%) (20820/21888)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 180 |  Loss: (0.1422) | Acc: (95.16%) (22046/23168)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 190 |  Loss: (0.1432) | Acc: (95.10%) (23250/24448)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 200 |  Loss: (0.1434) | Acc: (95.08%) (24463/25728)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 210 |  Loss: (0.1438) | Acc: (95.07%) (25676/27008)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 220 |  Loss: (0.1452) | Acc: (95.02%) (26878/28288)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 230 |  Loss: (0.1456) | Acc: (95.01%) (28093/29568)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 240 |  Loss: (0.1455) | Acc: (95.01%) (29310/30848)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 250 |  Loss: (0.1458) | Acc: (95.00%) (30520/32128)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 260 |  Loss: (0.1465) | Acc: (94.97%) (31729/33408)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 270 |  Loss: (0.1467) | Acc: (94.98%) (32947/34688)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 280 |  Loss: (0.1463) | Acc: (94.98%) (34161/35968)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 290 |  Loss: (0.1467) | Acc: (94.97%) (35374/37248)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 300 |  Loss: (0.1473) | Acc: (94.96%) (36586/38528)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 310 |  Loss: (0.1482) | Acc: (94.93%) (37791/39808)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 320 |  Loss: (0.1486) | Acc: (94.94%) (39007/41088)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 330 |  Loss: (0.1483) | Acc: (94.95%) (40227/42368)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 340 |  Loss: (0.1480) | Acc: (94.96%) (41450/43648)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 350 |  Loss: (0.1485) | Acc: (94.97%) (42668/44928)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 360 |  Loss: (0.1487) | Acc: (94.96%) (43879/46208)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 370 |  Loss: (0.1504) | Acc: (94.91%) (45072/47488)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 380 |  Loss: (0.1502) | Acc: (94.91%) (46287/48768)\n",
      "#TRAIN: Epoch: 57 | Batch_idx: 390 |  Loss: (0.1502) | Acc: (94.89%) (47444/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5195) | Acc: (86.37%) (8637/10000)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 0 |  Loss: (0.1176) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 10 |  Loss: (0.1483) | Acc: (94.39%) (1329/1408)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 20 |  Loss: (0.1552) | Acc: (94.27%) (2534/2688)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 30 |  Loss: (0.1436) | Acc: (94.81%) (3762/3968)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 40 |  Loss: (0.1442) | Acc: (94.95%) (4983/5248)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 50 |  Loss: (0.1485) | Acc: (94.85%) (6192/6528)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 60 |  Loss: (0.1487) | Acc: (94.83%) (7404/7808)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 70 |  Loss: (0.1460) | Acc: (94.91%) (8625/9088)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 80 |  Loss: (0.1480) | Acc: (94.89%) (9838/10368)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 90 |  Loss: (0.1469) | Acc: (94.98%) (11063/11648)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 100 |  Loss: (0.1453) | Acc: (95.03%) (12286/12928)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 110 |  Loss: (0.1456) | Acc: (95.00%) (13497/14208)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 120 |  Loss: (0.1441) | Acc: (95.11%) (14730/15488)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 130 |  Loss: (0.1432) | Acc: (95.13%) (15952/16768)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 140 |  Loss: (0.1437) | Acc: (95.10%) (17163/18048)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 150 |  Loss: (0.1446) | Acc: (95.05%) (18372/19328)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 160 |  Loss: (0.1450) | Acc: (95.03%) (19584/20608)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 170 |  Loss: (0.1434) | Acc: (95.08%) (20812/21888)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 180 |  Loss: (0.1440) | Acc: (95.08%) (22027/23168)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 190 |  Loss: (0.1439) | Acc: (95.07%) (23242/24448)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 200 |  Loss: (0.1454) | Acc: (95.03%) (24449/25728)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 210 |  Loss: (0.1453) | Acc: (95.03%) (25667/27008)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 220 |  Loss: (0.1449) | Acc: (95.05%) (26888/28288)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 230 |  Loss: (0.1443) | Acc: (95.05%) (28105/29568)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 240 |  Loss: (0.1440) | Acc: (95.08%) (29329/30848)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 250 |  Loss: (0.1434) | Acc: (95.10%) (30555/32128)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 260 |  Loss: (0.1432) | Acc: (95.09%) (31769/33408)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 270 |  Loss: (0.1421) | Acc: (95.13%) (32999/34688)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 280 |  Loss: (0.1424) | Acc: (95.14%) (34220/35968)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 290 |  Loss: (0.1427) | Acc: (95.11%) (35426/37248)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 300 |  Loss: (0.1435) | Acc: (95.11%) (36643/38528)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 310 |  Loss: (0.1436) | Acc: (95.12%) (37864/39808)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 320 |  Loss: (0.1436) | Acc: (95.13%) (39089/41088)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 330 |  Loss: (0.1440) | Acc: (95.14%) (40310/42368)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 340 |  Loss: (0.1444) | Acc: (95.15%) (41532/43648)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 350 |  Loss: (0.1436) | Acc: (95.18%) (42761/44928)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 360 |  Loss: (0.1434) | Acc: (95.17%) (43977/46208)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 370 |  Loss: (0.1431) | Acc: (95.18%) (45200/47488)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 380 |  Loss: (0.1432) | Acc: (95.17%) (46414/48768)\n",
      "#TRAIN: Epoch: 58 | Batch_idx: 390 |  Loss: (0.1432) | Acc: (95.17%) (47585/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5192) | Acc: (86.71%) (8671/10000)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 0 |  Loss: (0.1811) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 10 |  Loss: (0.1292) | Acc: (95.45%) (1344/1408)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 20 |  Loss: (0.1263) | Acc: (95.65%) (2571/2688)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 30 |  Loss: (0.1257) | Acc: (95.82%) (3802/3968)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 40 |  Loss: (0.1286) | Acc: (95.66%) (5020/5248)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 50 |  Loss: (0.1299) | Acc: (95.48%) (6233/6528)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 60 |  Loss: (0.1329) | Acc: (95.25%) (7437/7808)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 70 |  Loss: (0.1369) | Acc: (95.10%) (8643/9088)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 80 |  Loss: (0.1390) | Acc: (95.08%) (9858/10368)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 90 |  Loss: (0.1384) | Acc: (95.07%) (11074/11648)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 100 |  Loss: (0.1378) | Acc: (95.16%) (12302/12928)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 110 |  Loss: (0.1376) | Acc: (95.19%) (13524/14208)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 120 |  Loss: (0.1349) | Acc: (95.24%) (14751/15488)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 130 |  Loss: (0.1359) | Acc: (95.26%) (15973/16768)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 140 |  Loss: (0.1347) | Acc: (95.31%) (17201/18048)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 150 |  Loss: (0.1345) | Acc: (95.34%) (18427/19328)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 160 |  Loss: (0.1334) | Acc: (95.39%) (19657/20608)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 170 |  Loss: (0.1328) | Acc: (95.40%) (20881/21888)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 180 |  Loss: (0.1336) | Acc: (95.38%) (22097/23168)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 190 |  Loss: (0.1338) | Acc: (95.38%) (23318/24448)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 200 |  Loss: (0.1355) | Acc: (95.33%) (24527/25728)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 210 |  Loss: (0.1371) | Acc: (95.31%) (25741/27008)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 220 |  Loss: (0.1371) | Acc: (95.31%) (26961/28288)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 230 |  Loss: (0.1378) | Acc: (95.30%) (28178/29568)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 240 |  Loss: (0.1378) | Acc: (95.30%) (29398/30848)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 250 |  Loss: (0.1376) | Acc: (95.30%) (30617/32128)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 260 |  Loss: (0.1374) | Acc: (95.31%) (31841/33408)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 270 |  Loss: (0.1374) | Acc: (95.31%) (33062/34688)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 280 |  Loss: (0.1387) | Acc: (95.26%) (34262/35968)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 290 |  Loss: (0.1381) | Acc: (95.29%) (35492/37248)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 300 |  Loss: (0.1380) | Acc: (95.30%) (36716/38528)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 310 |  Loss: (0.1383) | Acc: (95.29%) (37934/39808)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 320 |  Loss: (0.1384) | Acc: (95.29%) (39154/41088)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 330 |  Loss: (0.1383) | Acc: (95.30%) (40376/42368)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 340 |  Loss: (0.1376) | Acc: (95.31%) (41600/43648)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 350 |  Loss: (0.1383) | Acc: (95.29%) (42811/44928)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 360 |  Loss: (0.1378) | Acc: (95.30%) (44037/46208)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 370 |  Loss: (0.1375) | Acc: (95.30%) (45258/47488)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 380 |  Loss: (0.1384) | Acc: (95.29%) (46469/48768)\n",
      "#TRAIN: Epoch: 59 | Batch_idx: 390 |  Loss: (0.1387) | Acc: (95.27%) (47636/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5186) | Acc: (86.58%) (8658/10000)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 0 |  Loss: (0.1439) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 10 |  Loss: (0.1245) | Acc: (95.31%) (1342/1408)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 20 |  Loss: (0.1505) | Acc: (94.87%) (2550/2688)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 30 |  Loss: (0.1469) | Acc: (94.91%) (3766/3968)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 40 |  Loss: (0.1459) | Acc: (94.87%) (4979/5248)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 50 |  Loss: (0.1429) | Acc: (95.05%) (6205/6528)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 60 |  Loss: (0.1406) | Acc: (95.13%) (7428/7808)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 70 |  Loss: (0.1351) | Acc: (95.27%) (8658/9088)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 80 |  Loss: (0.1330) | Acc: (95.36%) (9887/10368)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 90 |  Loss: (0.1345) | Acc: (95.30%) (11100/11648)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 100 |  Loss: (0.1369) | Acc: (95.20%) (12307/12928)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 110 |  Loss: (0.1344) | Acc: (95.28%) (13538/14208)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 120 |  Loss: (0.1344) | Acc: (95.30%) (14760/15488)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 130 |  Loss: (0.1320) | Acc: (95.38%) (15993/16768)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 140 |  Loss: (0.1321) | Acc: (95.36%) (17211/18048)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 150 |  Loss: (0.1307) | Acc: (95.38%) (18435/19328)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 160 |  Loss: (0.1306) | Acc: (95.38%) (19656/20608)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 170 |  Loss: (0.1305) | Acc: (95.41%) (20883/21888)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 180 |  Loss: (0.1314) | Acc: (95.36%) (22094/23168)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 190 |  Loss: (0.1324) | Acc: (95.35%) (23310/24448)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 200 |  Loss: (0.1322) | Acc: (95.35%) (24531/25728)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 210 |  Loss: (0.1316) | Acc: (95.38%) (25761/27008)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 220 |  Loss: (0.1307) | Acc: (95.45%) (27001/28288)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 230 |  Loss: (0.1309) | Acc: (95.45%) (28223/29568)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 240 |  Loss: (0.1310) | Acc: (95.46%) (29448/30848)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 250 |  Loss: (0.1315) | Acc: (95.46%) (30668/32128)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 260 |  Loss: (0.1319) | Acc: (95.47%) (31895/33408)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 270 |  Loss: (0.1316) | Acc: (95.49%) (33122/34688)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 280 |  Loss: (0.1321) | Acc: (95.46%) (34336/35968)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 290 |  Loss: (0.1321) | Acc: (95.43%) (35546/37248)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 300 |  Loss: (0.1327) | Acc: (95.41%) (36760/38528)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 310 |  Loss: (0.1323) | Acc: (95.42%) (37986/39808)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 320 |  Loss: (0.1322) | Acc: (95.43%) (39211/41088)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 330 |  Loss: (0.1323) | Acc: (95.43%) (40433/42368)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 340 |  Loss: (0.1330) | Acc: (95.40%) (41639/43648)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 350 |  Loss: (0.1329) | Acc: (95.42%) (42869/44928)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 360 |  Loss: (0.1329) | Acc: (95.41%) (44089/46208)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 370 |  Loss: (0.1332) | Acc: (95.39%) (45297/47488)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 380 |  Loss: (0.1338) | Acc: (95.36%) (46505/48768)\n",
      "#TRAIN: Epoch: 60 | Batch_idx: 390 |  Loss: (0.1341) | Acc: (95.34%) (47672/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5332) | Acc: (86.59%) (8659/10000)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 0 |  Loss: (0.1978) | Acc: (93.75%) (120/128)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 10 |  Loss: (0.1533) | Acc: (94.74%) (1334/1408)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 20 |  Loss: (0.1571) | Acc: (94.53%) (2541/2688)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 30 |  Loss: (0.1543) | Acc: (94.68%) (3757/3968)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 40 |  Loss: (0.1511) | Acc: (94.82%) (4976/5248)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 50 |  Loss: (0.1483) | Acc: (94.79%) (6188/6528)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 60 |  Loss: (0.1448) | Acc: (95.04%) (7421/7808)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 70 |  Loss: (0.1453) | Acc: (94.98%) (8632/9088)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 80 |  Loss: (0.1448) | Acc: (95.01%) (9851/10368)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 90 |  Loss: (0.1451) | Acc: (94.99%) (11064/11648)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 100 |  Loss: (0.1427) | Acc: (95.08%) (12292/12928)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 110 |  Loss: (0.1420) | Acc: (95.12%) (13514/14208)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 120 |  Loss: (0.1403) | Acc: (95.18%) (14742/15488)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 130 |  Loss: (0.1390) | Acc: (95.22%) (15966/16768)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 140 |  Loss: (0.1386) | Acc: (95.21%) (17184/18048)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 150 |  Loss: (0.1374) | Acc: (95.23%) (18407/19328)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 160 |  Loss: (0.1381) | Acc: (95.21%) (19621/20608)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 170 |  Loss: (0.1388) | Acc: (95.20%) (20838/21888)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 180 |  Loss: (0.1404) | Acc: (95.14%) (22041/23168)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 190 |  Loss: (0.1400) | Acc: (95.17%) (23267/24448)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 200 |  Loss: (0.1394) | Acc: (95.16%) (24482/25728)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 210 |  Loss: (0.1393) | Acc: (95.15%) (25697/27008)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 220 |  Loss: (0.1382) | Acc: (95.18%) (26925/28288)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 230 |  Loss: (0.1376) | Acc: (95.19%) (28147/29568)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 240 |  Loss: (0.1371) | Acc: (95.22%) (29375/30848)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 250 |  Loss: (0.1381) | Acc: (95.20%) (30585/32128)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 260 |  Loss: (0.1382) | Acc: (95.18%) (31799/33408)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 270 |  Loss: (0.1375) | Acc: (95.18%) (33017/34688)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 280 |  Loss: (0.1378) | Acc: (95.17%) (34231/35968)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 290 |  Loss: (0.1370) | Acc: (95.20%) (35459/37248)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 300 |  Loss: (0.1370) | Acc: (95.20%) (36679/38528)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 310 |  Loss: (0.1379) | Acc: (95.19%) (37894/39808)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 320 |  Loss: (0.1391) | Acc: (95.15%) (39096/41088)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 330 |  Loss: (0.1392) | Acc: (95.16%) (40319/42368)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 340 |  Loss: (0.1392) | Acc: (95.15%) (41530/43648)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 350 |  Loss: (0.1397) | Acc: (95.14%) (42745/44928)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 360 |  Loss: (0.1393) | Acc: (95.17%) (43974/46208)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 370 |  Loss: (0.1397) | Acc: (95.14%) (45182/47488)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 380 |  Loss: (0.1397) | Acc: (95.15%) (46401/48768)\n",
      "#TRAIN: Epoch: 61 | Batch_idx: 390 |  Loss: (0.1395) | Acc: (95.14%) (47570/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.6076) | Acc: (84.80%) (8480/10000)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 0 |  Loss: (0.0824) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 10 |  Loss: (0.1318) | Acc: (95.74%) (1348/1408)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 20 |  Loss: (0.1367) | Acc: (95.28%) (2561/2688)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 30 |  Loss: (0.1391) | Acc: (95.29%) (3781/3968)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 40 |  Loss: (0.1299) | Acc: (95.48%) (5011/5248)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 50 |  Loss: (0.1272) | Acc: (95.62%) (6242/6528)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 60 |  Loss: (0.1283) | Acc: (95.67%) (7470/7808)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 70 |  Loss: (0.1281) | Acc: (95.68%) (8695/9088)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 80 |  Loss: (0.1302) | Acc: (95.63%) (9915/10368)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 90 |  Loss: (0.1322) | Acc: (95.53%) (11127/11648)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 100 |  Loss: (0.1320) | Acc: (95.63%) (12363/12928)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 110 |  Loss: (0.1320) | Acc: (95.64%) (13588/14208)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 120 |  Loss: (0.1334) | Acc: (95.60%) (14807/15488)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 130 |  Loss: (0.1336) | Acc: (95.59%) (16029/16768)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 140 |  Loss: (0.1343) | Acc: (95.55%) (17245/18048)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 150 |  Loss: (0.1357) | Acc: (95.51%) (18460/19328)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 160 |  Loss: (0.1347) | Acc: (95.58%) (19698/20608)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 170 |  Loss: (0.1335) | Acc: (95.64%) (20933/21888)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 180 |  Loss: (0.1337) | Acc: (95.60%) (22149/23168)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 190 |  Loss: (0.1333) | Acc: (95.62%) (23378/24448)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 200 |  Loss: (0.1344) | Acc: (95.55%) (24582/25728)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 210 |  Loss: (0.1354) | Acc: (95.52%) (25798/27008)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 220 |  Loss: (0.1358) | Acc: (95.48%) (27010/28288)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 230 |  Loss: (0.1357) | Acc: (95.48%) (28233/29568)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 240 |  Loss: (0.1347) | Acc: (95.51%) (29464/30848)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 250 |  Loss: (0.1347) | Acc: (95.50%) (30682/32128)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 260 |  Loss: (0.1344) | Acc: (95.50%) (31903/33408)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 270 |  Loss: (0.1342) | Acc: (95.49%) (33123/34688)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 280 |  Loss: (0.1345) | Acc: (95.48%) (34344/35968)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 290 |  Loss: (0.1348) | Acc: (95.48%) (35564/37248)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 300 |  Loss: (0.1349) | Acc: (95.47%) (36783/38528)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 310 |  Loss: (0.1344) | Acc: (95.51%) (38022/39808)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 320 |  Loss: (0.1346) | Acc: (95.49%) (39234/41088)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 330 |  Loss: (0.1345) | Acc: (95.50%) (40461/42368)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 340 |  Loss: (0.1343) | Acc: (95.51%) (41687/43648)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 350 |  Loss: (0.1355) | Acc: (95.46%) (42887/44928)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 360 |  Loss: (0.1356) | Acc: (95.44%) (44103/46208)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 370 |  Loss: (0.1356) | Acc: (95.44%) (45321/47488)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 380 |  Loss: (0.1356) | Acc: (95.45%) (46548/48768)\n",
      "#TRAIN: Epoch: 62 | Batch_idx: 390 |  Loss: (0.1356) | Acc: (95.43%) (47717/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5095) | Acc: (86.54%) (8654/10000)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 0 |  Loss: (0.1398) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 10 |  Loss: (0.1016) | Acc: (97.02%) (1366/1408)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 20 |  Loss: (0.1069) | Acc: (96.73%) (2600/2688)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 30 |  Loss: (0.1152) | Acc: (96.27%) (3820/3968)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 40 |  Loss: (0.1201) | Acc: (96.06%) (5041/5248)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 50 |  Loss: (0.1251) | Acc: (95.88%) (6259/6528)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 60 |  Loss: (0.1281) | Acc: (95.84%) (7483/7808)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 70 |  Loss: (0.1263) | Acc: (95.84%) (8710/9088)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 80 |  Loss: (0.1256) | Acc: (95.87%) (9940/10368)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 90 |  Loss: (0.1279) | Acc: (95.72%) (11149/11648)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 100 |  Loss: (0.1280) | Acc: (95.70%) (12372/12928)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 110 |  Loss: (0.1266) | Acc: (95.71%) (13599/14208)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 120 |  Loss: (0.1267) | Acc: (95.68%) (14819/15488)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 130 |  Loss: (0.1274) | Acc: (95.65%) (16039/16768)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 140 |  Loss: (0.1274) | Acc: (95.65%) (17263/18048)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 150 |  Loss: (0.1269) | Acc: (95.67%) (18492/19328)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 160 |  Loss: (0.1258) | Acc: (95.72%) (19725/20608)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 170 |  Loss: (0.1245) | Acc: (95.78%) (20964/21888)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 180 |  Loss: (0.1245) | Acc: (95.77%) (22189/23168)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 190 |  Loss: (0.1261) | Acc: (95.77%) (23414/24448)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 200 |  Loss: (0.1244) | Acc: (95.82%) (24652/25728)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 210 |  Loss: (0.1250) | Acc: (95.79%) (25872/27008)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 220 |  Loss: (0.1247) | Acc: (95.79%) (27096/28288)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 230 |  Loss: (0.1257) | Acc: (95.78%) (28320/29568)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 240 |  Loss: (0.1251) | Acc: (95.80%) (29552/30848)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 250 |  Loss: (0.1245) | Acc: (95.83%) (30787/32128)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 260 |  Loss: (0.1237) | Acc: (95.85%) (32022/33408)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 270 |  Loss: (0.1238) | Acc: (95.86%) (33252/34688)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 280 |  Loss: (0.1241) | Acc: (95.85%) (34474/35968)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 290 |  Loss: (0.1239) | Acc: (95.84%) (35700/37248)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 300 |  Loss: (0.1247) | Acc: (95.81%) (36912/38528)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 310 |  Loss: (0.1256) | Acc: (95.77%) (38126/39808)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 320 |  Loss: (0.1254) | Acc: (95.79%) (39359/41088)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 330 |  Loss: (0.1258) | Acc: (95.78%) (40581/42368)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 340 |  Loss: (0.1260) | Acc: (95.77%) (41802/43648)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 350 |  Loss: (0.1263) | Acc: (95.75%) (43017/44928)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 360 |  Loss: (0.1270) | Acc: (95.73%) (44233/46208)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 370 |  Loss: (0.1275) | Acc: (95.71%) (45452/47488)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 380 |  Loss: (0.1268) | Acc: (95.74%) (46689/48768)\n",
      "#TRAIN: Epoch: 63 | Batch_idx: 390 |  Loss: (0.1269) | Acc: (95.74%) (47871/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5496) | Acc: (86.08%) (8608/10000)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 0 |  Loss: (0.1160) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 10 |  Loss: (0.1262) | Acc: (95.03%) (1338/1408)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 20 |  Loss: (0.1202) | Acc: (95.20%) (2559/2688)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 30 |  Loss: (0.1229) | Acc: (95.49%) (3789/3968)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 40 |  Loss: (0.1164) | Acc: (95.85%) (5030/5248)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 50 |  Loss: (0.1185) | Acc: (95.80%) (6254/6528)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 60 |  Loss: (0.1206) | Acc: (95.71%) (7473/7808)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 70 |  Loss: (0.1228) | Acc: (95.73%) (8700/9088)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 80 |  Loss: (0.1230) | Acc: (95.76%) (9928/10368)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 90 |  Loss: (0.1209) | Acc: (95.83%) (11162/11648)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 100 |  Loss: (0.1219) | Acc: (95.80%) (12385/12928)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 110 |  Loss: (0.1232) | Acc: (95.78%) (13609/14208)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 120 |  Loss: (0.1229) | Acc: (95.79%) (14836/15488)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 130 |  Loss: (0.1256) | Acc: (95.72%) (16051/16768)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 140 |  Loss: (0.1261) | Acc: (95.70%) (17272/18048)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 150 |  Loss: (0.1286) | Acc: (95.62%) (18481/19328)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 160 |  Loss: (0.1287) | Acc: (95.57%) (19696/20608)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 170 |  Loss: (0.1287) | Acc: (95.60%) (20925/21888)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 180 |  Loss: (0.1288) | Acc: (95.61%) (22152/23168)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 190 |  Loss: (0.1283) | Acc: (95.62%) (23377/24448)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 200 |  Loss: (0.1285) | Acc: (95.63%) (24603/25728)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 210 |  Loss: (0.1280) | Acc: (95.61%) (25823/27008)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 220 |  Loss: (0.1275) | Acc: (95.63%) (27053/28288)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 230 |  Loss: (0.1282) | Acc: (95.61%) (28271/29568)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 240 |  Loss: (0.1276) | Acc: (95.63%) (29501/30848)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 250 |  Loss: (0.1285) | Acc: (95.60%) (30713/32128)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 260 |  Loss: (0.1290) | Acc: (95.59%) (31934/33408)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 270 |  Loss: (0.1296) | Acc: (95.58%) (33156/34688)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 280 |  Loss: (0.1300) | Acc: (95.56%) (34371/35968)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 290 |  Loss: (0.1294) | Acc: (95.59%) (35604/37248)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 300 |  Loss: (0.1289) | Acc: (95.58%) (36825/38528)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 310 |  Loss: (0.1291) | Acc: (95.57%) (38046/39808)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 320 |  Loss: (0.1296) | Acc: (95.58%) (39270/41088)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 330 |  Loss: (0.1290) | Acc: (95.59%) (40500/42368)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 340 |  Loss: (0.1290) | Acc: (95.60%) (41727/43648)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 350 |  Loss: (0.1291) | Acc: (95.58%) (42944/44928)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 360 |  Loss: (0.1293) | Acc: (95.57%) (44162/46208)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 370 |  Loss: (0.1293) | Acc: (95.57%) (45383/47488)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 380 |  Loss: (0.1285) | Acc: (95.59%) (46617/48768)\n",
      "#TRAIN: Epoch: 64 | Batch_idx: 390 |  Loss: (0.1285) | Acc: (95.58%) (47792/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5333) | Acc: (86.51%) (8651/10000)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 0 |  Loss: (0.0431) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 10 |  Loss: (0.1015) | Acc: (96.88%) (1364/1408)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 20 |  Loss: (0.1059) | Acc: (96.69%) (2599/2688)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 30 |  Loss: (0.1087) | Acc: (96.30%) (3821/3968)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 40 |  Loss: (0.1091) | Acc: (96.30%) (5054/5248)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 50 |  Loss: (0.1090) | Acc: (96.31%) (6287/6528)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 60 |  Loss: (0.1116) | Acc: (96.25%) (7515/7808)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 70 |  Loss: (0.1109) | Acc: (96.26%) (8748/9088)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 80 |  Loss: (0.1119) | Acc: (96.24%) (9978/10368)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 90 |  Loss: (0.1142) | Acc: (96.21%) (11207/11648)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 100 |  Loss: (0.1137) | Acc: (96.20%) (12437/12928)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 110 |  Loss: (0.1127) | Acc: (96.19%) (13667/14208)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 120 |  Loss: (0.1129) | Acc: (96.19%) (14898/15488)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 130 |  Loss: (0.1138) | Acc: (96.12%) (16117/16768)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 140 |  Loss: (0.1156) | Acc: (96.07%) (17338/18048)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 150 |  Loss: (0.1148) | Acc: (96.08%) (18570/19328)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 160 |  Loss: (0.1133) | Acc: (96.13%) (19811/20608)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 170 |  Loss: (0.1128) | Acc: (96.13%) (21042/21888)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 180 |  Loss: (0.1144) | Acc: (96.05%) (22254/23168)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 190 |  Loss: (0.1140) | Acc: (96.06%) (23484/24448)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 200 |  Loss: (0.1149) | Acc: (96.02%) (24704/25728)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 210 |  Loss: (0.1150) | Acc: (96.01%) (25930/27008)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 220 |  Loss: (0.1157) | Acc: (96.00%) (27156/28288)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 230 |  Loss: (0.1164) | Acc: (95.99%) (28383/29568)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 240 |  Loss: (0.1181) | Acc: (95.94%) (29595/30848)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 250 |  Loss: (0.1196) | Acc: (95.90%) (30811/32128)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 260 |  Loss: (0.1191) | Acc: (95.92%) (32044/33408)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 270 |  Loss: (0.1196) | Acc: (95.91%) (33270/34688)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 280 |  Loss: (0.1199) | Acc: (95.91%) (34497/35968)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 290 |  Loss: (0.1203) | Acc: (95.88%) (35715/37248)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 300 |  Loss: (0.1192) | Acc: (95.92%) (36957/38528)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 310 |  Loss: (0.1195) | Acc: (95.91%) (38179/39808)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 320 |  Loss: (0.1198) | Acc: (95.89%) (39400/41088)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 330 |  Loss: (0.1196) | Acc: (95.90%) (40629/42368)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 340 |  Loss: (0.1191) | Acc: (95.90%) (41860/43648)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 350 |  Loss: (0.1193) | Acc: (95.91%) (43089/44928)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 360 |  Loss: (0.1204) | Acc: (95.87%) (44300/46208)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 370 |  Loss: (0.1200) | Acc: (95.90%) (45539/47488)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 380 |  Loss: (0.1207) | Acc: (95.88%) (46760/48768)\n",
      "#TRAIN: Epoch: 65 | Batch_idx: 390 |  Loss: (0.1203) | Acc: (95.90%) (47949/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5158) | Acc: (87.08%) (8708/10000)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 0 |  Loss: (0.0805) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 10 |  Loss: (0.1106) | Acc: (96.31%) (1356/1408)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 20 |  Loss: (0.1056) | Acc: (96.43%) (2592/2688)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 30 |  Loss: (0.1045) | Acc: (96.52%) (3830/3968)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 40 |  Loss: (0.0989) | Acc: (96.57%) (5068/5248)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 50 |  Loss: (0.0994) | Acc: (96.52%) (6301/6528)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 60 |  Loss: (0.0984) | Acc: (96.54%) (7538/7808)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 70 |  Loss: (0.1012) | Acc: (96.39%) (8760/9088)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 80 |  Loss: (0.1032) | Acc: (96.32%) (9986/10368)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 90 |  Loss: (0.1042) | Acc: (96.30%) (11217/11648)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 100 |  Loss: (0.1057) | Acc: (96.19%) (12435/12928)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 110 |  Loss: (0.1072) | Acc: (96.11%) (13655/14208)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 120 |  Loss: (0.1089) | Acc: (96.09%) (14883/15488)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 130 |  Loss: (0.1108) | Acc: (96.05%) (16106/16768)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 140 |  Loss: (0.1124) | Acc: (96.02%) (17329/18048)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 150 |  Loss: (0.1111) | Acc: (96.05%) (18565/19328)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 160 |  Loss: (0.1109) | Acc: (96.09%) (19802/20608)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 170 |  Loss: (0.1125) | Acc: (96.04%) (21022/21888)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 180 |  Loss: (0.1129) | Acc: (96.04%) (22251/23168)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 190 |  Loss: (0.1129) | Acc: (96.05%) (23482/24448)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 200 |  Loss: (0.1138) | Acc: (96.04%) (24708/25728)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 210 |  Loss: (0.1142) | Acc: (96.05%) (25940/27008)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 220 |  Loss: (0.1154) | Acc: (96.00%) (27157/28288)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 230 |  Loss: (0.1151) | Acc: (96.02%) (28390/29568)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 240 |  Loss: (0.1151) | Acc: (96.02%) (29621/30848)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 250 |  Loss: (0.1151) | Acc: (96.03%) (30852/32128)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 260 |  Loss: (0.1150) | Acc: (96.04%) (32086/33408)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 270 |  Loss: (0.1156) | Acc: (96.03%) (33312/34688)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 280 |  Loss: (0.1153) | Acc: (96.06%) (34552/35968)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 290 |  Loss: (0.1144) | Acc: (96.10%) (35796/37248)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 300 |  Loss: (0.1145) | Acc: (96.09%) (37022/38528)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 310 |  Loss: (0.1149) | Acc: (96.07%) (38245/39808)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 320 |  Loss: (0.1160) | Acc: (96.05%) (39465/41088)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 330 |  Loss: (0.1154) | Acc: (96.09%) (40710/42368)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 340 |  Loss: (0.1156) | Acc: (96.08%) (41937/43648)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 350 |  Loss: (0.1157) | Acc: (96.08%) (43167/44928)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 360 |  Loss: (0.1150) | Acc: (96.11%) (44411/46208)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 370 |  Loss: (0.1155) | Acc: (96.10%) (45636/47488)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 380 |  Loss: (0.1157) | Acc: (96.10%) (46867/48768)\n",
      "#TRAIN: Epoch: 66 | Batch_idx: 390 |  Loss: (0.1169) | Acc: (96.07%) (48035/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5208) | Acc: (87.09%) (8709/10000)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 0 |  Loss: (0.1357) | Acc: (95.31%) (122/128)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 10 |  Loss: (0.1258) | Acc: (96.09%) (1353/1408)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 20 |  Loss: (0.1158) | Acc: (96.13%) (2584/2688)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 30 |  Loss: (0.1178) | Acc: (95.97%) (3808/3968)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 40 |  Loss: (0.1114) | Acc: (96.25%) (5051/5248)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 50 |  Loss: (0.1092) | Acc: (96.31%) (6287/6528)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 60 |  Loss: (0.1114) | Acc: (96.21%) (7512/7808)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 70 |  Loss: (0.1117) | Acc: (96.16%) (8739/9088)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 80 |  Loss: (0.1114) | Acc: (96.18%) (9972/10368)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 90 |  Loss: (0.1102) | Acc: (96.20%) (11205/11648)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 100 |  Loss: (0.1111) | Acc: (96.19%) (12435/12928)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 110 |  Loss: (0.1123) | Acc: (96.19%) (13667/14208)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 120 |  Loss: (0.1128) | Acc: (96.16%) (14894/15488)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 130 |  Loss: (0.1132) | Acc: (96.11%) (16115/16768)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 140 |  Loss: (0.1119) | Acc: (96.17%) (17357/18048)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 150 |  Loss: (0.1111) | Acc: (96.19%) (18592/19328)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 160 |  Loss: (0.1114) | Acc: (96.22%) (19828/20608)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 170 |  Loss: (0.1123) | Acc: (96.16%) (21048/21888)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 180 |  Loss: (0.1126) | Acc: (96.16%) (22279/23168)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 190 |  Loss: (0.1127) | Acc: (96.18%) (23513/24448)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 200 |  Loss: (0.1128) | Acc: (96.17%) (24742/25728)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 210 |  Loss: (0.1137) | Acc: (96.15%) (25968/27008)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 220 |  Loss: (0.1145) | Acc: (96.09%) (27183/28288)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 230 |  Loss: (0.1143) | Acc: (96.08%) (28409/29568)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 240 |  Loss: (0.1141) | Acc: (96.10%) (29644/30848)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 250 |  Loss: (0.1151) | Acc: (96.05%) (30858/32128)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 260 |  Loss: (0.1152) | Acc: (96.03%) (32083/33408)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 270 |  Loss: (0.1157) | Acc: (96.02%) (33306/34688)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 280 |  Loss: (0.1153) | Acc: (96.02%) (34535/35968)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 290 |  Loss: (0.1152) | Acc: (96.04%) (35772/37248)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 300 |  Loss: (0.1155) | Acc: (96.03%) (36999/38528)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 310 |  Loss: (0.1155) | Acc: (96.05%) (38235/39808)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 320 |  Loss: (0.1154) | Acc: (96.05%) (39466/41088)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 330 |  Loss: (0.1159) | Acc: (96.03%) (40686/42368)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 340 |  Loss: (0.1161) | Acc: (96.01%) (41908/43648)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 350 |  Loss: (0.1169) | Acc: (96.01%) (43134/44928)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 360 |  Loss: (0.1173) | Acc: (95.98%) (44351/46208)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 370 |  Loss: (0.1176) | Acc: (95.97%) (45574/47488)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 380 |  Loss: (0.1183) | Acc: (95.94%) (46789/48768)\n",
      "#TRAIN: Epoch: 67 | Batch_idx: 390 |  Loss: (0.1179) | Acc: (95.96%) (47981/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5217) | Acc: (87.49%) (8749/10000)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 0 |  Loss: (0.0768) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 10 |  Loss: (0.1137) | Acc: (96.52%) (1359/1408)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 20 |  Loss: (0.1008) | Acc: (96.80%) (2602/2688)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 30 |  Loss: (0.1054) | Acc: (96.65%) (3835/3968)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 40 |  Loss: (0.1059) | Acc: (96.57%) (5068/5248)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 50 |  Loss: (0.1061) | Acc: (96.51%) (6300/6528)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 60 |  Loss: (0.1097) | Acc: (96.47%) (7532/7808)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 70 |  Loss: (0.1097) | Acc: (96.43%) (8764/9088)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 80 |  Loss: (0.1071) | Acc: (96.54%) (10009/10368)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 90 |  Loss: (0.1073) | Acc: (96.54%) (11245/11648)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 100 |  Loss: (0.1077) | Acc: (96.43%) (12466/12928)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 110 |  Loss: (0.1079) | Acc: (96.37%) (13692/14208)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 120 |  Loss: (0.1070) | Acc: (96.39%) (14929/15488)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 130 |  Loss: (0.1061) | Acc: (96.42%) (16168/16768)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 140 |  Loss: (0.1041) | Acc: (96.50%) (17416/18048)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 150 |  Loss: (0.1040) | Acc: (96.47%) (18645/19328)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 160 |  Loss: (0.1056) | Acc: (96.40%) (19866/20608)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 170 |  Loss: (0.1065) | Acc: (96.38%) (21095/21888)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 180 |  Loss: (0.1060) | Acc: (96.37%) (22327/23168)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 190 |  Loss: (0.1068) | Acc: (96.35%) (23555/24448)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 200 |  Loss: (0.1067) | Acc: (96.37%) (24793/25728)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 210 |  Loss: (0.1075) | Acc: (96.35%) (26022/27008)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 220 |  Loss: (0.1081) | Acc: (96.31%) (27244/28288)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 230 |  Loss: (0.1105) | Acc: (96.21%) (28447/29568)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 240 |  Loss: (0.1114) | Acc: (96.18%) (29670/30848)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 250 |  Loss: (0.1117) | Acc: (96.17%) (30899/32128)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 260 |  Loss: (0.1126) | Acc: (96.16%) (32125/33408)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 270 |  Loss: (0.1132) | Acc: (96.14%) (33350/34688)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 280 |  Loss: (0.1139) | Acc: (96.13%) (34576/35968)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 290 |  Loss: (0.1137) | Acc: (96.12%) (35803/37248)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 300 |  Loss: (0.1136) | Acc: (96.13%) (37038/38528)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 310 |  Loss: (0.1131) | Acc: (96.16%) (38279/39808)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 320 |  Loss: (0.1130) | Acc: (96.15%) (39505/41088)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 330 |  Loss: (0.1131) | Acc: (96.16%) (40739/42368)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 340 |  Loss: (0.1132) | Acc: (96.13%) (41961/43648)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 350 |  Loss: (0.1136) | Acc: (96.13%) (43189/44928)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 360 |  Loss: (0.1137) | Acc: (96.13%) (44420/46208)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 370 |  Loss: (0.1143) | Acc: (96.12%) (45646/47488)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 380 |  Loss: (0.1144) | Acc: (96.12%) (46877/48768)\n",
      "#TRAIN: Epoch: 68 | Batch_idx: 390 |  Loss: (0.1140) | Acc: (96.13%) (48065/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5271) | Acc: (87.35%) (8735/10000)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 0 |  Loss: (0.1124) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 10 |  Loss: (0.0949) | Acc: (96.73%) (1362/1408)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 20 |  Loss: (0.1105) | Acc: (96.21%) (2586/2688)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 30 |  Loss: (0.1063) | Acc: (96.27%) (3820/3968)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 40 |  Loss: (0.1014) | Acc: (96.46%) (5062/5248)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 50 |  Loss: (0.1004) | Acc: (96.45%) (6296/6528)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 60 |  Loss: (0.1027) | Acc: (96.30%) (7519/7808)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 70 |  Loss: (0.1046) | Acc: (96.21%) (8744/9088)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 80 |  Loss: (0.1035) | Acc: (96.35%) (9990/10368)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 90 |  Loss: (0.1056) | Acc: (96.24%) (11210/11648)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 100 |  Loss: (0.1068) | Acc: (96.23%) (12441/12928)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 110 |  Loss: (0.1083) | Acc: (96.24%) (13674/14208)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 120 |  Loss: (0.1096) | Acc: (96.17%) (14895/15488)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 130 |  Loss: (0.1105) | Acc: (96.14%) (16121/16768)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 140 |  Loss: (0.1115) | Acc: (96.12%) (17348/18048)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 150 |  Loss: (0.1124) | Acc: (96.12%) (18579/19328)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 160 |  Loss: (0.1128) | Acc: (96.10%) (19804/20608)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 170 |  Loss: (0.1122) | Acc: (96.12%) (21038/21888)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 180 |  Loss: (0.1132) | Acc: (96.09%) (22262/23168)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 190 |  Loss: (0.1144) | Acc: (96.06%) (23485/24448)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 200 |  Loss: (0.1141) | Acc: (96.07%) (24716/25728)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 210 |  Loss: (0.1142) | Acc: (96.08%) (25948/27008)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 220 |  Loss: (0.1136) | Acc: (96.10%) (27185/28288)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 230 |  Loss: (0.1137) | Acc: (96.09%) (28411/29568)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 240 |  Loss: (0.1128) | Acc: (96.12%) (29650/30848)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 250 |  Loss: (0.1125) | Acc: (96.13%) (30884/32128)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 260 |  Loss: (0.1122) | Acc: (96.13%) (32116/33408)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 270 |  Loss: (0.1117) | Acc: (96.14%) (33350/34688)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 280 |  Loss: (0.1116) | Acc: (96.15%) (34583/35968)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 290 |  Loss: (0.1119) | Acc: (96.14%) (35809/37248)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 300 |  Loss: (0.1125) | Acc: (96.09%) (37023/38528)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 310 |  Loss: (0.1126) | Acc: (96.11%) (38259/39808)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 320 |  Loss: (0.1135) | Acc: (96.07%) (39475/41088)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 330 |  Loss: (0.1131) | Acc: (96.08%) (40709/42368)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 340 |  Loss: (0.1134) | Acc: (96.06%) (41930/43648)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 350 |  Loss: (0.1135) | Acc: (96.07%) (43161/44928)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 360 |  Loss: (0.1137) | Acc: (96.07%) (44390/46208)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 370 |  Loss: (0.1138) | Acc: (96.06%) (45619/47488)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 380 |  Loss: (0.1142) | Acc: (96.06%) (46848/48768)\n",
      "#TRAIN: Epoch: 69 | Batch_idx: 390 |  Loss: (0.1140) | Acc: (96.07%) (48034/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5503) | Acc: (86.78%) (8678/10000)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 0 |  Loss: (0.0404) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 10 |  Loss: (0.1002) | Acc: (96.59%) (1360/1408)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 20 |  Loss: (0.1014) | Acc: (96.28%) (2588/2688)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 30 |  Loss: (0.1010) | Acc: (96.24%) (3819/3968)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 40 |  Loss: (0.1016) | Acc: (96.28%) (5053/5248)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 50 |  Loss: (0.1040) | Acc: (96.34%) (6289/6528)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 60 |  Loss: (0.1085) | Acc: (96.23%) (7514/7808)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 70 |  Loss: (0.1077) | Acc: (96.25%) (8747/9088)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 80 |  Loss: (0.1079) | Acc: (96.26%) (9980/10368)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 90 |  Loss: (0.1107) | Acc: (96.20%) (11205/11648)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 100 |  Loss: (0.1098) | Acc: (96.20%) (12437/12928)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 110 |  Loss: (0.1091) | Acc: (96.28%) (13679/14208)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 120 |  Loss: (0.1111) | Acc: (96.22%) (14903/15488)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 130 |  Loss: (0.1108) | Acc: (96.20%) (16130/16768)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 140 |  Loss: (0.1106) | Acc: (96.22%) (17366/18048)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 150 |  Loss: (0.1103) | Acc: (96.27%) (18608/19328)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 160 |  Loss: (0.1100) | Acc: (96.30%) (19845/20608)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 170 |  Loss: (0.1096) | Acc: (96.31%) (21081/21888)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 180 |  Loss: (0.1092) | Acc: (96.32%) (22316/23168)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 190 |  Loss: (0.1092) | Acc: (96.33%) (23551/24448)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 200 |  Loss: (0.1095) | Acc: (96.31%) (24778/25728)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 210 |  Loss: (0.1081) | Acc: (96.37%) (26028/27008)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 220 |  Loss: (0.1076) | Acc: (96.38%) (27263/28288)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 230 |  Loss: (0.1081) | Acc: (96.35%) (28488/29568)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 240 |  Loss: (0.1082) | Acc: (96.32%) (29713/30848)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 250 |  Loss: (0.1085) | Acc: (96.31%) (30942/32128)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 260 |  Loss: (0.1088) | Acc: (96.30%) (32173/33408)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 270 |  Loss: (0.1098) | Acc: (96.24%) (33384/34688)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 280 |  Loss: (0.1109) | Acc: (96.19%) (34597/35968)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 290 |  Loss: (0.1108) | Acc: (96.18%) (35826/37248)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 300 |  Loss: (0.1111) | Acc: (96.18%) (37056/38528)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 310 |  Loss: (0.1106) | Acc: (96.22%) (38302/39808)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 320 |  Loss: (0.1103) | Acc: (96.23%) (39537/41088)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 330 |  Loss: (0.1101) | Acc: (96.23%) (40769/42368)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 340 |  Loss: (0.1101) | Acc: (96.22%) (42000/43648)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 350 |  Loss: (0.1100) | Acc: (96.25%) (43242/44928)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 360 |  Loss: (0.1099) | Acc: (96.24%) (44470/46208)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 370 |  Loss: (0.1097) | Acc: (96.25%) (45705/47488)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 380 |  Loss: (0.1102) | Acc: (96.22%) (46923/48768)\n",
      "#TRAIN: Epoch: 70 | Batch_idx: 390 |  Loss: (0.1099) | Acc: (96.23%) (48115/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5186) | Acc: (87.54%) (8754/10000)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 0 |  Loss: (0.1015) | Acc: (96.88%) (124/128)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 10 |  Loss: (0.1117) | Acc: (95.88%) (1350/1408)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 20 |  Loss: (0.0972) | Acc: (96.54%) (2595/2688)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 30 |  Loss: (0.1012) | Acc: (96.45%) (3827/3968)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 40 |  Loss: (0.0977) | Acc: (96.61%) (5070/5248)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 50 |  Loss: (0.0967) | Acc: (96.65%) (6309/6528)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 60 |  Loss: (0.0969) | Acc: (96.67%) (7548/7808)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 70 |  Loss: (0.0960) | Acc: (96.76%) (8794/9088)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 80 |  Loss: (0.0949) | Acc: (96.81%) (10037/10368)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 90 |  Loss: (0.0963) | Acc: (96.77%) (11272/11648)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 100 |  Loss: (0.0966) | Acc: (96.70%) (12502/12928)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 110 |  Loss: (0.0957) | Acc: (96.77%) (13749/14208)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 120 |  Loss: (0.0963) | Acc: (96.75%) (14985/15488)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 130 |  Loss: (0.0962) | Acc: (96.73%) (16220/16768)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 140 |  Loss: (0.0974) | Acc: (96.70%) (17453/18048)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 150 |  Loss: (0.0988) | Acc: (96.68%) (18686/19328)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 160 |  Loss: (0.0998) | Acc: (96.61%) (19910/20608)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 170 |  Loss: (0.1011) | Acc: (96.56%) (21135/21888)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 180 |  Loss: (0.1017) | Acc: (96.54%) (22367/23168)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 190 |  Loss: (0.1026) | Acc: (96.51%) (23595/24448)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 200 |  Loss: (0.1044) | Acc: (96.45%) (24814/25728)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 210 |  Loss: (0.1051) | Acc: (96.43%) (26043/27008)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 220 |  Loss: (0.1056) | Acc: (96.41%) (27272/28288)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 230 |  Loss: (0.1051) | Acc: (96.41%) (28507/29568)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 240 |  Loss: (0.1044) | Acc: (96.43%) (29746/30848)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 250 |  Loss: (0.1040) | Acc: (96.44%) (30984/32128)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 260 |  Loss: (0.1038) | Acc: (96.45%) (32221/33408)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 270 |  Loss: (0.1039) | Acc: (96.46%) (33461/34688)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 280 |  Loss: (0.1044) | Acc: (96.47%) (34698/35968)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 290 |  Loss: (0.1045) | Acc: (96.46%) (35928/37248)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 300 |  Loss: (0.1047) | Acc: (96.44%) (37158/38528)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 310 |  Loss: (0.1053) | Acc: (96.42%) (38384/39808)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 320 |  Loss: (0.1059) | Acc: (96.40%) (39609/41088)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 330 |  Loss: (0.1057) | Acc: (96.42%) (40851/42368)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 340 |  Loss: (0.1058) | Acc: (96.41%) (42082/43648)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 350 |  Loss: (0.1061) | Acc: (96.40%) (43312/44928)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 360 |  Loss: (0.1056) | Acc: (96.43%) (44558/46208)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 370 |  Loss: (0.1061) | Acc: (96.41%) (45782/47488)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 380 |  Loss: (0.1063) | Acc: (96.40%) (47011/48768)\n",
      "#TRAIN: Epoch: 71 | Batch_idx: 390 |  Loss: (0.1060) | Acc: (96.41%) (48203/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5326) | Acc: (87.01%) (8701/10000)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 0 |  Loss: (0.2367) | Acc: (94.53%) (121/128)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 10 |  Loss: (0.1177) | Acc: (96.38%) (1357/1408)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 20 |  Loss: (0.1102) | Acc: (96.39%) (2591/2688)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 30 |  Loss: (0.1056) | Acc: (96.57%) (3832/3968)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 40 |  Loss: (0.1056) | Acc: (96.53%) (5066/5248)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 50 |  Loss: (0.1079) | Acc: (96.48%) (6298/6528)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 60 |  Loss: (0.1046) | Acc: (96.50%) (7535/7808)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 70 |  Loss: (0.1058) | Acc: (96.50%) (8770/9088)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 80 |  Loss: (0.1045) | Acc: (96.60%) (10015/10368)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 90 |  Loss: (0.1057) | Acc: (96.57%) (11249/11648)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 100 |  Loss: (0.1055) | Acc: (96.57%) (12484/12928)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 110 |  Loss: (0.1065) | Acc: (96.50%) (13711/14208)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 120 |  Loss: (0.1061) | Acc: (96.49%) (14944/15488)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 130 |  Loss: (0.1053) | Acc: (96.51%) (16182/16768)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 140 |  Loss: (0.1043) | Acc: (96.53%) (17421/18048)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 150 |  Loss: (0.1037) | Acc: (96.53%) (18658/19328)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 160 |  Loss: (0.1024) | Acc: (96.58%) (19904/20608)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 170 |  Loss: (0.1030) | Acc: (96.56%) (21136/21888)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 180 |  Loss: (0.1033) | Acc: (96.57%) (22374/23168)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 190 |  Loss: (0.1033) | Acc: (96.54%) (23603/24448)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 200 |  Loss: (0.1036) | Acc: (96.53%) (24834/25728)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 210 |  Loss: (0.1036) | Acc: (96.51%) (26065/27008)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 220 |  Loss: (0.1032) | Acc: (96.53%) (27306/28288)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 230 |  Loss: (0.1042) | Acc: (96.49%) (28531/29568)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 240 |  Loss: (0.1040) | Acc: (96.49%) (29765/30848)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 250 |  Loss: (0.1040) | Acc: (96.50%) (31003/32128)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 260 |  Loss: (0.1048) | Acc: (96.49%) (32235/33408)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 270 |  Loss: (0.1042) | Acc: (96.53%) (33483/34688)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 280 |  Loss: (0.1041) | Acc: (96.53%) (34719/35968)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 290 |  Loss: (0.1037) | Acc: (96.53%) (35957/37248)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 300 |  Loss: (0.1039) | Acc: (96.52%) (37187/38528)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 310 |  Loss: (0.1038) | Acc: (96.53%) (38427/39808)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 320 |  Loss: (0.1036) | Acc: (96.53%) (39663/41088)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 330 |  Loss: (0.1038) | Acc: (96.51%) (40890/42368)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 340 |  Loss: (0.1039) | Acc: (96.50%) (42122/43648)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 350 |  Loss: (0.1033) | Acc: (96.53%) (43368/44928)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 360 |  Loss: (0.1030) | Acc: (96.53%) (44606/46208)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 370 |  Loss: (0.1033) | Acc: (96.53%) (45838/47488)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 380 |  Loss: (0.1032) | Acc: (96.52%) (47069/48768)\n",
      "#TRAIN: Epoch: 72 | Batch_idx: 390 |  Loss: (0.1033) | Acc: (96.51%) (48255/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5624) | Acc: (86.98%) (8698/10000)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 0 |  Loss: (0.1232) | Acc: (95.31%) (122/128)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 10 |  Loss: (0.0895) | Acc: (97.23%) (1369/1408)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 20 |  Loss: (0.0886) | Acc: (97.28%) (2615/2688)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 30 |  Loss: (0.0887) | Acc: (97.25%) (3859/3968)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 40 |  Loss: (0.0920) | Acc: (96.99%) (5090/5248)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 50 |  Loss: (0.0960) | Acc: (96.80%) (6319/6528)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 60 |  Loss: (0.0971) | Acc: (96.75%) (7554/7808)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 70 |  Loss: (0.0964) | Acc: (96.80%) (8797/9088)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 80 |  Loss: (0.0974) | Acc: (96.74%) (10030/10368)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 90 |  Loss: (0.1014) | Acc: (96.63%) (11256/11648)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 100 |  Loss: (0.1001) | Acc: (96.66%) (12496/12928)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 110 |  Loss: (0.1001) | Acc: (96.61%) (13727/14208)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 120 |  Loss: (0.1003) | Acc: (96.64%) (14967/15488)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 130 |  Loss: (0.0995) | Acc: (96.65%) (16206/16768)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 140 |  Loss: (0.0994) | Acc: (96.65%) (17444/18048)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 150 |  Loss: (0.0984) | Acc: (96.68%) (18687/19328)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 160 |  Loss: (0.0984) | Acc: (96.67%) (19922/20608)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 170 |  Loss: (0.0987) | Acc: (96.65%) (21154/21888)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 180 |  Loss: (0.0989) | Acc: (96.63%) (22387/23168)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 190 |  Loss: (0.0993) | Acc: (96.66%) (23631/24448)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 200 |  Loss: (0.0988) | Acc: (96.67%) (24870/25728)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 210 |  Loss: (0.1001) | Acc: (96.59%) (26088/27008)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 220 |  Loss: (0.1013) | Acc: (96.56%) (27314/28288)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 230 |  Loss: (0.1014) | Acc: (96.56%) (28552/29568)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 240 |  Loss: (0.1017) | Acc: (96.54%) (29782/30848)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 250 |  Loss: (0.1019) | Acc: (96.54%) (31017/32128)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 260 |  Loss: (0.1011) | Acc: (96.58%) (32264/33408)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 270 |  Loss: (0.1015) | Acc: (96.58%) (33503/34688)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 280 |  Loss: (0.1015) | Acc: (96.57%) (34734/35968)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 290 |  Loss: (0.1028) | Acc: (96.50%) (35946/37248)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 300 |  Loss: (0.1039) | Acc: (96.46%) (37166/38528)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 310 |  Loss: (0.1035) | Acc: (96.47%) (38404/39808)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 320 |  Loss: (0.1038) | Acc: (96.46%) (39634/41088)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 330 |  Loss: (0.1039) | Acc: (96.45%) (40865/42368)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 340 |  Loss: (0.1043) | Acc: (96.44%) (42092/43648)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 350 |  Loss: (0.1042) | Acc: (96.44%) (43330/44928)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 360 |  Loss: (0.1037) | Acc: (96.45%) (44568/46208)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 370 |  Loss: (0.1041) | Acc: (96.42%) (45789/47488)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 380 |  Loss: (0.1042) | Acc: (96.43%) (47027/48768)\n",
      "#TRAIN: Epoch: 73 | Batch_idx: 390 |  Loss: (0.1038) | Acc: (96.46%) (48229/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5408) | Acc: (87.13%) (8713/10000)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 0 |  Loss: (0.0988) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 10 |  Loss: (0.0802) | Acc: (97.44%) (1372/1408)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 20 |  Loss: (0.0834) | Acc: (97.40%) (2618/2688)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 30 |  Loss: (0.0823) | Acc: (97.40%) (3865/3968)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 40 |  Loss: (0.0844) | Acc: (97.24%) (5103/5248)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 50 |  Loss: (0.0875) | Acc: (97.09%) (6338/6528)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 60 |  Loss: (0.0911) | Acc: (96.93%) (7568/7808)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 70 |  Loss: (0.0929) | Acc: (96.86%) (8803/9088)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 80 |  Loss: (0.0940) | Acc: (96.85%) (10041/10368)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 90 |  Loss: (0.0941) | Acc: (96.86%) (11282/11648)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 100 |  Loss: (0.0947) | Acc: (96.79%) (12513/12928)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 110 |  Loss: (0.0981) | Acc: (96.73%) (13743/14208)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 120 |  Loss: (0.0976) | Acc: (96.75%) (14985/15488)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 130 |  Loss: (0.0981) | Acc: (96.71%) (16216/16768)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 140 |  Loss: (0.0981) | Acc: (96.74%) (17459/18048)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 150 |  Loss: (0.0985) | Acc: (96.70%) (18691/19328)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 160 |  Loss: (0.0992) | Acc: (96.73%) (19934/20608)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 170 |  Loss: (0.0998) | Acc: (96.72%) (21169/21888)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 180 |  Loss: (0.0990) | Acc: (96.72%) (22409/23168)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 190 |  Loss: (0.1000) | Acc: (96.71%) (23643/24448)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 200 |  Loss: (0.1001) | Acc: (96.70%) (24879/25728)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 210 |  Loss: (0.1004) | Acc: (96.68%) (26111/27008)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 220 |  Loss: (0.0992) | Acc: (96.72%) (27359/28288)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 230 |  Loss: (0.0992) | Acc: (96.72%) (28598/29568)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 240 |  Loss: (0.0988) | Acc: (96.73%) (29839/30848)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 250 |  Loss: (0.0982) | Acc: (96.75%) (31085/32128)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 260 |  Loss: (0.0987) | Acc: (96.74%) (32320/33408)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 270 |  Loss: (0.0988) | Acc: (96.72%) (33550/34688)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 280 |  Loss: (0.0987) | Acc: (96.72%) (34787/35968)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 290 |  Loss: (0.0981) | Acc: (96.73%) (36031/37248)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 300 |  Loss: (0.0983) | Acc: (96.72%) (37266/38528)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 310 |  Loss: (0.0982) | Acc: (96.72%) (38504/39808)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 320 |  Loss: (0.0978) | Acc: (96.73%) (39744/41088)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 330 |  Loss: (0.0983) | Acc: (96.70%) (40971/42368)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 340 |  Loss: (0.0979) | Acc: (96.72%) (42217/43648)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 350 |  Loss: (0.0983) | Acc: (96.70%) (43445/44928)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 360 |  Loss: (0.0984) | Acc: (96.70%) (44685/46208)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 370 |  Loss: (0.0988) | Acc: (96.69%) (45916/47488)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 380 |  Loss: (0.0989) | Acc: (96.69%) (47155/48768)\n",
      "#TRAIN: Epoch: 74 | Batch_idx: 390 |  Loss: (0.0991) | Acc: (96.69%) (48344/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5367) | Acc: (87.28%) (8728/10000)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 0 |  Loss: (0.0679) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 10 |  Loss: (0.0853) | Acc: (96.73%) (1362/1408)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 20 |  Loss: (0.0939) | Acc: (96.80%) (2602/2688)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 30 |  Loss: (0.0918) | Acc: (96.90%) (3845/3968)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 40 |  Loss: (0.0900) | Acc: (97.03%) (5092/5248)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 50 |  Loss: (0.0897) | Acc: (96.97%) (6330/6528)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 60 |  Loss: (0.0887) | Acc: (96.91%) (7567/7808)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 70 |  Loss: (0.0910) | Acc: (96.86%) (8803/9088)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 80 |  Loss: (0.0915) | Acc: (96.86%) (10042/10368)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 90 |  Loss: (0.0940) | Acc: (96.77%) (11272/11648)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 100 |  Loss: (0.0949) | Acc: (96.74%) (12507/12928)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 110 |  Loss: (0.0966) | Acc: (96.68%) (13736/14208)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 120 |  Loss: (0.0981) | Acc: (96.66%) (14970/15488)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 130 |  Loss: (0.0982) | Acc: (96.65%) (16207/16768)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 140 |  Loss: (0.0972) | Acc: (96.64%) (17441/18048)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 150 |  Loss: (0.0956) | Acc: (96.66%) (18683/19328)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 160 |  Loss: (0.0968) | Acc: (96.63%) (19914/20608)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 170 |  Loss: (0.0958) | Acc: (96.65%) (21154/21888)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 180 |  Loss: (0.0961) | Acc: (96.65%) (22391/23168)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 190 |  Loss: (0.0970) | Acc: (96.60%) (23616/24448)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 200 |  Loss: (0.0969) | Acc: (96.62%) (24858/25728)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 210 |  Loss: (0.0976) | Acc: (96.59%) (26088/27008)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 220 |  Loss: (0.0978) | Acc: (96.57%) (27317/28288)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 230 |  Loss: (0.0976) | Acc: (96.57%) (28555/29568)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 240 |  Loss: (0.0974) | Acc: (96.58%) (29794/30848)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 250 |  Loss: (0.0983) | Acc: (96.54%) (31016/32128)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 260 |  Loss: (0.0970) | Acc: (96.58%) (32266/33408)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 270 |  Loss: (0.0973) | Acc: (96.59%) (33505/34688)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 280 |  Loss: (0.0972) | Acc: (96.60%) (34745/35968)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 290 |  Loss: (0.0972) | Acc: (96.61%) (35986/37248)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 300 |  Loss: (0.0967) | Acc: (96.63%) (37228/38528)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 310 |  Loss: (0.0967) | Acc: (96.63%) (38465/39808)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 320 |  Loss: (0.0971) | Acc: (96.61%) (39697/41088)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 330 |  Loss: (0.0972) | Acc: (96.62%) (40936/42368)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 340 |  Loss: (0.0975) | Acc: (96.62%) (42174/43648)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 350 |  Loss: (0.0977) | Acc: (96.62%) (43410/44928)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 360 |  Loss: (0.0977) | Acc: (96.61%) (44641/46208)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 370 |  Loss: (0.0980) | Acc: (96.60%) (45873/47488)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 380 |  Loss: (0.0985) | Acc: (96.58%) (47102/48768)\n",
      "#TRAIN: Epoch: 75 | Batch_idx: 390 |  Loss: (0.0983) | Acc: (96.59%) (48296/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5664) | Acc: (87.05%) (8705/10000)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 0 |  Loss: (0.1033) | Acc: (95.31%) (122/128)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 10 |  Loss: (0.0960) | Acc: (96.52%) (1359/1408)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 20 |  Loss: (0.0945) | Acc: (96.73%) (2600/2688)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 30 |  Loss: (0.0928) | Acc: (96.77%) (3840/3968)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 40 |  Loss: (0.0958) | Acc: (96.65%) (5072/5248)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 50 |  Loss: (0.0955) | Acc: (96.74%) (6315/6528)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 60 |  Loss: (0.0928) | Acc: (96.85%) (7562/7808)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 70 |  Loss: (0.0923) | Acc: (96.83%) (8800/9088)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 80 |  Loss: (0.0951) | Acc: (96.78%) (10034/10368)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 90 |  Loss: (0.0962) | Acc: (96.79%) (11274/11648)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 100 |  Loss: (0.0958) | Acc: (96.76%) (12509/12928)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 110 |  Loss: (0.0954) | Acc: (96.78%) (13750/14208)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 120 |  Loss: (0.0969) | Acc: (96.72%) (14980/15488)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 130 |  Loss: (0.0970) | Acc: (96.74%) (16222/16768)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 140 |  Loss: (0.0956) | Acc: (96.80%) (17470/18048)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 150 |  Loss: (0.0949) | Acc: (96.83%) (18716/19328)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 160 |  Loss: (0.0941) | Acc: (96.87%) (19962/20608)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 170 |  Loss: (0.0943) | Acc: (96.85%) (21198/21888)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 180 |  Loss: (0.0957) | Acc: (96.79%) (22425/23168)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 190 |  Loss: (0.0960) | Acc: (96.78%) (23660/24448)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 200 |  Loss: (0.0961) | Acc: (96.78%) (24900/25728)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 210 |  Loss: (0.0967) | Acc: (96.78%) (26138/27008)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 220 |  Loss: (0.0972) | Acc: (96.76%) (27371/28288)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 230 |  Loss: (0.0973) | Acc: (96.75%) (28608/29568)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 240 |  Loss: (0.0972) | Acc: (96.76%) (29847/30848)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 250 |  Loss: (0.0968) | Acc: (96.76%) (31086/32128)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 260 |  Loss: (0.0969) | Acc: (96.75%) (32321/33408)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 270 |  Loss: (0.0968) | Acc: (96.73%) (33555/34688)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 280 |  Loss: (0.0969) | Acc: (96.73%) (34792/35968)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 290 |  Loss: (0.0969) | Acc: (96.72%) (36028/37248)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 300 |  Loss: (0.0976) | Acc: (96.70%) (37257/38528)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 310 |  Loss: (0.0970) | Acc: (96.72%) (38502/39808)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 320 |  Loss: (0.0969) | Acc: (96.73%) (39743/41088)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 330 |  Loss: (0.0984) | Acc: (96.67%) (40956/42368)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 340 |  Loss: (0.0985) | Acc: (96.68%) (42198/43648)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 350 |  Loss: (0.0983) | Acc: (96.67%) (43432/44928)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 360 |  Loss: (0.0991) | Acc: (96.64%) (44654/46208)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 370 |  Loss: (0.0991) | Acc: (96.63%) (45889/47488)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 380 |  Loss: (0.0989) | Acc: (96.65%) (47132/48768)\n",
      "#TRAIN: Epoch: 76 | Batch_idx: 390 |  Loss: (0.0987) | Acc: (96.64%) (48321/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5325) | Acc: (87.41%) (8741/10000)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 0 |  Loss: (0.1261) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 10 |  Loss: (0.1034) | Acc: (96.59%) (1360/1408)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 20 |  Loss: (0.0958) | Acc: (96.69%) (2599/2688)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 30 |  Loss: (0.0912) | Acc: (96.88%) (3844/3968)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 40 |  Loss: (0.0921) | Acc: (96.76%) (5078/5248)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 50 |  Loss: (0.0927) | Acc: (96.83%) (6321/6528)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 60 |  Loss: (0.0914) | Acc: (96.89%) (7565/7808)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 70 |  Loss: (0.0898) | Acc: (96.93%) (8809/9088)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 80 |  Loss: (0.0906) | Acc: (96.98%) (10055/10368)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 90 |  Loss: (0.0906) | Acc: (96.94%) (11292/11648)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 100 |  Loss: (0.0920) | Acc: (96.84%) (12520/12928)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 110 |  Loss: (0.0939) | Acc: (96.81%) (13755/14208)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 120 |  Loss: (0.0939) | Acc: (96.79%) (14991/15488)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 130 |  Loss: (0.0934) | Acc: (96.81%) (16233/16768)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 140 |  Loss: (0.0937) | Acc: (96.82%) (17474/18048)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 150 |  Loss: (0.0939) | Acc: (96.79%) (18708/19328)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 160 |  Loss: (0.0950) | Acc: (96.75%) (19939/20608)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 170 |  Loss: (0.0954) | Acc: (96.71%) (21167/21888)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 180 |  Loss: (0.0952) | Acc: (96.71%) (22406/23168)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 190 |  Loss: (0.0962) | Acc: (96.69%) (23638/24448)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 200 |  Loss: (0.0967) | Acc: (96.68%) (24875/25728)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 210 |  Loss: (0.0961) | Acc: (96.71%) (26119/27008)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 220 |  Loss: (0.0966) | Acc: (96.69%) (27352/28288)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 230 |  Loss: (0.0970) | Acc: (96.66%) (28581/29568)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 240 |  Loss: (0.0981) | Acc: (96.63%) (29808/30848)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 250 |  Loss: (0.0988) | Acc: (96.63%) (31046/32128)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 260 |  Loss: (0.0987) | Acc: (96.64%) (32286/33408)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 270 |  Loss: (0.0978) | Acc: (96.67%) (33533/34688)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 280 |  Loss: (0.0980) | Acc: (96.67%) (34771/35968)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 290 |  Loss: (0.0971) | Acc: (96.69%) (36015/37248)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 300 |  Loss: (0.0973) | Acc: (96.68%) (37250/38528)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 310 |  Loss: (0.0968) | Acc: (96.69%) (38491/39808)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 320 |  Loss: (0.0968) | Acc: (96.68%) (39724/41088)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 330 |  Loss: (0.0970) | Acc: (96.69%) (40965/42368)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 340 |  Loss: (0.0971) | Acc: (96.68%) (42198/43648)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 350 |  Loss: (0.0968) | Acc: (96.68%) (43438/44928)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 360 |  Loss: (0.0970) | Acc: (96.67%) (44670/46208)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 370 |  Loss: (0.0971) | Acc: (96.67%) (45907/47488)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 380 |  Loss: (0.0967) | Acc: (96.69%) (47152/48768)\n",
      "#TRAIN: Epoch: 77 | Batch_idx: 390 |  Loss: (0.0965) | Acc: (96.70%) (48349/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5565) | Acc: (87.40%) (8740/10000)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 0 |  Loss: (0.0595) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 10 |  Loss: (0.0806) | Acc: (97.30%) (1370/1408)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 20 |  Loss: (0.0827) | Acc: (97.28%) (2615/2688)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 30 |  Loss: (0.0857) | Acc: (97.20%) (3857/3968)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 40 |  Loss: (0.0930) | Acc: (97.03%) (5092/5248)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 50 |  Loss: (0.0900) | Acc: (97.10%) (6339/6528)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 60 |  Loss: (0.0900) | Acc: (97.05%) (7578/7808)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 70 |  Loss: (0.0887) | Acc: (97.01%) (8816/9088)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 80 |  Loss: (0.0896) | Acc: (96.98%) (10055/10368)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 90 |  Loss: (0.0894) | Acc: (96.95%) (11293/11648)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 100 |  Loss: (0.0890) | Acc: (97.00%) (12540/12928)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 110 |  Loss: (0.0878) | Acc: (96.99%) (13781/14208)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 120 |  Loss: (0.0884) | Acc: (97.02%) (15026/15488)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 130 |  Loss: (0.0906) | Acc: (96.94%) (16255/16768)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 140 |  Loss: (0.0896) | Acc: (96.96%) (17500/18048)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 150 |  Loss: (0.0905) | Acc: (96.96%) (18741/19328)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 160 |  Loss: (0.0915) | Acc: (96.94%) (19977/20608)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 170 |  Loss: (0.0905) | Acc: (96.97%) (21224/21888)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 180 |  Loss: (0.0895) | Acc: (96.98%) (22468/23168)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 190 |  Loss: (0.0897) | Acc: (96.95%) (23703/24448)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 200 |  Loss: (0.0899) | Acc: (96.96%) (24946/25728)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 210 |  Loss: (0.0914) | Acc: (96.91%) (26174/27008)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 220 |  Loss: (0.0906) | Acc: (96.92%) (27417/28288)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 230 |  Loss: (0.0912) | Acc: (96.90%) (28651/29568)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 240 |  Loss: (0.0909) | Acc: (96.89%) (29888/30848)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 250 |  Loss: (0.0905) | Acc: (96.91%) (31134/32128)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 260 |  Loss: (0.0908) | Acc: (96.89%) (32370/33408)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 270 |  Loss: (0.0911) | Acc: (96.90%) (33612/34688)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 280 |  Loss: (0.0908) | Acc: (96.91%) (34855/35968)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 290 |  Loss: (0.0920) | Acc: (96.88%) (36086/37248)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 300 |  Loss: (0.0922) | Acc: (96.89%) (37328/38528)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 310 |  Loss: (0.0923) | Acc: (96.88%) (38567/39808)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 320 |  Loss: (0.0921) | Acc: (96.88%) (39808/41088)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 330 |  Loss: (0.0914) | Acc: (96.90%) (41056/42368)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 340 |  Loss: (0.0916) | Acc: (96.89%) (42291/43648)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 350 |  Loss: (0.0911) | Acc: (96.90%) (43537/44928)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 360 |  Loss: (0.0917) | Acc: (96.89%) (44771/46208)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 370 |  Loss: (0.0915) | Acc: (96.90%) (46014/47488)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 380 |  Loss: (0.0914) | Acc: (96.90%) (47256/48768)\n",
      "#TRAIN: Epoch: 78 | Batch_idx: 390 |  Loss: (0.0914) | Acc: (96.92%) (48461/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5470) | Acc: (87.44%) (8744/10000)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 0 |  Loss: (0.0591) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 10 |  Loss: (0.0799) | Acc: (97.51%) (1373/1408)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 20 |  Loss: (0.0778) | Acc: (97.51%) (2621/2688)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 30 |  Loss: (0.0832) | Acc: (97.25%) (3859/3968)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 40 |  Loss: (0.0846) | Acc: (97.20%) (5101/5248)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 50 |  Loss: (0.0812) | Acc: (97.27%) (6350/6528)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 60 |  Loss: (0.0819) | Acc: (97.20%) (7589/7808)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 70 |  Loss: (0.0845) | Acc: (97.16%) (8830/9088)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 80 |  Loss: (0.0837) | Acc: (97.13%) (10070/10368)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 90 |  Loss: (0.0824) | Acc: (97.17%) (11318/11648)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 100 |  Loss: (0.0847) | Acc: (97.11%) (12554/12928)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 110 |  Loss: (0.0862) | Acc: (97.04%) (13787/14208)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 120 |  Loss: (0.0862) | Acc: (97.07%) (15034/15488)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 130 |  Loss: (0.0872) | Acc: (97.05%) (16273/16768)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 140 |  Loss: (0.0874) | Acc: (97.05%) (17516/18048)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 150 |  Loss: (0.0884) | Acc: (97.02%) (18752/19328)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 160 |  Loss: (0.0897) | Acc: (96.99%) (19988/20608)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 170 |  Loss: (0.0904) | Acc: (96.97%) (21224/21888)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 180 |  Loss: (0.0898) | Acc: (96.98%) (22468/23168)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 190 |  Loss: (0.0901) | Acc: (96.96%) (23704/24448)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 200 |  Loss: (0.0910) | Acc: (96.94%) (24942/25728)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 210 |  Loss: (0.0907) | Acc: (96.95%) (26183/27008)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 220 |  Loss: (0.0907) | Acc: (96.94%) (27421/28288)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 230 |  Loss: (0.0921) | Acc: (96.89%) (28648/29568)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 240 |  Loss: (0.0928) | Acc: (96.88%) (29884/30848)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 250 |  Loss: (0.0927) | Acc: (96.88%) (31127/32128)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 260 |  Loss: (0.0918) | Acc: (96.91%) (32377/33408)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 270 |  Loss: (0.0916) | Acc: (96.92%) (33619/34688)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 280 |  Loss: (0.0911) | Acc: (96.93%) (34865/35968)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 290 |  Loss: (0.0913) | Acc: (96.92%) (36101/37248)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 300 |  Loss: (0.0914) | Acc: (96.90%) (37335/38528)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 310 |  Loss: (0.0915) | Acc: (96.89%) (38571/39808)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 320 |  Loss: (0.0912) | Acc: (96.91%) (39817/41088)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 330 |  Loss: (0.0914) | Acc: (96.91%) (41057/42368)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 340 |  Loss: (0.0919) | Acc: (96.89%) (42290/43648)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 350 |  Loss: (0.0916) | Acc: (96.88%) (43525/44928)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 360 |  Loss: (0.0918) | Acc: (96.88%) (44764/46208)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 370 |  Loss: (0.0920) | Acc: (96.86%) (45999/47488)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 380 |  Loss: (0.0917) | Acc: (96.88%) (47248/48768)\n",
      "#TRAIN: Epoch: 79 | Batch_idx: 390 |  Loss: (0.0922) | Acc: (96.88%) (48439/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5309) | Acc: (87.35%) (8735/10000)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 0 |  Loss: (0.0484) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 10 |  Loss: (0.0931) | Acc: (96.73%) (1362/1408)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 20 |  Loss: (0.0873) | Acc: (97.02%) (2608/2688)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 30 |  Loss: (0.0801) | Acc: (97.25%) (3859/3968)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 40 |  Loss: (0.0798) | Acc: (97.35%) (5109/5248)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 50 |  Loss: (0.0797) | Acc: (97.41%) (6359/6528)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 60 |  Loss: (0.0804) | Acc: (97.41%) (7606/7808)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 70 |  Loss: (0.0765) | Acc: (97.55%) (8865/9088)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 80 |  Loss: (0.0742) | Acc: (97.62%) (10121/10368)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 90 |  Loss: (0.0738) | Acc: (97.62%) (11371/11648)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 100 |  Loss: (0.0732) | Acc: (97.62%) (12620/12928)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 110 |  Loss: (0.0731) | Acc: (97.64%) (13872/14208)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 120 |  Loss: (0.0719) | Acc: (97.69%) (15131/15488)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 130 |  Loss: (0.0705) | Acc: (97.73%) (16387/16768)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 140 |  Loss: (0.0705) | Acc: (97.71%) (17635/18048)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 150 |  Loss: (0.0697) | Acc: (97.74%) (18892/19328)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 160 |  Loss: (0.0691) | Acc: (97.76%) (20146/20608)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 170 |  Loss: (0.0682) | Acc: (97.78%) (21402/21888)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 180 |  Loss: (0.0671) | Acc: (97.82%) (22663/23168)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 190 |  Loss: (0.0668) | Acc: (97.83%) (23918/24448)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 200 |  Loss: (0.0668) | Acc: (97.84%) (25172/25728)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 210 |  Loss: (0.0667) | Acc: (97.85%) (26426/27008)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 220 |  Loss: (0.0656) | Acc: (97.86%) (27684/28288)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 230 |  Loss: (0.0648) | Acc: (97.88%) (28940/29568)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 240 |  Loss: (0.0645) | Acc: (97.88%) (30194/30848)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 250 |  Loss: (0.0645) | Acc: (97.88%) (31446/32128)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 260 |  Loss: (0.0639) | Acc: (97.90%) (32706/33408)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 270 |  Loss: (0.0636) | Acc: (97.89%) (33955/34688)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 280 |  Loss: (0.0635) | Acc: (97.88%) (35204/35968)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 290 |  Loss: (0.0630) | Acc: (97.89%) (36462/37248)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 300 |  Loss: (0.0626) | Acc: (97.91%) (37724/38528)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 310 |  Loss: (0.0621) | Acc: (97.93%) (38983/39808)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 320 |  Loss: (0.0615) | Acc: (97.94%) (40242/41088)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 330 |  Loss: (0.0611) | Acc: (97.96%) (41504/42368)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 340 |  Loss: (0.0608) | Acc: (97.97%) (42764/43648)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 350 |  Loss: (0.0605) | Acc: (97.98%) (44022/44928)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 360 |  Loss: (0.0605) | Acc: (97.98%) (45276/46208)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 370 |  Loss: (0.0606) | Acc: (98.00%) (46537/47488)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 380 |  Loss: (0.0604) | Acc: (98.00%) (47792/48768)\n",
      "#TRAIN: Epoch: 80 | Batch_idx: 390 |  Loss: (0.0603) | Acc: (98.01%) (49004/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4800) | Acc: (88.69%) (8869/10000)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 0 |  Loss: (0.0269) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 10 |  Loss: (0.0526) | Acc: (98.15%) (1382/1408)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 20 |  Loss: (0.0528) | Acc: (98.29%) (2642/2688)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 30 |  Loss: (0.0480) | Acc: (98.51%) (3909/3968)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 40 |  Loss: (0.0488) | Acc: (98.42%) (5165/5248)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 50 |  Loss: (0.0512) | Acc: (98.31%) (6418/6528)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 60 |  Loss: (0.0502) | Acc: (98.32%) (7677/7808)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 70 |  Loss: (0.0502) | Acc: (98.29%) (8933/9088)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 80 |  Loss: (0.0498) | Acc: (98.32%) (10194/10368)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 90 |  Loss: (0.0506) | Acc: (98.27%) (11446/11648)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 100 |  Loss: (0.0517) | Acc: (98.21%) (12696/12928)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 110 |  Loss: (0.0516) | Acc: (98.23%) (13956/14208)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 120 |  Loss: (0.0508) | Acc: (98.24%) (15216/15488)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 130 |  Loss: (0.0513) | Acc: (98.25%) (16474/16768)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 140 |  Loss: (0.0509) | Acc: (98.25%) (17732/18048)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 150 |  Loss: (0.0510) | Acc: (98.26%) (18992/19328)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 160 |  Loss: (0.0501) | Acc: (98.29%) (20256/20608)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 170 |  Loss: (0.0508) | Acc: (98.27%) (21510/21888)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 180 |  Loss: (0.0504) | Acc: (98.30%) (22773/23168)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 190 |  Loss: (0.0499) | Acc: (98.31%) (24034/24448)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 200 |  Loss: (0.0506) | Acc: (98.28%) (25285/25728)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 210 |  Loss: (0.0509) | Acc: (98.25%) (26536/27008)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 220 |  Loss: (0.0517) | Acc: (98.24%) (27790/28288)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 230 |  Loss: (0.0520) | Acc: (98.25%) (29051/29568)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 240 |  Loss: (0.0516) | Acc: (98.26%) (30310/30848)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 250 |  Loss: (0.0515) | Acc: (98.26%) (31569/32128)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 260 |  Loss: (0.0509) | Acc: (98.29%) (32837/33408)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 270 |  Loss: (0.0508) | Acc: (98.30%) (34097/34688)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 280 |  Loss: (0.0506) | Acc: (98.30%) (35356/35968)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 290 |  Loss: (0.0503) | Acc: (98.30%) (36616/37248)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 300 |  Loss: (0.0502) | Acc: (98.31%) (37876/38528)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 310 |  Loss: (0.0502) | Acc: (98.30%) (39133/39808)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 320 |  Loss: (0.0500) | Acc: (98.31%) (40394/41088)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 330 |  Loss: (0.0497) | Acc: (98.33%) (41660/42368)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 340 |  Loss: (0.0500) | Acc: (98.32%) (42913/43648)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 350 |  Loss: (0.0501) | Acc: (98.32%) (44175/44928)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 360 |  Loss: (0.0503) | Acc: (98.30%) (45424/46208)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 370 |  Loss: (0.0505) | Acc: (98.30%) (46681/47488)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 380 |  Loss: (0.0503) | Acc: (98.30%) (47939/48768)\n",
      "#TRAIN: Epoch: 81 | Batch_idx: 390 |  Loss: (0.0504) | Acc: (98.30%) (49149/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4805) | Acc: (88.72%) (8872/10000)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 0 |  Loss: (0.0489) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 10 |  Loss: (0.0493) | Acc: (98.30%) (1384/1408)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 20 |  Loss: (0.0501) | Acc: (98.33%) (2643/2688)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 30 |  Loss: (0.0461) | Acc: (98.44%) (3906/3968)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 40 |  Loss: (0.0446) | Acc: (98.51%) (5170/5248)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 50 |  Loss: (0.0448) | Acc: (98.48%) (6429/6528)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 60 |  Loss: (0.0446) | Acc: (98.50%) (7691/7808)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 70 |  Loss: (0.0455) | Acc: (98.47%) (8949/9088)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 80 |  Loss: (0.0454) | Acc: (98.49%) (10211/10368)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 90 |  Loss: (0.0464) | Acc: (98.46%) (11469/11648)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 100 |  Loss: (0.0463) | Acc: (98.44%) (12726/12928)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 110 |  Loss: (0.0453) | Acc: (98.47%) (13991/14208)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 120 |  Loss: (0.0458) | Acc: (98.46%) (15249/15488)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 130 |  Loss: (0.0458) | Acc: (98.45%) (16508/16768)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 140 |  Loss: (0.0469) | Acc: (98.42%) (17763/18048)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 150 |  Loss: (0.0459) | Acc: (98.45%) (19029/19328)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 160 |  Loss: (0.0457) | Acc: (98.48%) (20295/20608)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 170 |  Loss: (0.0461) | Acc: (98.48%) (21556/21888)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 180 |  Loss: (0.0460) | Acc: (98.48%) (22815/23168)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 190 |  Loss: (0.0464) | Acc: (98.45%) (24068/24448)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 200 |  Loss: (0.0455) | Acc: (98.47%) (25334/25728)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 210 |  Loss: (0.0456) | Acc: (98.47%) (26595/27008)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 220 |  Loss: (0.0451) | Acc: (98.49%) (27862/28288)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 230 |  Loss: (0.0449) | Acc: (98.49%) (29123/29568)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 240 |  Loss: (0.0444) | Acc: (98.52%) (30391/30848)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 250 |  Loss: (0.0443) | Acc: (98.52%) (31652/32128)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 260 |  Loss: (0.0442) | Acc: (98.53%) (32918/33408)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 270 |  Loss: (0.0441) | Acc: (98.54%) (34181/34688)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 280 |  Loss: (0.0438) | Acc: (98.55%) (35446/35968)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 290 |  Loss: (0.0438) | Acc: (98.55%) (36708/37248)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 300 |  Loss: (0.0438) | Acc: (98.55%) (37969/38528)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 310 |  Loss: (0.0439) | Acc: (98.55%) (39232/39808)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 320 |  Loss: (0.0441) | Acc: (98.54%) (40490/41088)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 330 |  Loss: (0.0438) | Acc: (98.56%) (41757/42368)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 340 |  Loss: (0.0440) | Acc: (98.56%) (43018/43648)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 350 |  Loss: (0.0442) | Acc: (98.55%) (44276/44928)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 360 |  Loss: (0.0440) | Acc: (98.56%) (45541/46208)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 370 |  Loss: (0.0441) | Acc: (98.55%) (46798/47488)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 380 |  Loss: (0.0442) | Acc: (98.54%) (48055/48768)\n",
      "#TRAIN: Epoch: 82 | Batch_idx: 390 |  Loss: (0.0440) | Acc: (98.55%) (49274/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4821) | Acc: (89.06%) (8906/10000)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 0 |  Loss: (0.0460) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 10 |  Loss: (0.0352) | Acc: (99.01%) (1394/1408)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 20 |  Loss: (0.0378) | Acc: (98.96%) (2660/2688)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 30 |  Loss: (0.0401) | Acc: (98.74%) (3918/3968)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 40 |  Loss: (0.0376) | Acc: (98.88%) (5189/5248)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 50 |  Loss: (0.0375) | Acc: (98.88%) (6455/6528)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 60 |  Loss: (0.0407) | Acc: (98.78%) (7713/7808)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 70 |  Loss: (0.0407) | Acc: (98.75%) (8974/9088)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 80 |  Loss: (0.0397) | Acc: (98.78%) (10242/10368)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 90 |  Loss: (0.0402) | Acc: (98.77%) (11505/11648)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 100 |  Loss: (0.0397) | Acc: (98.77%) (12769/12928)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 110 |  Loss: (0.0392) | Acc: (98.80%) (14037/14208)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 120 |  Loss: (0.0388) | Acc: (98.83%) (15307/15488)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 130 |  Loss: (0.0394) | Acc: (98.83%) (16572/16768)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 140 |  Loss: (0.0394) | Acc: (98.84%) (17839/18048)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 150 |  Loss: (0.0401) | Acc: (98.80%) (19097/19328)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 160 |  Loss: (0.0399) | Acc: (98.80%) (20360/20608)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 170 |  Loss: (0.0392) | Acc: (98.83%) (21631/21888)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 180 |  Loss: (0.0391) | Acc: (98.83%) (22896/23168)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 190 |  Loss: (0.0391) | Acc: (98.82%) (24159/24448)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 200 |  Loss: (0.0393) | Acc: (98.83%) (25428/25728)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 210 |  Loss: (0.0391) | Acc: (98.85%) (26697/27008)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 220 |  Loss: (0.0390) | Acc: (98.85%) (27962/28288)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 230 |  Loss: (0.0392) | Acc: (98.82%) (29219/29568)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 240 |  Loss: (0.0393) | Acc: (98.82%) (30484/30848)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 250 |  Loss: (0.0395) | Acc: (98.80%) (31742/32128)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 260 |  Loss: (0.0399) | Acc: (98.78%) (33002/33408)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 270 |  Loss: (0.0396) | Acc: (98.79%) (34270/34688)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 280 |  Loss: (0.0403) | Acc: (98.77%) (35525/35968)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 290 |  Loss: (0.0399) | Acc: (98.78%) (36793/37248)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 300 |  Loss: (0.0398) | Acc: (98.78%) (38058/38528)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 310 |  Loss: (0.0397) | Acc: (98.79%) (39328/39808)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 320 |  Loss: (0.0396) | Acc: (98.79%) (40591/41088)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 330 |  Loss: (0.0398) | Acc: (98.79%) (41856/42368)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 340 |  Loss: (0.0400) | Acc: (98.78%) (43116/43648)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 350 |  Loss: (0.0398) | Acc: (98.78%) (44380/44928)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 360 |  Loss: (0.0397) | Acc: (98.78%) (45644/46208)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 370 |  Loss: (0.0400) | Acc: (98.77%) (46903/47488)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 380 |  Loss: (0.0398) | Acc: (98.77%) (48167/48768)\n",
      "#TRAIN: Epoch: 83 | Batch_idx: 390 |  Loss: (0.0395) | Acc: (98.77%) (49385/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4809) | Acc: (89.06%) (8906/10000)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 0 |  Loss: (0.0522) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 10 |  Loss: (0.0421) | Acc: (98.79%) (1391/1408)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 20 |  Loss: (0.0369) | Acc: (98.88%) (2658/2688)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 30 |  Loss: (0.0379) | Acc: (98.82%) (3921/3968)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 40 |  Loss: (0.0370) | Acc: (98.88%) (5189/5248)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 50 |  Loss: (0.0381) | Acc: (98.84%) (6452/6528)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 60 |  Loss: (0.0389) | Acc: (98.83%) (7717/7808)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 70 |  Loss: (0.0394) | Acc: (98.79%) (8978/9088)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 80 |  Loss: (0.0386) | Acc: (98.79%) (10243/10368)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 90 |  Loss: (0.0396) | Acc: (98.77%) (11505/11648)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 100 |  Loss: (0.0391) | Acc: (98.77%) (12769/12928)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 110 |  Loss: (0.0396) | Acc: (98.75%) (14030/14208)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 120 |  Loss: (0.0399) | Acc: (98.75%) (15295/15488)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 130 |  Loss: (0.0395) | Acc: (98.78%) (16564/16768)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 140 |  Loss: (0.0407) | Acc: (98.73%) (17818/18048)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 150 |  Loss: (0.0412) | Acc: (98.72%) (19080/19328)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 160 |  Loss: (0.0407) | Acc: (98.75%) (20350/20608)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 170 |  Loss: (0.0407) | Acc: (98.76%) (21616/21888)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 180 |  Loss: (0.0401) | Acc: (98.78%) (22885/23168)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 190 |  Loss: (0.0401) | Acc: (98.77%) (24148/24448)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 200 |  Loss: (0.0398) | Acc: (98.77%) (25411/25728)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 210 |  Loss: (0.0399) | Acc: (98.77%) (26677/27008)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 220 |  Loss: (0.0401) | Acc: (98.76%) (27938/28288)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 230 |  Loss: (0.0403) | Acc: (98.75%) (29199/29568)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 240 |  Loss: (0.0404) | Acc: (98.74%) (30460/30848)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 250 |  Loss: (0.0402) | Acc: (98.74%) (31724/32128)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 260 |  Loss: (0.0405) | Acc: (98.73%) (32983/33408)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 270 |  Loss: (0.0407) | Acc: (98.72%) (34244/34688)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 280 |  Loss: (0.0406) | Acc: (98.72%) (35507/35968)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 290 |  Loss: (0.0402) | Acc: (98.74%) (36777/37248)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 300 |  Loss: (0.0406) | Acc: (98.72%) (38034/38528)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 310 |  Loss: (0.0403) | Acc: (98.72%) (39298/39808)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 320 |  Loss: (0.0406) | Acc: (98.71%) (40558/41088)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 330 |  Loss: (0.0404) | Acc: (98.71%) (41822/42368)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 340 |  Loss: (0.0406) | Acc: (98.70%) (43082/43648)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 350 |  Loss: (0.0404) | Acc: (98.71%) (44347/44928)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 360 |  Loss: (0.0407) | Acc: (98.70%) (45607/46208)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 370 |  Loss: (0.0405) | Acc: (98.71%) (46875/47488)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 380 |  Loss: (0.0405) | Acc: (98.71%) (48137/48768)\n",
      "#TRAIN: Epoch: 84 | Batch_idx: 390 |  Loss: (0.0409) | Acc: (98.69%) (49343/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4917) | Acc: (88.84%) (8884/10000)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 0 |  Loss: (0.0269) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 10 |  Loss: (0.0407) | Acc: (98.79%) (1391/1408)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 20 |  Loss: (0.0396) | Acc: (98.70%) (2653/2688)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 30 |  Loss: (0.0374) | Acc: (98.71%) (3917/3968)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 40 |  Loss: (0.0395) | Acc: (98.69%) (5179/5248)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 50 |  Loss: (0.0417) | Acc: (98.62%) (6438/6528)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 60 |  Loss: (0.0396) | Acc: (98.69%) (7706/7808)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 70 |  Loss: (0.0389) | Acc: (98.77%) (8976/9088)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 80 |  Loss: (0.0385) | Acc: (98.81%) (10245/10368)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 90 |  Loss: (0.0392) | Acc: (98.79%) (11507/11648)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 100 |  Loss: (0.0387) | Acc: (98.78%) (12770/12928)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 110 |  Loss: (0.0389) | Acc: (98.78%) (14035/14208)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 120 |  Loss: (0.0392) | Acc: (98.75%) (15294/15488)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 130 |  Loss: (0.0384) | Acc: (98.79%) (16565/16768)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 140 |  Loss: (0.0389) | Acc: (98.76%) (17825/18048)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 150 |  Loss: (0.0385) | Acc: (98.78%) (19093/19328)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 160 |  Loss: (0.0378) | Acc: (98.82%) (20365/20608)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 170 |  Loss: (0.0378) | Acc: (98.81%) (21628/21888)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 180 |  Loss: (0.0381) | Acc: (98.80%) (22889/23168)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 190 |  Loss: (0.0380) | Acc: (98.81%) (24157/24448)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 200 |  Loss: (0.0378) | Acc: (98.82%) (25424/25728)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 210 |  Loss: (0.0379) | Acc: (98.82%) (26688/27008)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 220 |  Loss: (0.0379) | Acc: (98.83%) (27956/28288)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 230 |  Loss: (0.0381) | Acc: (98.81%) (29216/29568)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 240 |  Loss: (0.0384) | Acc: (98.81%) (30480/30848)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 250 |  Loss: (0.0384) | Acc: (98.80%) (31742/32128)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 260 |  Loss: (0.0386) | Acc: (98.78%) (33000/33408)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 270 |  Loss: (0.0383) | Acc: (98.79%) (34268/34688)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 280 |  Loss: (0.0384) | Acc: (98.78%) (35530/35968)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 290 |  Loss: (0.0387) | Acc: (98.78%) (36793/37248)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 300 |  Loss: (0.0388) | Acc: (98.77%) (38054/38528)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 310 |  Loss: (0.0385) | Acc: (98.78%) (39321/39808)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 320 |  Loss: (0.0386) | Acc: (98.78%) (40585/41088)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 330 |  Loss: (0.0382) | Acc: (98.79%) (41856/42368)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 340 |  Loss: (0.0381) | Acc: (98.79%) (43119/43648)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 350 |  Loss: (0.0380) | Acc: (98.79%) (44385/44928)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 360 |  Loss: (0.0380) | Acc: (98.79%) (45650/46208)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 370 |  Loss: (0.0383) | Acc: (98.78%) (46911/47488)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 380 |  Loss: (0.0380) | Acc: (98.79%) (48178/48768)\n",
      "#TRAIN: Epoch: 85 | Batch_idx: 390 |  Loss: (0.0381) | Acc: (98.78%) (49390/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4896) | Acc: (89.03%) (8903/10000)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 0 |  Loss: (0.0132) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 10 |  Loss: (0.0323) | Acc: (99.29%) (1398/1408)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 20 |  Loss: (0.0367) | Acc: (99.03%) (2662/2688)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 30 |  Loss: (0.0366) | Acc: (98.92%) (3925/3968)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 40 |  Loss: (0.0344) | Acc: (99.01%) (5196/5248)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 50 |  Loss: (0.0359) | Acc: (98.99%) (6462/6528)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 60 |  Loss: (0.0361) | Acc: (98.92%) (7724/7808)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 70 |  Loss: (0.0354) | Acc: (98.93%) (8991/9088)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 80 |  Loss: (0.0345) | Acc: (98.92%) (10256/10368)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 90 |  Loss: (0.0355) | Acc: (98.90%) (11520/11648)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 100 |  Loss: (0.0345) | Acc: (98.95%) (12792/12928)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 110 |  Loss: (0.0354) | Acc: (98.92%) (14055/14208)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 120 |  Loss: (0.0355) | Acc: (98.92%) (15321/15488)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 130 |  Loss: (0.0346) | Acc: (98.96%) (16593/16768)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 140 |  Loss: (0.0344) | Acc: (98.95%) (17858/18048)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 150 |  Loss: (0.0343) | Acc: (98.93%) (19122/19328)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 160 |  Loss: (0.0350) | Acc: (98.90%) (20381/20608)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 170 |  Loss: (0.0350) | Acc: (98.91%) (21649/21888)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 180 |  Loss: (0.0352) | Acc: (98.91%) (22915/23168)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 190 |  Loss: (0.0350) | Acc: (98.89%) (24177/24448)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 200 |  Loss: (0.0353) | Acc: (98.87%) (25438/25728)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 210 |  Loss: (0.0353) | Acc: (98.87%) (26703/27008)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 220 |  Loss: (0.0356) | Acc: (98.87%) (27968/28288)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 230 |  Loss: (0.0354) | Acc: (98.88%) (29236/29568)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 240 |  Loss: (0.0351) | Acc: (98.88%) (30504/30848)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 250 |  Loss: (0.0352) | Acc: (98.87%) (31766/32128)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 260 |  Loss: (0.0352) | Acc: (98.87%) (33030/33408)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 270 |  Loss: (0.0353) | Acc: (98.86%) (34292/34688)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 280 |  Loss: (0.0355) | Acc: (98.86%) (35558/35968)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 290 |  Loss: (0.0353) | Acc: (98.86%) (36823/37248)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 300 |  Loss: (0.0353) | Acc: (98.87%) (38092/38528)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 310 |  Loss: (0.0352) | Acc: (98.87%) (39359/39808)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 320 |  Loss: (0.0350) | Acc: (98.88%) (40628/41088)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 330 |  Loss: (0.0349) | Acc: (98.88%) (41893/42368)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 340 |  Loss: (0.0349) | Acc: (98.88%) (43157/43648)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 350 |  Loss: (0.0350) | Acc: (98.88%) (44425/44928)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 360 |  Loss: (0.0349) | Acc: (98.89%) (45693/46208)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 370 |  Loss: (0.0347) | Acc: (98.90%) (46964/47488)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 380 |  Loss: (0.0346) | Acc: (98.90%) (48233/48768)\n",
      "#TRAIN: Epoch: 86 | Batch_idx: 390 |  Loss: (0.0346) | Acc: (98.90%) (49450/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4928) | Acc: (89.16%) (8916/10000)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 0 |  Loss: (0.0248) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 10 |  Loss: (0.0312) | Acc: (99.01%) (1394/1408)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 20 |  Loss: (0.0300) | Acc: (98.92%) (2659/2688)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 30 |  Loss: (0.0307) | Acc: (98.94%) (3926/3968)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 40 |  Loss: (0.0287) | Acc: (99.10%) (5201/5248)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 50 |  Loss: (0.0304) | Acc: (99.00%) (6463/6528)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 60 |  Loss: (0.0310) | Acc: (98.98%) (7728/7808)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 70 |  Loss: (0.0317) | Acc: (98.98%) (8995/9088)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 80 |  Loss: (0.0313) | Acc: (98.99%) (10263/10368)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 90 |  Loss: (0.0322) | Acc: (98.94%) (11525/11648)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 100 |  Loss: (0.0324) | Acc: (98.94%) (12791/12928)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 110 |  Loss: (0.0326) | Acc: (98.94%) (14058/14208)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 120 |  Loss: (0.0326) | Acc: (98.95%) (15326/15488)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 130 |  Loss: (0.0326) | Acc: (98.95%) (16592/16768)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 140 |  Loss: (0.0330) | Acc: (98.94%) (17856/18048)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 150 |  Loss: (0.0324) | Acc: (98.95%) (19125/19328)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 160 |  Loss: (0.0329) | Acc: (98.94%) (20390/20608)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 170 |  Loss: (0.0328) | Acc: (98.95%) (21659/21888)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 180 |  Loss: (0.0328) | Acc: (98.96%) (22927/23168)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 190 |  Loss: (0.0326) | Acc: (98.97%) (24196/24448)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 200 |  Loss: (0.0328) | Acc: (98.97%) (25462/25728)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 210 |  Loss: (0.0333) | Acc: (98.94%) (26723/27008)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 220 |  Loss: (0.0333) | Acc: (98.95%) (27991/28288)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 230 |  Loss: (0.0334) | Acc: (98.94%) (29255/29568)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 240 |  Loss: (0.0340) | Acc: (98.92%) (30514/30848)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 250 |  Loss: (0.0343) | Acc: (98.90%) (31775/32128)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 260 |  Loss: (0.0345) | Acc: (98.90%) (33040/33408)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 270 |  Loss: (0.0346) | Acc: (98.91%) (34309/34688)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 280 |  Loss: (0.0347) | Acc: (98.89%) (35567/35968)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 290 |  Loss: (0.0349) | Acc: (98.87%) (36828/37248)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 300 |  Loss: (0.0348) | Acc: (98.88%) (38097/38528)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 310 |  Loss: (0.0345) | Acc: (98.88%) (39364/39808)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 320 |  Loss: (0.0347) | Acc: (98.88%) (40626/41088)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 330 |  Loss: (0.0348) | Acc: (98.87%) (41889/42368)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 340 |  Loss: (0.0351) | Acc: (98.87%) (43154/43648)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 350 |  Loss: (0.0351) | Acc: (98.87%) (44419/44928)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 360 |  Loss: (0.0349) | Acc: (98.87%) (45686/46208)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 370 |  Loss: (0.0349) | Acc: (98.86%) (46948/47488)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 380 |  Loss: (0.0347) | Acc: (98.87%) (48216/48768)\n",
      "#TRAIN: Epoch: 87 | Batch_idx: 390 |  Loss: (0.0349) | Acc: (98.87%) (49433/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4942) | Acc: (89.08%) (8908/10000)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 0 |  Loss: (0.0712) | Acc: (96.09%) (123/128)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 10 |  Loss: (0.0357) | Acc: (98.79%) (1391/1408)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 20 |  Loss: (0.0355) | Acc: (98.85%) (2657/2688)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 30 |  Loss: (0.0358) | Acc: (98.89%) (3924/3968)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 40 |  Loss: (0.0359) | Acc: (98.86%) (5188/5248)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 50 |  Loss: (0.0331) | Acc: (98.93%) (6458/6528)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 60 |  Loss: (0.0347) | Acc: (98.85%) (7718/7808)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 70 |  Loss: (0.0347) | Acc: (98.90%) (8988/9088)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 80 |  Loss: (0.0335) | Acc: (98.93%) (10257/10368)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 90 |  Loss: (0.0341) | Acc: (98.93%) (11523/11648)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 100 |  Loss: (0.0336) | Acc: (98.95%) (12792/12928)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 110 |  Loss: (0.0330) | Acc: (98.98%) (14063/14208)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 120 |  Loss: (0.0333) | Acc: (98.99%) (15331/15488)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 130 |  Loss: (0.0338) | Acc: (98.99%) (16599/16768)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 140 |  Loss: (0.0340) | Acc: (98.97%) (17862/18048)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 150 |  Loss: (0.0339) | Acc: (98.98%) (19130/19328)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 160 |  Loss: (0.0342) | Acc: (98.95%) (20391/20608)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 170 |  Loss: (0.0338) | Acc: (98.95%) (21658/21888)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 180 |  Loss: (0.0334) | Acc: (98.98%) (22931/23168)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 190 |  Loss: (0.0329) | Acc: (98.99%) (24201/24448)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 200 |  Loss: (0.0325) | Acc: (98.99%) (25468/25728)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 210 |  Loss: (0.0324) | Acc: (99.00%) (26738/27008)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 220 |  Loss: (0.0328) | Acc: (98.97%) (27997/28288)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 230 |  Loss: (0.0324) | Acc: (98.99%) (29268/29568)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 240 |  Loss: (0.0319) | Acc: (99.00%) (30540/30848)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 250 |  Loss: (0.0317) | Acc: (99.01%) (31810/32128)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 260 |  Loss: (0.0321) | Acc: (98.98%) (33066/33408)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 270 |  Loss: (0.0321) | Acc: (98.99%) (34336/34688)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 280 |  Loss: (0.0319) | Acc: (98.99%) (35606/35968)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 290 |  Loss: (0.0319) | Acc: (99.00%) (36877/37248)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 300 |  Loss: (0.0319) | Acc: (99.01%) (38145/38528)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 310 |  Loss: (0.0320) | Acc: (99.00%) (39411/39808)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 320 |  Loss: (0.0323) | Acc: (99.00%) (40677/41088)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 330 |  Loss: (0.0321) | Acc: (99.00%) (41945/42368)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 340 |  Loss: (0.0321) | Acc: (99.00%) (43210/43648)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 350 |  Loss: (0.0321) | Acc: (99.00%) (44477/44928)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 360 |  Loss: (0.0324) | Acc: (98.99%) (45741/46208)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 370 |  Loss: (0.0326) | Acc: (98.98%) (47003/47488)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 380 |  Loss: (0.0326) | Acc: (98.98%) (48272/48768)\n",
      "#TRAIN: Epoch: 88 | Batch_idx: 390 |  Loss: (0.0327) | Acc: (98.98%) (49491/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.4984) | Acc: (89.05%) (8905/10000)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 0 |  Loss: (0.0195) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 10 |  Loss: (0.0304) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 20 |  Loss: (0.0349) | Acc: (98.88%) (2658/2688)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 30 |  Loss: (0.0324) | Acc: (98.94%) (3926/3968)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 40 |  Loss: (0.0324) | Acc: (98.99%) (5195/5248)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 50 |  Loss: (0.0305) | Acc: (99.08%) (6468/6528)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 60 |  Loss: (0.0319) | Acc: (99.03%) (7732/7808)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 70 |  Loss: (0.0320) | Acc: (99.01%) (8998/9088)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 80 |  Loss: (0.0312) | Acc: (99.05%) (10269/10368)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 90 |  Loss: (0.0306) | Acc: (99.07%) (11540/11648)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 100 |  Loss: (0.0319) | Acc: (99.05%) (12805/12928)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 110 |  Loss: (0.0328) | Acc: (99.01%) (14067/14208)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 120 |  Loss: (0.0324) | Acc: (99.02%) (15336/15488)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 130 |  Loss: (0.0325) | Acc: (99.02%) (16603/16768)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 140 |  Loss: (0.0329) | Acc: (99.00%) (17867/18048)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 150 |  Loss: (0.0325) | Acc: (99.01%) (19137/19328)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 160 |  Loss: (0.0322) | Acc: (99.01%) (20404/20608)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 170 |  Loss: (0.0324) | Acc: (98.98%) (21665/21888)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 180 |  Loss: (0.0324) | Acc: (98.98%) (22932/23168)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 190 |  Loss: (0.0326) | Acc: (98.96%) (24193/24448)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 200 |  Loss: (0.0323) | Acc: (98.95%) (25459/25728)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 210 |  Loss: (0.0319) | Acc: (98.98%) (26732/27008)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 220 |  Loss: (0.0322) | Acc: (98.97%) (27998/28288)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 230 |  Loss: (0.0321) | Acc: (98.97%) (29262/29568)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 240 |  Loss: (0.0319) | Acc: (98.98%) (30534/30848)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 250 |  Loss: (0.0320) | Acc: (98.98%) (31801/32128)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 260 |  Loss: (0.0318) | Acc: (98.98%) (33068/33408)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 270 |  Loss: (0.0324) | Acc: (98.97%) (34331/34688)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 280 |  Loss: (0.0326) | Acc: (98.97%) (35596/35968)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 290 |  Loss: (0.0324) | Acc: (98.97%) (36863/37248)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 300 |  Loss: (0.0321) | Acc: (98.98%) (38134/38528)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 310 |  Loss: (0.0320) | Acc: (98.98%) (39401/39808)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 320 |  Loss: (0.0319) | Acc: (98.99%) (40671/41088)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 330 |  Loss: (0.0320) | Acc: (98.99%) (41938/42368)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 340 |  Loss: (0.0320) | Acc: (98.99%) (43206/43648)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 350 |  Loss: (0.0323) | Acc: (98.98%) (44468/44928)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 360 |  Loss: (0.0323) | Acc: (98.97%) (45732/46208)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 370 |  Loss: (0.0322) | Acc: (98.97%) (47000/47488)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 380 |  Loss: (0.0325) | Acc: (98.96%) (48263/48768)\n",
      "#TRAIN: Epoch: 89 | Batch_idx: 390 |  Loss: (0.0324) | Acc: (98.97%) (49484/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5082) | Acc: (88.87%) (8887/10000)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 0 |  Loss: (0.0243) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 10 |  Loss: (0.0187) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 20 |  Loss: (0.0279) | Acc: (99.03%) (2662/2688)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 30 |  Loss: (0.0297) | Acc: (98.99%) (3928/3968)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 40 |  Loss: (0.0313) | Acc: (98.91%) (5191/5248)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 50 |  Loss: (0.0316) | Acc: (98.90%) (6456/6528)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 60 |  Loss: (0.0305) | Acc: (98.96%) (7727/7808)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 70 |  Loss: (0.0312) | Acc: (98.94%) (8992/9088)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 80 |  Loss: (0.0319) | Acc: (98.93%) (10257/10368)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 90 |  Loss: (0.0315) | Acc: (98.94%) (11524/11648)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 100 |  Loss: (0.0313) | Acc: (98.93%) (12790/12928)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 110 |  Loss: (0.0317) | Acc: (98.94%) (14057/14208)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 120 |  Loss: (0.0315) | Acc: (98.97%) (15329/15488)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 130 |  Loss: (0.0307) | Acc: (99.00%) (16601/16768)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 140 |  Loss: (0.0310) | Acc: (98.97%) (17862/18048)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 150 |  Loss: (0.0310) | Acc: (98.98%) (19130/19328)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 160 |  Loss: (0.0304) | Acc: (99.01%) (20405/20608)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 170 |  Loss: (0.0304) | Acc: (99.02%) (21673/21888)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 180 |  Loss: (0.0306) | Acc: (99.00%) (22937/23168)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 190 |  Loss: (0.0308) | Acc: (98.99%) (24202/24448)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 200 |  Loss: (0.0311) | Acc: (98.99%) (25468/25728)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 210 |  Loss: (0.0306) | Acc: (99.01%) (26741/27008)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 220 |  Loss: (0.0307) | Acc: (99.01%) (28008/28288)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 230 |  Loss: (0.0305) | Acc: (99.01%) (29275/29568)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 240 |  Loss: (0.0303) | Acc: (99.01%) (30544/30848)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 250 |  Loss: (0.0309) | Acc: (98.99%) (31805/32128)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 260 |  Loss: (0.0310) | Acc: (98.99%) (33072/33408)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 270 |  Loss: (0.0306) | Acc: (99.01%) (34345/34688)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 280 |  Loss: (0.0311) | Acc: (98.99%) (35606/35968)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 290 |  Loss: (0.0313) | Acc: (98.99%) (36872/37248)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 300 |  Loss: (0.0313) | Acc: (98.99%) (38140/38528)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 310 |  Loss: (0.0314) | Acc: (98.98%) (39403/39808)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 320 |  Loss: (0.0314) | Acc: (98.98%) (40670/41088)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 330 |  Loss: (0.0319) | Acc: (98.97%) (41932/42368)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 340 |  Loss: (0.0323) | Acc: (98.96%) (43195/43648)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 350 |  Loss: (0.0323) | Acc: (98.97%) (44463/44928)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 360 |  Loss: (0.0323) | Acc: (98.97%) (45732/46208)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 370 |  Loss: (0.0324) | Acc: (98.97%) (46997/47488)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 380 |  Loss: (0.0325) | Acc: (98.97%) (48265/48768)\n",
      "#TRAIN: Epoch: 90 | Batch_idx: 390 |  Loss: (0.0323) | Acc: (98.98%) (49490/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5025) | Acc: (89.07%) (8907/10000)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 0 |  Loss: (0.0355) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 10 |  Loss: (0.0324) | Acc: (98.79%) (1391/1408)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 20 |  Loss: (0.0326) | Acc: (98.92%) (2659/2688)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 30 |  Loss: (0.0323) | Acc: (98.92%) (3925/3968)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 40 |  Loss: (0.0311) | Acc: (98.95%) (5193/5248)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 50 |  Loss: (0.0312) | Acc: (98.94%) (6459/6528)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 60 |  Loss: (0.0307) | Acc: (98.96%) (7727/7808)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 70 |  Loss: (0.0318) | Acc: (98.94%) (8992/9088)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 80 |  Loss: (0.0304) | Acc: (99.00%) (10264/10368)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 90 |  Loss: (0.0322) | Acc: (98.97%) (11528/11648)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 100 |  Loss: (0.0319) | Acc: (98.98%) (12796/12928)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 110 |  Loss: (0.0320) | Acc: (98.95%) (14059/14208)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 120 |  Loss: (0.0325) | Acc: (98.92%) (15321/15488)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 130 |  Loss: (0.0317) | Acc: (98.96%) (16593/16768)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 140 |  Loss: (0.0326) | Acc: (98.93%) (17855/18048)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 150 |  Loss: (0.0319) | Acc: (98.95%) (19126/19328)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 160 |  Loss: (0.0328) | Acc: (98.92%) (20385/20608)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 170 |  Loss: (0.0327) | Acc: (98.91%) (21649/21888)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 180 |  Loss: (0.0326) | Acc: (98.92%) (22918/23168)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 190 |  Loss: (0.0322) | Acc: (98.94%) (24189/24448)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 200 |  Loss: (0.0320) | Acc: (98.94%) (25455/25728)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 210 |  Loss: (0.0323) | Acc: (98.94%) (26721/27008)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 220 |  Loss: (0.0323) | Acc: (98.93%) (27986/28288)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 230 |  Loss: (0.0318) | Acc: (98.95%) (29257/29568)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 240 |  Loss: (0.0315) | Acc: (98.95%) (30525/30848)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 250 |  Loss: (0.0319) | Acc: (98.94%) (31786/32128)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 260 |  Loss: (0.0318) | Acc: (98.95%) (33056/33408)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 270 |  Loss: (0.0314) | Acc: (98.97%) (34331/34688)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 280 |  Loss: (0.0314) | Acc: (98.98%) (35600/35968)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 290 |  Loss: (0.0316) | Acc: (98.97%) (36865/37248)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 300 |  Loss: (0.0314) | Acc: (98.98%) (38135/38528)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 310 |  Loss: (0.0319) | Acc: (98.98%) (39400/39808)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 320 |  Loss: (0.0319) | Acc: (98.98%) (40668/41088)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 330 |  Loss: (0.0320) | Acc: (98.96%) (41928/42368)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 340 |  Loss: (0.0321) | Acc: (98.96%) (43192/43648)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 350 |  Loss: (0.0319) | Acc: (98.97%) (44465/44928)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 360 |  Loss: (0.0317) | Acc: (98.98%) (45738/46208)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 370 |  Loss: (0.0315) | Acc: (98.99%) (47009/47488)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 380 |  Loss: (0.0319) | Acc: (98.98%) (48272/48768)\n",
      "#TRAIN: Epoch: 91 | Batch_idx: 390 |  Loss: (0.0318) | Acc: (98.99%) (49495/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5101) | Acc: (89.04%) (8904/10000)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 0 |  Loss: (0.0171) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 10 |  Loss: (0.0235) | Acc: (99.29%) (1398/1408)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 20 |  Loss: (0.0272) | Acc: (98.96%) (2660/2688)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 30 |  Loss: (0.0275) | Acc: (98.94%) (3926/3968)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 40 |  Loss: (0.0282) | Acc: (98.95%) (5193/5248)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 50 |  Loss: (0.0297) | Acc: (98.97%) (6461/6528)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 60 |  Loss: (0.0309) | Acc: (98.96%) (7727/7808)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 70 |  Loss: (0.0306) | Acc: (98.99%) (8996/9088)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 80 |  Loss: (0.0308) | Acc: (99.00%) (10264/10368)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 90 |  Loss: (0.0302) | Acc: (98.99%) (11530/11648)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 100 |  Loss: (0.0297) | Acc: (99.00%) (12799/12928)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 110 |  Loss: (0.0287) | Acc: (99.06%) (14075/14208)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 120 |  Loss: (0.0289) | Acc: (99.06%) (15343/15488)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 130 |  Loss: (0.0287) | Acc: (99.08%) (16614/16768)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 140 |  Loss: (0.0293) | Acc: (99.09%) (17883/18048)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 150 |  Loss: (0.0289) | Acc: (99.09%) (19152/19328)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 160 |  Loss: (0.0291) | Acc: (99.08%) (20419/20608)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 170 |  Loss: (0.0291) | Acc: (99.08%) (21687/21888)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 180 |  Loss: (0.0288) | Acc: (99.10%) (22959/23168)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 190 |  Loss: (0.0289) | Acc: (99.08%) (24223/24448)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 200 |  Loss: (0.0286) | Acc: (99.10%) (25497/25728)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 210 |  Loss: (0.0285) | Acc: (99.10%) (26766/27008)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 220 |  Loss: (0.0289) | Acc: (99.08%) (28029/28288)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 230 |  Loss: (0.0292) | Acc: (99.08%) (29297/29568)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 240 |  Loss: (0.0290) | Acc: (99.10%) (30570/30848)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 250 |  Loss: (0.0288) | Acc: (99.12%) (31845/32128)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 260 |  Loss: (0.0292) | Acc: (99.11%) (33112/33408)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 270 |  Loss: (0.0292) | Acc: (99.11%) (34379/34688)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 280 |  Loss: (0.0294) | Acc: (99.10%) (35646/35968)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 290 |  Loss: (0.0296) | Acc: (99.10%) (36913/37248)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 300 |  Loss: (0.0296) | Acc: (99.09%) (38176/38528)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 310 |  Loss: (0.0296) | Acc: (99.09%) (39444/39808)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 320 |  Loss: (0.0295) | Acc: (99.09%) (40715/41088)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 330 |  Loss: (0.0293) | Acc: (99.10%) (41986/42368)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 340 |  Loss: (0.0293) | Acc: (99.10%) (43253/43648)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 350 |  Loss: (0.0294) | Acc: (99.10%) (44524/44928)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 360 |  Loss: (0.0293) | Acc: (99.10%) (45793/46208)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 370 |  Loss: (0.0294) | Acc: (99.09%) (47058/47488)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 380 |  Loss: (0.0294) | Acc: (99.09%) (48325/48768)\n",
      "#TRAIN: Epoch: 92 | Batch_idx: 390 |  Loss: (0.0292) | Acc: (99.10%) (49550/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5081) | Acc: (89.15%) (8915/10000)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 0 |  Loss: (0.0066) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 10 |  Loss: (0.0313) | Acc: (99.08%) (1395/1408)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 20 |  Loss: (0.0290) | Acc: (99.11%) (2664/2688)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 30 |  Loss: (0.0290) | Acc: (99.09%) (3932/3968)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 40 |  Loss: (0.0285) | Acc: (99.10%) (5201/5248)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 50 |  Loss: (0.0289) | Acc: (99.03%) (6465/6528)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 60 |  Loss: (0.0302) | Acc: (99.05%) (7734/7808)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 70 |  Loss: (0.0290) | Acc: (99.10%) (9006/9088)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 80 |  Loss: (0.0285) | Acc: (99.12%) (10277/10368)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 90 |  Loss: (0.0281) | Acc: (99.12%) (11545/11648)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 100 |  Loss: (0.0286) | Acc: (99.07%) (12808/12928)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 110 |  Loss: (0.0280) | Acc: (99.09%) (14079/14208)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 120 |  Loss: (0.0281) | Acc: (99.08%) (15346/15488)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 130 |  Loss: (0.0289) | Acc: (99.09%) (16615/16768)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 140 |  Loss: (0.0294) | Acc: (99.06%) (17879/18048)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 150 |  Loss: (0.0298) | Acc: (99.06%) (19147/19328)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 160 |  Loss: (0.0296) | Acc: (99.07%) (20416/20608)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 170 |  Loss: (0.0301) | Acc: (99.05%) (21680/21888)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 180 |  Loss: (0.0298) | Acc: (99.06%) (22951/23168)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 190 |  Loss: (0.0291) | Acc: (99.09%) (24225/24448)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 200 |  Loss: (0.0295) | Acc: (99.05%) (25484/25728)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 210 |  Loss: (0.0294) | Acc: (99.06%) (26753/27008)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 220 |  Loss: (0.0296) | Acc: (99.05%) (28019/28288)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 230 |  Loss: (0.0294) | Acc: (99.06%) (29290/29568)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 240 |  Loss: (0.0296) | Acc: (99.06%) (30557/30848)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 250 |  Loss: (0.0299) | Acc: (99.04%) (31821/32128)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 260 |  Loss: (0.0295) | Acc: (99.06%) (33095/33408)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 270 |  Loss: (0.0293) | Acc: (99.07%) (34365/34688)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 280 |  Loss: (0.0291) | Acc: (99.07%) (35635/35968)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 290 |  Loss: (0.0292) | Acc: (99.07%) (36902/37248)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 300 |  Loss: (0.0289) | Acc: (99.08%) (38175/38528)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 310 |  Loss: (0.0292) | Acc: (99.07%) (39438/39808)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 320 |  Loss: (0.0291) | Acc: (99.08%) (40711/41088)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 330 |  Loss: (0.0293) | Acc: (99.07%) (41975/42368)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 340 |  Loss: (0.0292) | Acc: (99.08%) (43245/43648)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 350 |  Loss: (0.0291) | Acc: (99.07%) (44511/44928)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 360 |  Loss: (0.0290) | Acc: (99.07%) (45779/46208)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 370 |  Loss: (0.0290) | Acc: (99.06%) (47043/47488)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 380 |  Loss: (0.0289) | Acc: (99.07%) (48315/48768)\n",
      "#TRAIN: Epoch: 93 | Batch_idx: 390 |  Loss: (0.0286) | Acc: (99.08%) (49538/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5111) | Acc: (89.02%) (8902/10000)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 0 |  Loss: (0.0381) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 10 |  Loss: (0.0232) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 20 |  Loss: (0.0230) | Acc: (99.29%) (2669/2688)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 30 |  Loss: (0.0244) | Acc: (99.19%) (3936/3968)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 40 |  Loss: (0.0270) | Acc: (99.12%) (5202/5248)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 50 |  Loss: (0.0288) | Acc: (99.05%) (6466/6528)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 60 |  Loss: (0.0291) | Acc: (99.04%) (7733/7808)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 70 |  Loss: (0.0279) | Acc: (99.08%) (9004/9088)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 80 |  Loss: (0.0269) | Acc: (99.11%) (10276/10368)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 90 |  Loss: (0.0271) | Acc: (99.10%) (11543/11648)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 100 |  Loss: (0.0274) | Acc: (99.10%) (12812/12928)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 110 |  Loss: (0.0270) | Acc: (99.11%) (14082/14208)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 120 |  Loss: (0.0272) | Acc: (99.12%) (15351/15488)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 130 |  Loss: (0.0269) | Acc: (99.13%) (16622/16768)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 140 |  Loss: (0.0272) | Acc: (99.12%) (17890/18048)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 150 |  Loss: (0.0278) | Acc: (99.10%) (19155/19328)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 160 |  Loss: (0.0275) | Acc: (99.12%) (20426/20608)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 170 |  Loss: (0.0277) | Acc: (99.10%) (21691/21888)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 180 |  Loss: (0.0276) | Acc: (99.08%) (22956/23168)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 190 |  Loss: (0.0275) | Acc: (99.09%) (24225/24448)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 200 |  Loss: (0.0280) | Acc: (99.07%) (25490/25728)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 210 |  Loss: (0.0281) | Acc: (99.08%) (26759/27008)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 220 |  Loss: (0.0284) | Acc: (99.06%) (28023/28288)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 230 |  Loss: (0.0285) | Acc: (99.06%) (29291/29568)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 240 |  Loss: (0.0287) | Acc: (99.05%) (30556/30848)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 250 |  Loss: (0.0288) | Acc: (99.05%) (31824/32128)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 260 |  Loss: (0.0285) | Acc: (99.06%) (33094/33408)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 270 |  Loss: (0.0287) | Acc: (99.06%) (34361/34688)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 280 |  Loss: (0.0289) | Acc: (99.05%) (35628/35968)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 290 |  Loss: (0.0291) | Acc: (99.04%) (36892/37248)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 300 |  Loss: (0.0294) | Acc: (99.03%) (38155/38528)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 310 |  Loss: (0.0295) | Acc: (99.03%) (39422/39808)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 320 |  Loss: (0.0293) | Acc: (99.04%) (40694/41088)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 330 |  Loss: (0.0292) | Acc: (99.05%) (41965/42368)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 340 |  Loss: (0.0292) | Acc: (99.06%) (43237/43648)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 350 |  Loss: (0.0291) | Acc: (99.06%) (44504/44928)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 360 |  Loss: (0.0290) | Acc: (99.05%) (45771/46208)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 370 |  Loss: (0.0289) | Acc: (99.06%) (47041/47488)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 380 |  Loss: (0.0288) | Acc: (99.07%) (48314/48768)\n",
      "#TRAIN: Epoch: 94 | Batch_idx: 390 |  Loss: (0.0286) | Acc: (99.07%) (49534/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5174) | Acc: (88.98%) (8898/10000)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 0 |  Loss: (0.0028) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 10 |  Loss: (0.0235) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 20 |  Loss: (0.0254) | Acc: (99.26%) (2668/2688)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 30 |  Loss: (0.0278) | Acc: (99.07%) (3931/3968)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 40 |  Loss: (0.0268) | Acc: (99.07%) (5199/5248)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 50 |  Loss: (0.0262) | Acc: (99.13%) (6471/6528)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 60 |  Loss: (0.0256) | Acc: (99.18%) (7744/7808)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 70 |  Loss: (0.0255) | Acc: (99.21%) (9016/9088)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 80 |  Loss: (0.0270) | Acc: (99.16%) (10281/10368)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 90 |  Loss: (0.0267) | Acc: (99.17%) (11551/11648)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 100 |  Loss: (0.0269) | Acc: (99.14%) (12817/12928)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 110 |  Loss: (0.0270) | Acc: (99.17%) (14090/14208)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 120 |  Loss: (0.0280) | Acc: (99.12%) (15352/15488)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 130 |  Loss: (0.0280) | Acc: (99.11%) (16618/16768)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 140 |  Loss: (0.0275) | Acc: (99.12%) (17890/18048)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 150 |  Loss: (0.0280) | Acc: (99.13%) (19160/19328)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 160 |  Loss: (0.0280) | Acc: (99.12%) (20427/20608)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 170 |  Loss: (0.0285) | Acc: (99.10%) (21691/21888)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 180 |  Loss: (0.0283) | Acc: (99.11%) (22962/23168)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 190 |  Loss: (0.0281) | Acc: (99.12%) (24234/24448)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 200 |  Loss: (0.0278) | Acc: (99.14%) (25506/25728)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 210 |  Loss: (0.0277) | Acc: (99.14%) (26777/27008)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 220 |  Loss: (0.0275) | Acc: (99.14%) (28046/28288)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 230 |  Loss: (0.0279) | Acc: (99.13%) (29311/29568)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 240 |  Loss: (0.0284) | Acc: (99.12%) (30575/30848)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 250 |  Loss: (0.0287) | Acc: (99.10%) (31838/32128)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 260 |  Loss: (0.0286) | Acc: (99.11%) (33110/33408)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 270 |  Loss: (0.0288) | Acc: (99.10%) (34377/34688)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 280 |  Loss: (0.0285) | Acc: (99.12%) (35650/35968)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 290 |  Loss: (0.0285) | Acc: (99.11%) (36918/37248)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 300 |  Loss: (0.0283) | Acc: (99.13%) (38191/38528)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 310 |  Loss: (0.0283) | Acc: (99.12%) (39458/39808)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 320 |  Loss: (0.0280) | Acc: (99.13%) (40730/41088)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 330 |  Loss: (0.0282) | Acc: (99.12%) (41995/42368)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 340 |  Loss: (0.0281) | Acc: (99.13%) (43269/43648)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 350 |  Loss: (0.0285) | Acc: (99.11%) (44530/44928)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 360 |  Loss: (0.0285) | Acc: (99.11%) (45795/46208)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 370 |  Loss: (0.0283) | Acc: (99.12%) (47068/47488)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 380 |  Loss: (0.0281) | Acc: (99.11%) (48335/48768)\n",
      "#TRAIN: Epoch: 95 | Batch_idx: 390 |  Loss: (0.0284) | Acc: (99.11%) (49554/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5208) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 0 |  Loss: (0.0079) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 10 |  Loss: (0.0288) | Acc: (99.22%) (1397/1408)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 20 |  Loss: (0.0257) | Acc: (99.22%) (2667/2688)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 30 |  Loss: (0.0269) | Acc: (99.19%) (3936/3968)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 40 |  Loss: (0.0264) | Acc: (99.24%) (5208/5248)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 50 |  Loss: (0.0255) | Acc: (99.26%) (6480/6528)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 60 |  Loss: (0.0258) | Acc: (99.22%) (7747/7808)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 70 |  Loss: (0.0254) | Acc: (99.19%) (9014/9088)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 80 |  Loss: (0.0265) | Acc: (99.20%) (10285/10368)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 90 |  Loss: (0.0255) | Acc: (99.24%) (11560/11648)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 100 |  Loss: (0.0250) | Acc: (99.28%) (12835/12928)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 110 |  Loss: (0.0250) | Acc: (99.28%) (14105/14208)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 120 |  Loss: (0.0252) | Acc: (99.27%) (15375/15488)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 130 |  Loss: (0.0252) | Acc: (99.27%) (16646/16768)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 140 |  Loss: (0.0250) | Acc: (99.28%) (17918/18048)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 150 |  Loss: (0.0259) | Acc: (99.24%) (19181/19328)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 160 |  Loss: (0.0260) | Acc: (99.22%) (20448/20608)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 170 |  Loss: (0.0259) | Acc: (99.24%) (21722/21888)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 180 |  Loss: (0.0257) | Acc: (99.24%) (22993/23168)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 190 |  Loss: (0.0258) | Acc: (99.24%) (24261/24448)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 200 |  Loss: (0.0255) | Acc: (99.25%) (25534/25728)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 210 |  Loss: (0.0255) | Acc: (99.24%) (26802/27008)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 220 |  Loss: (0.0256) | Acc: (99.24%) (28072/28288)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 230 |  Loss: (0.0257) | Acc: (99.22%) (29337/29568)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 240 |  Loss: (0.0255) | Acc: (99.23%) (30611/30848)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 250 |  Loss: (0.0256) | Acc: (99.22%) (31877/32128)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 260 |  Loss: (0.0257) | Acc: (99.22%) (33149/33408)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 270 |  Loss: (0.0261) | Acc: (99.20%) (34412/34688)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 280 |  Loss: (0.0260) | Acc: (99.20%) (35682/35968)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 290 |  Loss: (0.0259) | Acc: (99.22%) (36956/37248)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 300 |  Loss: (0.0259) | Acc: (99.21%) (38224/38528)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 310 |  Loss: (0.0261) | Acc: (99.21%) (39492/39808)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 320 |  Loss: (0.0263) | Acc: (99.19%) (40755/41088)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 330 |  Loss: (0.0262) | Acc: (99.19%) (42026/42368)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 340 |  Loss: (0.0264) | Acc: (99.19%) (43295/43648)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 350 |  Loss: (0.0264) | Acc: (99.19%) (44562/44928)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 360 |  Loss: (0.0263) | Acc: (99.18%) (45829/46208)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 370 |  Loss: (0.0262) | Acc: (99.19%) (47101/47488)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 380 |  Loss: (0.0262) | Acc: (99.19%) (48372/48768)\n",
      "#TRAIN: Epoch: 96 | Batch_idx: 390 |  Loss: (0.0262) | Acc: (99.19%) (49593/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5211) | Acc: (89.03%) (8903/10000)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 0 |  Loss: (0.0157) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 10 |  Loss: (0.0267) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 20 |  Loss: (0.0227) | Acc: (99.26%) (2668/2688)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 30 |  Loss: (0.0230) | Acc: (99.22%) (3937/3968)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 40 |  Loss: (0.0225) | Acc: (99.28%) (5210/5248)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 50 |  Loss: (0.0241) | Acc: (99.23%) (6478/6528)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 60 |  Loss: (0.0245) | Acc: (99.21%) (7746/7808)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 70 |  Loss: (0.0261) | Acc: (99.20%) (9015/9088)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 80 |  Loss: (0.0262) | Acc: (99.19%) (10284/10368)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 90 |  Loss: (0.0265) | Acc: (99.18%) (11553/11648)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 100 |  Loss: (0.0260) | Acc: (99.18%) (12822/12928)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 110 |  Loss: (0.0262) | Acc: (99.17%) (14090/14208)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 120 |  Loss: (0.0253) | Acc: (99.21%) (15366/15488)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 130 |  Loss: (0.0257) | Acc: (99.19%) (16633/16768)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 140 |  Loss: (0.0255) | Acc: (99.20%) (17904/18048)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 150 |  Loss: (0.0255) | Acc: (99.20%) (19173/19328)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 160 |  Loss: (0.0260) | Acc: (99.18%) (20440/20608)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 170 |  Loss: (0.0259) | Acc: (99.18%) (21708/21888)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 180 |  Loss: (0.0258) | Acc: (99.18%) (22978/23168)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 190 |  Loss: (0.0263) | Acc: (99.17%) (24246/24448)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 200 |  Loss: (0.0260) | Acc: (99.19%) (25520/25728)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 210 |  Loss: (0.0262) | Acc: (99.18%) (26786/27008)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 220 |  Loss: (0.0262) | Acc: (99.18%) (28056/28288)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 230 |  Loss: (0.0264) | Acc: (99.17%) (29323/29568)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 240 |  Loss: (0.0264) | Acc: (99.18%) (30594/30848)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 250 |  Loss: (0.0260) | Acc: (99.19%) (31868/32128)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 260 |  Loss: (0.0260) | Acc: (99.19%) (33136/33408)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 270 |  Loss: (0.0257) | Acc: (99.19%) (34408/34688)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 280 |  Loss: (0.0258) | Acc: (99.20%) (35680/35968)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 290 |  Loss: (0.0256) | Acc: (99.20%) (36951/37248)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 300 |  Loss: (0.0259) | Acc: (99.18%) (38212/38528)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 310 |  Loss: (0.0259) | Acc: (99.18%) (39482/39808)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 320 |  Loss: (0.0259) | Acc: (99.18%) (40753/41088)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 330 |  Loss: (0.0261) | Acc: (99.17%) (42016/42368)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 340 |  Loss: (0.0261) | Acc: (99.17%) (43284/43648)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 350 |  Loss: (0.0261) | Acc: (99.16%) (44552/44928)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 360 |  Loss: (0.0265) | Acc: (99.16%) (45818/46208)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 370 |  Loss: (0.0268) | Acc: (99.15%) (47082/47488)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 380 |  Loss: (0.0267) | Acc: (99.15%) (48352/48768)\n",
      "#TRAIN: Epoch: 97 | Batch_idx: 390 |  Loss: (0.0269) | Acc: (99.13%) (49567/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5220) | Acc: (89.25%) (8925/10000)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 0 |  Loss: (0.0241) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 10 |  Loss: (0.0200) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 20 |  Loss: (0.0215) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 30 |  Loss: (0.0213) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 40 |  Loss: (0.0236) | Acc: (99.26%) (5209/5248)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 50 |  Loss: (0.0242) | Acc: (99.22%) (6477/6528)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 60 |  Loss: (0.0247) | Acc: (99.21%) (7746/7808)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 70 |  Loss: (0.0241) | Acc: (99.26%) (9021/9088)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 80 |  Loss: (0.0242) | Acc: (99.23%) (10288/10368)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 90 |  Loss: (0.0239) | Acc: (99.24%) (11559/11648)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 100 |  Loss: (0.0229) | Acc: (99.28%) (12835/12928)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 110 |  Loss: (0.0227) | Acc: (99.30%) (14109/14208)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 120 |  Loss: (0.0226) | Acc: (99.30%) (15379/15488)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 130 |  Loss: (0.0227) | Acc: (99.30%) (16650/16768)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 140 |  Loss: (0.0224) | Acc: (99.32%) (17925/18048)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 150 |  Loss: (0.0227) | Acc: (99.30%) (19192/19328)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 160 |  Loss: (0.0232) | Acc: (99.29%) (20461/20608)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 170 |  Loss: (0.0231) | Acc: (99.29%) (21732/21888)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 180 |  Loss: (0.0232) | Acc: (99.27%) (23000/23168)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 190 |  Loss: (0.0234) | Acc: (99.28%) (24272/24448)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 200 |  Loss: (0.0236) | Acc: (99.28%) (25542/25728)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 210 |  Loss: (0.0236) | Acc: (99.29%) (26815/27008)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 220 |  Loss: (0.0237) | Acc: (99.29%) (28088/28288)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 230 |  Loss: (0.0240) | Acc: (99.28%) (29355/29568)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 240 |  Loss: (0.0242) | Acc: (99.27%) (30623/30848)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 250 |  Loss: (0.0246) | Acc: (99.27%) (31893/32128)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 260 |  Loss: (0.0246) | Acc: (99.26%) (33162/33408)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 270 |  Loss: (0.0250) | Acc: (99.24%) (34426/34688)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 280 |  Loss: (0.0256) | Acc: (99.23%) (35690/35968)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 290 |  Loss: (0.0255) | Acc: (99.23%) (36962/37248)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 300 |  Loss: (0.0254) | Acc: (99.23%) (38233/38528)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 310 |  Loss: (0.0255) | Acc: (99.24%) (39504/39808)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 320 |  Loss: (0.0255) | Acc: (99.23%) (40772/41088)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 330 |  Loss: (0.0255) | Acc: (99.23%) (42042/42368)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 340 |  Loss: (0.0257) | Acc: (99.22%) (43308/43648)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 350 |  Loss: (0.0257) | Acc: (99.22%) (44576/44928)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 360 |  Loss: (0.0255) | Acc: (99.22%) (45847/46208)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 370 |  Loss: (0.0256) | Acc: (99.21%) (47115/47488)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 380 |  Loss: (0.0256) | Acc: (99.22%) (48387/48768)\n",
      "#TRAIN: Epoch: 98 | Batch_idx: 390 |  Loss: (0.0255) | Acc: (99.22%) (49611/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5265) | Acc: (89.30%) (8930/10000)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 0 |  Loss: (0.0083) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 10 |  Loss: (0.0255) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 20 |  Loss: (0.0268) | Acc: (99.07%) (2663/2688)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 30 |  Loss: (0.0276) | Acc: (99.07%) (3931/3968)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 40 |  Loss: (0.0267) | Acc: (99.09%) (5200/5248)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 50 |  Loss: (0.0262) | Acc: (99.10%) (6469/6528)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 60 |  Loss: (0.0254) | Acc: (99.13%) (7740/7808)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 70 |  Loss: (0.0252) | Acc: (99.12%) (9008/9088)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 80 |  Loss: (0.0253) | Acc: (99.15%) (10280/10368)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 90 |  Loss: (0.0257) | Acc: (99.12%) (11545/11648)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 100 |  Loss: (0.0258) | Acc: (99.11%) (12813/12928)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 110 |  Loss: (0.0262) | Acc: (99.09%) (14079/14208)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 120 |  Loss: (0.0255) | Acc: (99.12%) (15352/15488)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 130 |  Loss: (0.0250) | Acc: (99.15%) (16626/16768)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 140 |  Loss: (0.0247) | Acc: (99.16%) (17896/18048)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 150 |  Loss: (0.0249) | Acc: (99.15%) (19164/19328)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 160 |  Loss: (0.0251) | Acc: (99.15%) (20433/20608)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 170 |  Loss: (0.0258) | Acc: (99.12%) (21695/21888)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 180 |  Loss: (0.0262) | Acc: (99.11%) (22961/23168)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 190 |  Loss: (0.0262) | Acc: (99.10%) (24229/24448)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 200 |  Loss: (0.0257) | Acc: (99.13%) (25503/25728)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 210 |  Loss: (0.0260) | Acc: (99.13%) (26772/27008)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 220 |  Loss: (0.0260) | Acc: (99.12%) (28040/28288)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 230 |  Loss: (0.0263) | Acc: (99.11%) (29306/29568)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 240 |  Loss: (0.0261) | Acc: (99.13%) (30579/30848)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 250 |  Loss: (0.0261) | Acc: (99.13%) (31848/32128)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 260 |  Loss: (0.0263) | Acc: (99.11%) (33112/33408)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 270 |  Loss: (0.0264) | Acc: (99.11%) (34380/34688)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 280 |  Loss: (0.0264) | Acc: (99.10%) (35646/35968)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 290 |  Loss: (0.0266) | Acc: (99.11%) (36917/37248)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 300 |  Loss: (0.0264) | Acc: (99.12%) (38189/38528)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 310 |  Loss: (0.0265) | Acc: (99.12%) (39458/39808)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 320 |  Loss: (0.0266) | Acc: (99.11%) (40723/41088)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 330 |  Loss: (0.0266) | Acc: (99.12%) (41994/42368)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 340 |  Loss: (0.0265) | Acc: (99.12%) (43264/43648)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 350 |  Loss: (0.0268) | Acc: (99.11%) (44530/44928)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 360 |  Loss: (0.0268) | Acc: (99.11%) (45799/46208)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 370 |  Loss: (0.0268) | Acc: (99.12%) (47071/47488)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 380 |  Loss: (0.0269) | Acc: (99.12%) (48339/48768)\n",
      "#TRAIN: Epoch: 99 | Batch_idx: 390 |  Loss: (0.0268) | Acc: (99.13%) (49564/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5282) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 0 |  Loss: (0.0106) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 10 |  Loss: (0.0224) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 20 |  Loss: (0.0240) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 30 |  Loss: (0.0259) | Acc: (99.29%) (3940/3968)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 40 |  Loss: (0.0263) | Acc: (99.22%) (5207/5248)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 50 |  Loss: (0.0255) | Acc: (99.25%) (6479/6528)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 60 |  Loss: (0.0248) | Acc: (99.26%) (7750/7808)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 70 |  Loss: (0.0245) | Acc: (99.28%) (9023/9088)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 80 |  Loss: (0.0252) | Acc: (99.26%) (10291/10368)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 90 |  Loss: (0.0246) | Acc: (99.28%) (11564/11648)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 100 |  Loss: (0.0243) | Acc: (99.29%) (12836/12928)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 110 |  Loss: (0.0246) | Acc: (99.27%) (14104/14208)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 120 |  Loss: (0.0242) | Acc: (99.28%) (15376/15488)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 130 |  Loss: (0.0238) | Acc: (99.30%) (16650/16768)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 140 |  Loss: (0.0238) | Acc: (99.27%) (17917/18048)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 150 |  Loss: (0.0234) | Acc: (99.30%) (19192/19328)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 160 |  Loss: (0.0233) | Acc: (99.30%) (20463/20608)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 170 |  Loss: (0.0233) | Acc: (99.30%) (21734/21888)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 180 |  Loss: (0.0231) | Acc: (99.29%) (23003/23168)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 190 |  Loss: (0.0233) | Acc: (99.29%) (24274/24448)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 200 |  Loss: (0.0232) | Acc: (99.27%) (25540/25728)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 210 |  Loss: (0.0239) | Acc: (99.24%) (26803/27008)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 220 |  Loss: (0.0238) | Acc: (99.24%) (28073/28288)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 230 |  Loss: (0.0236) | Acc: (99.25%) (29345/29568)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 240 |  Loss: (0.0236) | Acc: (99.25%) (30616/30848)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 250 |  Loss: (0.0236) | Acc: (99.25%) (31886/32128)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 260 |  Loss: (0.0238) | Acc: (99.25%) (33157/33408)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 270 |  Loss: (0.0238) | Acc: (99.24%) (34424/34688)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 280 |  Loss: (0.0241) | Acc: (99.23%) (35692/35968)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 290 |  Loss: (0.0240) | Acc: (99.23%) (36963/37248)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 300 |  Loss: (0.0239) | Acc: (99.24%) (38235/38528)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 310 |  Loss: (0.0237) | Acc: (99.25%) (39509/39808)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 320 |  Loss: (0.0237) | Acc: (99.25%) (40778/41088)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 330 |  Loss: (0.0237) | Acc: (99.24%) (42047/42368)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 340 |  Loss: (0.0237) | Acc: (99.23%) (43314/43648)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 350 |  Loss: (0.0236) | Acc: (99.24%) (44586/44928)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 360 |  Loss: (0.0237) | Acc: (99.24%) (45857/46208)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 370 |  Loss: (0.0239) | Acc: (99.23%) (47124/47488)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 380 |  Loss: (0.0238) | Acc: (99.24%) (48396/48768)\n",
      "#TRAIN: Epoch: 100 | Batch_idx: 390 |  Loss: (0.0238) | Acc: (99.23%) (49616/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5356) | Acc: (89.07%) (8907/10000)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 0 |  Loss: (0.0287) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 10 |  Loss: (0.0231) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 20 |  Loss: (0.0274) | Acc: (99.03%) (2662/2688)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 30 |  Loss: (0.0278) | Acc: (98.97%) (3927/3968)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 40 |  Loss: (0.0293) | Acc: (98.97%) (5194/5248)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 50 |  Loss: (0.0278) | Acc: (99.05%) (6466/6528)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 60 |  Loss: (0.0267) | Acc: (99.04%) (7733/7808)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 70 |  Loss: (0.0256) | Acc: (99.11%) (9007/9088)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 80 |  Loss: (0.0263) | Acc: (99.10%) (10275/10368)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 90 |  Loss: (0.0253) | Acc: (99.17%) (11551/11648)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 100 |  Loss: (0.0254) | Acc: (99.13%) (12816/12928)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 110 |  Loss: (0.0247) | Acc: (99.17%) (14090/14208)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 120 |  Loss: (0.0243) | Acc: (99.20%) (15364/15488)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 130 |  Loss: (0.0242) | Acc: (99.19%) (16632/16768)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 140 |  Loss: (0.0241) | Acc: (99.21%) (17905/18048)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 150 |  Loss: (0.0239) | Acc: (99.22%) (19177/19328)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 160 |  Loss: (0.0235) | Acc: (99.23%) (20449/20608)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 170 |  Loss: (0.0234) | Acc: (99.24%) (21721/21888)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 180 |  Loss: (0.0239) | Acc: (99.24%) (22991/23168)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 190 |  Loss: (0.0241) | Acc: (99.24%) (24261/24448)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 200 |  Loss: (0.0239) | Acc: (99.24%) (25533/25728)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 210 |  Loss: (0.0239) | Acc: (99.23%) (26801/27008)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 220 |  Loss: (0.0236) | Acc: (99.24%) (28073/28288)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 230 |  Loss: (0.0240) | Acc: (99.22%) (29338/29568)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 240 |  Loss: (0.0241) | Acc: (99.21%) (30605/30848)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 250 |  Loss: (0.0240) | Acc: (99.22%) (31879/32128)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 260 |  Loss: (0.0244) | Acc: (99.22%) (33146/33408)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 270 |  Loss: (0.0247) | Acc: (99.20%) (34410/34688)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 280 |  Loss: (0.0249) | Acc: (99.19%) (35675/35968)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 290 |  Loss: (0.0249) | Acc: (99.19%) (36946/37248)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 300 |  Loss: (0.0249) | Acc: (99.20%) (38218/38528)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 310 |  Loss: (0.0248) | Acc: (99.20%) (39488/39808)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 320 |  Loss: (0.0247) | Acc: (99.20%) (40761/41088)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 330 |  Loss: (0.0249) | Acc: (99.20%) (42031/42368)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 340 |  Loss: (0.0245) | Acc: (99.22%) (43307/43648)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 350 |  Loss: (0.0246) | Acc: (99.22%) (44577/44928)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 360 |  Loss: (0.0248) | Acc: (99.22%) (45846/46208)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 370 |  Loss: (0.0247) | Acc: (99.22%) (47116/47488)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 380 |  Loss: (0.0248) | Acc: (99.22%) (48386/48768)\n",
      "#TRAIN: Epoch: 101 | Batch_idx: 390 |  Loss: (0.0249) | Acc: (99.21%) (49606/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5391) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 0 |  Loss: (0.0172) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 10 |  Loss: (0.0323) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 20 |  Loss: (0.0256) | Acc: (99.33%) (2670/2688)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 30 |  Loss: (0.0261) | Acc: (99.27%) (3939/3968)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 40 |  Loss: (0.0250) | Acc: (99.33%) (5213/5248)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 50 |  Loss: (0.0249) | Acc: (99.30%) (6482/6528)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 60 |  Loss: (0.0243) | Acc: (99.30%) (7753/7808)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 70 |  Loss: (0.0233) | Acc: (99.32%) (9026/9088)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 80 |  Loss: (0.0227) | Acc: (99.33%) (10299/10368)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 90 |  Loss: (0.0226) | Acc: (99.36%) (11573/11648)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 100 |  Loss: (0.0226) | Acc: (99.36%) (12845/12928)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 110 |  Loss: (0.0228) | Acc: (99.32%) (14112/14208)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 120 |  Loss: (0.0227) | Acc: (99.33%) (15384/15488)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 130 |  Loss: (0.0228) | Acc: (99.32%) (16654/16768)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 140 |  Loss: (0.0233) | Acc: (99.30%) (17922/18048)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 150 |  Loss: (0.0239) | Acc: (99.27%) (19187/19328)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 160 |  Loss: (0.0238) | Acc: (99.27%) (20458/20608)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 170 |  Loss: (0.0239) | Acc: (99.27%) (21728/21888)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 180 |  Loss: (0.0246) | Acc: (99.25%) (22995/23168)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 190 |  Loss: (0.0251) | Acc: (99.23%) (24259/24448)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 200 |  Loss: (0.0247) | Acc: (99.25%) (25534/25728)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 210 |  Loss: (0.0247) | Acc: (99.24%) (26802/27008)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 220 |  Loss: (0.0246) | Acc: (99.24%) (28072/28288)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 230 |  Loss: (0.0248) | Acc: (99.23%) (29340/29568)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 240 |  Loss: (0.0247) | Acc: (99.23%) (30609/30848)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 250 |  Loss: (0.0245) | Acc: (99.23%) (31881/32128)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 260 |  Loss: (0.0243) | Acc: (99.24%) (33154/33408)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 270 |  Loss: (0.0243) | Acc: (99.23%) (34422/34688)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 280 |  Loss: (0.0241) | Acc: (99.24%) (35694/35968)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 290 |  Loss: (0.0245) | Acc: (99.22%) (36956/37248)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 300 |  Loss: (0.0246) | Acc: (99.22%) (38228/38528)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 310 |  Loss: (0.0251) | Acc: (99.20%) (39491/39808)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 320 |  Loss: (0.0249) | Acc: (99.21%) (40764/41088)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 330 |  Loss: (0.0248) | Acc: (99.22%) (42036/42368)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 340 |  Loss: (0.0249) | Acc: (99.21%) (43303/43648)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 350 |  Loss: (0.0249) | Acc: (99.21%) (44575/44928)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 360 |  Loss: (0.0249) | Acc: (99.22%) (45848/46208)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 370 |  Loss: (0.0252) | Acc: (99.22%) (47117/47488)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 380 |  Loss: (0.0250) | Acc: (99.22%) (48386/48768)\n",
      "#TRAIN: Epoch: 102 | Batch_idx: 390 |  Loss: (0.0250) | Acc: (99.21%) (49606/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5375) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 0 |  Loss: (0.0391) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 10 |  Loss: (0.0269) | Acc: (99.01%) (1394/1408)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 20 |  Loss: (0.0212) | Acc: (99.29%) (2669/2688)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 30 |  Loss: (0.0211) | Acc: (99.27%) (3939/3968)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 40 |  Loss: (0.0190) | Acc: (99.35%) (5214/5248)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 50 |  Loss: (0.0199) | Acc: (99.37%) (6487/6528)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 60 |  Loss: (0.0218) | Acc: (99.32%) (7755/7808)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 70 |  Loss: (0.0223) | Acc: (99.28%) (9023/9088)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 80 |  Loss: (0.0222) | Acc: (99.31%) (10296/10368)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 90 |  Loss: (0.0225) | Acc: (99.28%) (11564/11648)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 100 |  Loss: (0.0229) | Acc: (99.28%) (12835/12928)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 110 |  Loss: (0.0231) | Acc: (99.28%) (14106/14208)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 120 |  Loss: (0.0225) | Acc: (99.30%) (15379/15488)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 130 |  Loss: (0.0226) | Acc: (99.29%) (16649/16768)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 140 |  Loss: (0.0224) | Acc: (99.31%) (17923/18048)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 150 |  Loss: (0.0231) | Acc: (99.30%) (19192/19328)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 160 |  Loss: (0.0224) | Acc: (99.31%) (20466/20608)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 170 |  Loss: (0.0225) | Acc: (99.30%) (21735/21888)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 180 |  Loss: (0.0226) | Acc: (99.29%) (23003/23168)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 190 |  Loss: (0.0229) | Acc: (99.28%) (24272/24448)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 200 |  Loss: (0.0230) | Acc: (99.28%) (25543/25728)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 210 |  Loss: (0.0234) | Acc: (99.26%) (26809/27008)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 220 |  Loss: (0.0234) | Acc: (99.26%) (28079/28288)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 230 |  Loss: (0.0234) | Acc: (99.26%) (29350/29568)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 240 |  Loss: (0.0235) | Acc: (99.25%) (30618/30848)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 250 |  Loss: (0.0234) | Acc: (99.27%) (31892/32128)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 260 |  Loss: (0.0235) | Acc: (99.26%) (33162/33408)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 270 |  Loss: (0.0235) | Acc: (99.26%) (34432/34688)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 280 |  Loss: (0.0234) | Acc: (99.26%) (35701/35968)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 290 |  Loss: (0.0236) | Acc: (99.25%) (36968/37248)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 300 |  Loss: (0.0235) | Acc: (99.25%) (38239/38528)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 310 |  Loss: (0.0236) | Acc: (99.24%) (39504/39808)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 320 |  Loss: (0.0236) | Acc: (99.24%) (40774/41088)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 330 |  Loss: (0.0234) | Acc: (99.24%) (42044/42368)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 340 |  Loss: (0.0233) | Acc: (99.24%) (43315/43648)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 350 |  Loss: (0.0233) | Acc: (99.24%) (44587/44928)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 360 |  Loss: (0.0235) | Acc: (99.24%) (45856/46208)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 370 |  Loss: (0.0233) | Acc: (99.25%) (47133/47488)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 380 |  Loss: (0.0232) | Acc: (99.26%) (48408/48768)\n",
      "#TRAIN: Epoch: 103 | Batch_idx: 390 |  Loss: (0.0231) | Acc: (99.26%) (49630/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5312) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 0 |  Loss: (0.0209) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 10 |  Loss: (0.0282) | Acc: (99.01%) (1394/1408)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 20 |  Loss: (0.0290) | Acc: (99.03%) (2662/2688)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 30 |  Loss: (0.0277) | Acc: (99.07%) (3931/3968)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 40 |  Loss: (0.0248) | Acc: (99.18%) (5205/5248)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 50 |  Loss: (0.0241) | Acc: (99.20%) (6476/6528)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 60 |  Loss: (0.0244) | Acc: (99.18%) (7744/7808)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 70 |  Loss: (0.0247) | Acc: (99.16%) (9012/9088)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 80 |  Loss: (0.0250) | Acc: (99.15%) (10280/10368)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 90 |  Loss: (0.0252) | Acc: (99.16%) (11550/11648)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 100 |  Loss: (0.0244) | Acc: (99.18%) (12822/12928)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 110 |  Loss: (0.0240) | Acc: (99.16%) (14089/14208)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 120 |  Loss: (0.0234) | Acc: (99.19%) (15363/15488)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 130 |  Loss: (0.0236) | Acc: (99.21%) (16635/16768)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 140 |  Loss: (0.0241) | Acc: (99.20%) (17904/18048)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 150 |  Loss: (0.0241) | Acc: (99.21%) (19176/19328)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 160 |  Loss: (0.0239) | Acc: (99.24%) (20451/20608)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 170 |  Loss: (0.0240) | Acc: (99.25%) (21724/21888)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 180 |  Loss: (0.0241) | Acc: (99.24%) (22993/23168)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 190 |  Loss: (0.0240) | Acc: (99.24%) (24263/24448)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 200 |  Loss: (0.0238) | Acc: (99.24%) (25533/25728)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 210 |  Loss: (0.0237) | Acc: (99.24%) (26802/27008)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 220 |  Loss: (0.0244) | Acc: (99.20%) (28061/28288)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 230 |  Loss: (0.0243) | Acc: (99.20%) (29332/29568)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 240 |  Loss: (0.0246) | Acc: (99.19%) (30598/30848)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 250 |  Loss: (0.0247) | Acc: (99.20%) (31871/32128)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 260 |  Loss: (0.0244) | Acc: (99.21%) (33145/33408)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 270 |  Loss: (0.0244) | Acc: (99.21%) (34413/34688)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 280 |  Loss: (0.0243) | Acc: (99.21%) (35683/35968)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 290 |  Loss: (0.0244) | Acc: (99.20%) (36951/37248)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 300 |  Loss: (0.0246) | Acc: (99.20%) (38220/38528)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 310 |  Loss: (0.0246) | Acc: (99.20%) (39491/39808)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 320 |  Loss: (0.0247) | Acc: (99.21%) (40762/41088)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 330 |  Loss: (0.0249) | Acc: (99.20%) (42027/42368)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 340 |  Loss: (0.0250) | Acc: (99.19%) (43293/43648)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 350 |  Loss: (0.0248) | Acc: (99.20%) (44567/44928)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 360 |  Loss: (0.0249) | Acc: (99.19%) (45833/46208)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 370 |  Loss: (0.0251) | Acc: (99.19%) (47101/47488)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 380 |  Loss: (0.0252) | Acc: (99.18%) (48369/48768)\n",
      "#TRAIN: Epoch: 104 | Batch_idx: 390 |  Loss: (0.0253) | Acc: (99.18%) (49588/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5327) | Acc: (89.12%) (8912/10000)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 0 |  Loss: (0.0320) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 10 |  Loss: (0.0239) | Acc: (99.22%) (1397/1408)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 20 |  Loss: (0.0229) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 30 |  Loss: (0.0242) | Acc: (99.24%) (3938/3968)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 40 |  Loss: (0.0252) | Acc: (99.26%) (5209/5248)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 50 |  Loss: (0.0240) | Acc: (99.31%) (6483/6528)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 60 |  Loss: (0.0253) | Acc: (99.26%) (7750/7808)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 70 |  Loss: (0.0238) | Acc: (99.31%) (9025/9088)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 80 |  Loss: (0.0236) | Acc: (99.32%) (10297/10368)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 90 |  Loss: (0.0231) | Acc: (99.30%) (11567/11648)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 100 |  Loss: (0.0231) | Acc: (99.29%) (12836/12928)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 110 |  Loss: (0.0227) | Acc: (99.29%) (14107/14208)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 120 |  Loss: (0.0230) | Acc: (99.28%) (15376/15488)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 130 |  Loss: (0.0235) | Acc: (99.24%) (16641/16768)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 140 |  Loss: (0.0234) | Acc: (99.24%) (17911/18048)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 150 |  Loss: (0.0240) | Acc: (99.22%) (19177/19328)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 160 |  Loss: (0.0238) | Acc: (99.22%) (20447/20608)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 170 |  Loss: (0.0239) | Acc: (99.21%) (21716/21888)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 180 |  Loss: (0.0236) | Acc: (99.21%) (22986/23168)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 190 |  Loss: (0.0233) | Acc: (99.23%) (24260/24448)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 200 |  Loss: (0.0233) | Acc: (99.24%) (25532/25728)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 210 |  Loss: (0.0233) | Acc: (99.24%) (26802/27008)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 220 |  Loss: (0.0234) | Acc: (99.24%) (28074/28288)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 230 |  Loss: (0.0233) | Acc: (99.24%) (29344/29568)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 240 |  Loss: (0.0233) | Acc: (99.23%) (30612/30848)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 250 |  Loss: (0.0234) | Acc: (99.23%) (31881/32128)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 260 |  Loss: (0.0234) | Acc: (99.24%) (33153/33408)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 270 |  Loss: (0.0235) | Acc: (99.23%) (34422/34688)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 280 |  Loss: (0.0238) | Acc: (99.22%) (35688/35968)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 290 |  Loss: (0.0239) | Acc: (99.22%) (36956/37248)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 300 |  Loss: (0.0241) | Acc: (99.21%) (38225/38528)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 310 |  Loss: (0.0242) | Acc: (99.22%) (39496/39808)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 320 |  Loss: (0.0241) | Acc: (99.22%) (40768/41088)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 330 |  Loss: (0.0241) | Acc: (99.22%) (42038/42368)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 340 |  Loss: (0.0243) | Acc: (99.21%) (43301/43648)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 350 |  Loss: (0.0242) | Acc: (99.21%) (44571/44928)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 360 |  Loss: (0.0241) | Acc: (99.21%) (45842/46208)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 370 |  Loss: (0.0242) | Acc: (99.20%) (47109/47488)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 380 |  Loss: (0.0243) | Acc: (99.20%) (48377/48768)\n",
      "#TRAIN: Epoch: 105 | Batch_idx: 390 |  Loss: (0.0248) | Acc: (99.18%) (49592/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5370) | Acc: (89.20%) (8920/10000)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 0 |  Loss: (0.0186) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 10 |  Loss: (0.0279) | Acc: (98.93%) (1393/1408)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 20 |  Loss: (0.0255) | Acc: (99.11%) (2664/2688)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 30 |  Loss: (0.0256) | Acc: (99.07%) (3931/3968)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 40 |  Loss: (0.0240) | Acc: (99.12%) (5202/5248)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 50 |  Loss: (0.0251) | Acc: (99.13%) (6471/6528)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 60 |  Loss: (0.0244) | Acc: (99.15%) (7742/7808)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 70 |  Loss: (0.0242) | Acc: (99.16%) (9012/9088)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 80 |  Loss: (0.0244) | Acc: (99.18%) (10283/10368)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 90 |  Loss: (0.0241) | Acc: (99.19%) (11554/11648)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 100 |  Loss: (0.0236) | Acc: (99.21%) (12826/12928)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 110 |  Loss: (0.0233) | Acc: (99.24%) (14100/14208)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 120 |  Loss: (0.0232) | Acc: (99.25%) (15372/15488)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 130 |  Loss: (0.0226) | Acc: (99.27%) (16646/16768)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 140 |  Loss: (0.0224) | Acc: (99.29%) (17920/18048)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 150 |  Loss: (0.0225) | Acc: (99.29%) (19190/19328)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 160 |  Loss: (0.0226) | Acc: (99.30%) (20464/20608)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 170 |  Loss: (0.0230) | Acc: (99.28%) (21731/21888)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 180 |  Loss: (0.0234) | Acc: (99.26%) (22997/23168)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 190 |  Loss: (0.0229) | Acc: (99.27%) (24270/24448)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 200 |  Loss: (0.0231) | Acc: (99.27%) (25539/25728)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 210 |  Loss: (0.0230) | Acc: (99.26%) (26808/27008)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 220 |  Loss: (0.0231) | Acc: (99.25%) (28076/28288)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 230 |  Loss: (0.0232) | Acc: (99.26%) (29348/29568)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 240 |  Loss: (0.0233) | Acc: (99.24%) (30615/30848)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 250 |  Loss: (0.0238) | Acc: (99.23%) (31882/32128)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 260 |  Loss: (0.0237) | Acc: (99.23%) (33152/33408)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 270 |  Loss: (0.0239) | Acc: (99.22%) (34419/34688)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 280 |  Loss: (0.0238) | Acc: (99.23%) (35691/35968)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 290 |  Loss: (0.0237) | Acc: (99.23%) (36963/37248)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 300 |  Loss: (0.0234) | Acc: (99.25%) (38238/38528)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 310 |  Loss: (0.0234) | Acc: (99.24%) (39505/39808)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 320 |  Loss: (0.0233) | Acc: (99.24%) (40776/41088)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 330 |  Loss: (0.0231) | Acc: (99.25%) (42051/42368)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 340 |  Loss: (0.0230) | Acc: (99.25%) (43321/43648)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 350 |  Loss: (0.0230) | Acc: (99.25%) (44593/44928)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 360 |  Loss: (0.0229) | Acc: (99.25%) (45862/46208)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 370 |  Loss: (0.0230) | Acc: (99.25%) (47134/47488)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 380 |  Loss: (0.0230) | Acc: (99.25%) (48404/48768)\n",
      "#TRAIN: Epoch: 106 | Batch_idx: 390 |  Loss: (0.0229) | Acc: (99.25%) (49627/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5398) | Acc: (89.29%) (8929/10000)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 0 |  Loss: (0.0029) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 10 |  Loss: (0.0133) | Acc: (99.79%) (1405/1408)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 20 |  Loss: (0.0190) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 30 |  Loss: (0.0223) | Acc: (99.37%) (3943/3968)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 40 |  Loss: (0.0218) | Acc: (99.39%) (5216/5248)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 50 |  Loss: (0.0213) | Acc: (99.42%) (6490/6528)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 60 |  Loss: (0.0212) | Acc: (99.40%) (7761/7808)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 70 |  Loss: (0.0213) | Acc: (99.39%) (9033/9088)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 80 |  Loss: (0.0217) | Acc: (99.39%) (10305/10368)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 90 |  Loss: (0.0218) | Acc: (99.36%) (11573/11648)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 100 |  Loss: (0.0216) | Acc: (99.37%) (12846/12928)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 110 |  Loss: (0.0221) | Acc: (99.35%) (14116/14208)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 120 |  Loss: (0.0220) | Acc: (99.35%) (15387/15488)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 130 |  Loss: (0.0215) | Acc: (99.37%) (16663/16768)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 140 |  Loss: (0.0212) | Acc: (99.38%) (17937/18048)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 150 |  Loss: (0.0212) | Acc: (99.38%) (19208/19328)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 160 |  Loss: (0.0214) | Acc: (99.36%) (20477/20608)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 170 |  Loss: (0.0214) | Acc: (99.36%) (21749/21888)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 180 |  Loss: (0.0214) | Acc: (99.36%) (23020/23168)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 190 |  Loss: (0.0211) | Acc: (99.37%) (24293/24448)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 200 |  Loss: (0.0210) | Acc: (99.37%) (25565/25728)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 210 |  Loss: (0.0212) | Acc: (99.35%) (26833/27008)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 220 |  Loss: (0.0214) | Acc: (99.34%) (28102/28288)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 230 |  Loss: (0.0216) | Acc: (99.34%) (29373/29568)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 240 |  Loss: (0.0216) | Acc: (99.34%) (30643/30848)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 250 |  Loss: (0.0216) | Acc: (99.33%) (31913/32128)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 260 |  Loss: (0.0218) | Acc: (99.32%) (33181/33408)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 270 |  Loss: (0.0218) | Acc: (99.32%) (34453/34688)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 280 |  Loss: (0.0216) | Acc: (99.33%) (35728/35968)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 290 |  Loss: (0.0215) | Acc: (99.33%) (36999/37248)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 300 |  Loss: (0.0213) | Acc: (99.34%) (38274/38528)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 310 |  Loss: (0.0213) | Acc: (99.34%) (39545/39808)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 320 |  Loss: (0.0213) | Acc: (99.34%) (40817/41088)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 330 |  Loss: (0.0211) | Acc: (99.35%) (42093/42368)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 340 |  Loss: (0.0212) | Acc: (99.35%) (43366/43648)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 350 |  Loss: (0.0212) | Acc: (99.35%) (44634/44928)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 360 |  Loss: (0.0215) | Acc: (99.34%) (45904/46208)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 370 |  Loss: (0.0213) | Acc: (99.35%) (47180/47488)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 380 |  Loss: (0.0212) | Acc: (99.35%) (48453/48768)\n",
      "#TRAIN: Epoch: 107 | Batch_idx: 390 |  Loss: (0.0216) | Acc: (99.34%) (49672/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5471) | Acc: (88.97%) (8897/10000)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 0 |  Loss: (0.0114) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 10 |  Loss: (0.0187) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 20 |  Loss: (0.0197) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 30 |  Loss: (0.0195) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 40 |  Loss: (0.0192) | Acc: (99.37%) (5215/5248)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 50 |  Loss: (0.0201) | Acc: (99.31%) (6483/6528)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 60 |  Loss: (0.0200) | Acc: (99.35%) (7757/7808)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 70 |  Loss: (0.0205) | Acc: (99.32%) (9026/9088)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 80 |  Loss: (0.0198) | Acc: (99.36%) (10302/10368)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 90 |  Loss: (0.0200) | Acc: (99.36%) (11574/11648)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 100 |  Loss: (0.0208) | Acc: (99.33%) (12842/12928)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 110 |  Loss: (0.0205) | Acc: (99.34%) (14114/14208)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 120 |  Loss: (0.0206) | Acc: (99.34%) (15386/15488)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 130 |  Loss: (0.0205) | Acc: (99.34%) (16658/16768)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 140 |  Loss: (0.0205) | Acc: (99.35%) (17930/18048)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 150 |  Loss: (0.0204) | Acc: (99.34%) (19200/19328)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 160 |  Loss: (0.0204) | Acc: (99.34%) (20471/20608)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 170 |  Loss: (0.0203) | Acc: (99.35%) (21745/21888)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 180 |  Loss: (0.0204) | Acc: (99.34%) (23014/23168)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 190 |  Loss: (0.0202) | Acc: (99.34%) (24287/24448)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 200 |  Loss: (0.0204) | Acc: (99.34%) (25559/25728)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 210 |  Loss: (0.0205) | Acc: (99.35%) (26832/27008)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 220 |  Loss: (0.0212) | Acc: (99.32%) (28096/28288)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 230 |  Loss: (0.0214) | Acc: (99.33%) (29369/29568)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 240 |  Loss: (0.0216) | Acc: (99.31%) (30634/30848)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 250 |  Loss: (0.0219) | Acc: (99.29%) (31899/32128)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 260 |  Loss: (0.0218) | Acc: (99.29%) (33170/33408)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 270 |  Loss: (0.0218) | Acc: (99.30%) (34445/34688)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 280 |  Loss: (0.0217) | Acc: (99.31%) (35721/35968)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 290 |  Loss: (0.0220) | Acc: (99.30%) (36988/37248)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 300 |  Loss: (0.0220) | Acc: (99.30%) (38258/38528)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 310 |  Loss: (0.0220) | Acc: (99.30%) (39529/39808)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 320 |  Loss: (0.0219) | Acc: (99.30%) (40802/41088)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 330 |  Loss: (0.0219) | Acc: (99.30%) (42072/42368)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 340 |  Loss: (0.0218) | Acc: (99.31%) (43347/43648)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 350 |  Loss: (0.0216) | Acc: (99.31%) (44620/44928)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 360 |  Loss: (0.0216) | Acc: (99.31%) (45889/46208)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 370 |  Loss: (0.0216) | Acc: (99.31%) (47161/47488)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 380 |  Loss: (0.0217) | Acc: (99.32%) (48435/48768)\n",
      "#TRAIN: Epoch: 108 | Batch_idx: 390 |  Loss: (0.0217) | Acc: (99.32%) (49659/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5412) | Acc: (88.94%) (8894/10000)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 10 |  Loss: (0.0172) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 20 |  Loss: (0.0203) | Acc: (99.29%) (2669/2688)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 30 |  Loss: (0.0233) | Acc: (99.22%) (3937/3968)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 40 |  Loss: (0.0233) | Acc: (99.29%) (5211/5248)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 50 |  Loss: (0.0229) | Acc: (99.31%) (6483/6528)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 60 |  Loss: (0.0223) | Acc: (99.31%) (7754/7808)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 70 |  Loss: (0.0230) | Acc: (99.24%) (9019/9088)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 80 |  Loss: (0.0232) | Acc: (99.19%) (10284/10368)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 90 |  Loss: (0.0231) | Acc: (99.22%) (11557/11648)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 100 |  Loss: (0.0227) | Acc: (99.20%) (12825/12928)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 110 |  Loss: (0.0224) | Acc: (99.24%) (14100/14208)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 120 |  Loss: (0.0223) | Acc: (99.26%) (15373/15488)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 130 |  Loss: (0.0223) | Acc: (99.26%) (16644/16768)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 140 |  Loss: (0.0225) | Acc: (99.26%) (17915/18048)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 150 |  Loss: (0.0230) | Acc: (99.24%) (19181/19328)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 160 |  Loss: (0.0234) | Acc: (99.22%) (20447/20608)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 170 |  Loss: (0.0232) | Acc: (99.22%) (21718/21888)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 180 |  Loss: (0.0235) | Acc: (99.21%) (22986/23168)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 190 |  Loss: (0.0232) | Acc: (99.23%) (24259/24448)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 200 |  Loss: (0.0231) | Acc: (99.23%) (25529/25728)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 210 |  Loss: (0.0231) | Acc: (99.22%) (26798/27008)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 220 |  Loss: (0.0229) | Acc: (99.23%) (28069/28288)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 230 |  Loss: (0.0230) | Acc: (99.23%) (29339/29568)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 240 |  Loss: (0.0228) | Acc: (99.23%) (30611/30848)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 250 |  Loss: (0.0234) | Acc: (99.22%) (31878/32128)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 260 |  Loss: (0.0236) | Acc: (99.22%) (33148/33408)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 270 |  Loss: (0.0236) | Acc: (99.22%) (34419/34688)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 280 |  Loss: (0.0237) | Acc: (99.22%) (35689/35968)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 290 |  Loss: (0.0238) | Acc: (99.21%) (36954/37248)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 300 |  Loss: (0.0238) | Acc: (99.22%) (38226/38528)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 310 |  Loss: (0.0234) | Acc: (99.23%) (39502/39808)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 320 |  Loss: (0.0231) | Acc: (99.24%) (40775/41088)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 330 |  Loss: (0.0231) | Acc: (99.24%) (42048/42368)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 340 |  Loss: (0.0232) | Acc: (99.25%) (43319/43648)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 350 |  Loss: (0.0231) | Acc: (99.25%) (44591/44928)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 360 |  Loss: (0.0230) | Acc: (99.26%) (45865/46208)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 370 |  Loss: (0.0231) | Acc: (99.26%) (47136/47488)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 380 |  Loss: (0.0231) | Acc: (99.26%) (48407/48768)\n",
      "#TRAIN: Epoch: 109 | Batch_idx: 390 |  Loss: (0.0229) | Acc: (99.26%) (49632/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5486) | Acc: (88.87%) (8887/10000)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 0 |  Loss: (0.0232) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 10 |  Loss: (0.0229) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 20 |  Loss: (0.0220) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 30 |  Loss: (0.0207) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 40 |  Loss: (0.0212) | Acc: (99.45%) (5219/5248)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 50 |  Loss: (0.0201) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 60 |  Loss: (0.0194) | Acc: (99.49%) (7768/7808)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 70 |  Loss: (0.0204) | Acc: (99.42%) (9035/9088)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 80 |  Loss: (0.0205) | Acc: (99.37%) (10303/10368)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 90 |  Loss: (0.0204) | Acc: (99.36%) (11574/11648)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 100 |  Loss: (0.0202) | Acc: (99.38%) (12848/12928)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 110 |  Loss: (0.0204) | Acc: (99.36%) (14117/14208)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 120 |  Loss: (0.0202) | Acc: (99.35%) (15388/15488)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 130 |  Loss: (0.0205) | Acc: (99.34%) (16658/16768)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 140 |  Loss: (0.0212) | Acc: (99.34%) (17928/18048)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 150 |  Loss: (0.0213) | Acc: (99.33%) (19198/19328)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 160 |  Loss: (0.0212) | Acc: (99.32%) (20468/20608)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 170 |  Loss: (0.0214) | Acc: (99.32%) (21739/21888)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 180 |  Loss: (0.0216) | Acc: (99.32%) (23010/23168)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 190 |  Loss: (0.0213) | Acc: (99.33%) (24283/24448)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 200 |  Loss: (0.0212) | Acc: (99.33%) (25556/25728)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 210 |  Loss: (0.0209) | Acc: (99.34%) (26831/27008)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 220 |  Loss: (0.0206) | Acc: (99.36%) (28107/28288)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 230 |  Loss: (0.0207) | Acc: (99.35%) (29377/29568)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 240 |  Loss: (0.0206) | Acc: (99.35%) (30648/30848)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 250 |  Loss: (0.0207) | Acc: (99.35%) (31918/32128)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 260 |  Loss: (0.0207) | Acc: (99.35%) (33191/33408)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 270 |  Loss: (0.0206) | Acc: (99.35%) (34463/34688)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 280 |  Loss: (0.0205) | Acc: (99.35%) (35734/35968)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 290 |  Loss: (0.0206) | Acc: (99.34%) (37002/37248)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 300 |  Loss: (0.0206) | Acc: (99.34%) (38273/38528)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 310 |  Loss: (0.0206) | Acc: (99.34%) (39546/39808)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 320 |  Loss: (0.0208) | Acc: (99.34%) (40817/41088)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 330 |  Loss: (0.0208) | Acc: (99.34%) (42089/42368)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 340 |  Loss: (0.0206) | Acc: (99.35%) (43364/43648)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 350 |  Loss: (0.0208) | Acc: (99.34%) (44632/44928)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 360 |  Loss: (0.0207) | Acc: (99.34%) (45904/46208)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 370 |  Loss: (0.0206) | Acc: (99.34%) (47176/47488)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 380 |  Loss: (0.0205) | Acc: (99.34%) (48448/48768)\n",
      "#TRAIN: Epoch: 110 | Batch_idx: 390 |  Loss: (0.0207) | Acc: (99.34%) (49668/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5470) | Acc: (89.05%) (8905/10000)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 0 |  Loss: (0.0172) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 10 |  Loss: (0.0251) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 20 |  Loss: (0.0216) | Acc: (99.29%) (2669/2688)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 30 |  Loss: (0.0228) | Acc: (99.22%) (3937/3968)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 40 |  Loss: (0.0221) | Acc: (99.22%) (5207/5248)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 50 |  Loss: (0.0231) | Acc: (99.22%) (6477/6528)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 60 |  Loss: (0.0216) | Acc: (99.33%) (7756/7808)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 70 |  Loss: (0.0210) | Acc: (99.35%) (9029/9088)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 80 |  Loss: (0.0201) | Acc: (99.39%) (10305/10368)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 90 |  Loss: (0.0195) | Acc: (99.42%) (11580/11648)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 100 |  Loss: (0.0199) | Acc: (99.40%) (12851/12928)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 110 |  Loss: (0.0201) | Acc: (99.39%) (14122/14208)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 120 |  Loss: (0.0202) | Acc: (99.38%) (15392/15488)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 130 |  Loss: (0.0203) | Acc: (99.36%) (16661/16768)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 140 |  Loss: (0.0202) | Acc: (99.37%) (17934/18048)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 150 |  Loss: (0.0197) | Acc: (99.39%) (19211/19328)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 160 |  Loss: (0.0194) | Acc: (99.40%) (20484/20608)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 170 |  Loss: (0.0200) | Acc: (99.38%) (21752/21888)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 180 |  Loss: (0.0200) | Acc: (99.39%) (23027/23168)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 190 |  Loss: (0.0201) | Acc: (99.38%) (24297/24448)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 200 |  Loss: (0.0197) | Acc: (99.41%) (25575/25728)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 210 |  Loss: (0.0193) | Acc: (99.42%) (26851/27008)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 220 |  Loss: (0.0194) | Acc: (99.41%) (28122/28288)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 230 |  Loss: (0.0199) | Acc: (99.39%) (29389/29568)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 240 |  Loss: (0.0200) | Acc: (99.39%) (30660/30848)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 250 |  Loss: (0.0202) | Acc: (99.38%) (31928/32128)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 260 |  Loss: (0.0202) | Acc: (99.37%) (33196/33408)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 270 |  Loss: (0.0203) | Acc: (99.36%) (34465/34688)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 280 |  Loss: (0.0206) | Acc: (99.34%) (35730/35968)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 290 |  Loss: (0.0208) | Acc: (99.33%) (36999/37248)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 300 |  Loss: (0.0206) | Acc: (99.34%) (38274/38528)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 310 |  Loss: (0.0206) | Acc: (99.35%) (39549/39808)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 320 |  Loss: (0.0205) | Acc: (99.36%) (40824/41088)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 330 |  Loss: (0.0204) | Acc: (99.36%) (42095/42368)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 340 |  Loss: (0.0205) | Acc: (99.35%) (43364/43648)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 350 |  Loss: (0.0209) | Acc: (99.34%) (44631/44928)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 360 |  Loss: (0.0212) | Acc: (99.33%) (45899/46208)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 370 |  Loss: (0.0209) | Acc: (99.34%) (47175/47488)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 380 |  Loss: (0.0209) | Acc: (99.35%) (48450/48768)\n",
      "#TRAIN: Epoch: 111 | Batch_idx: 390 |  Loss: (0.0210) | Acc: (99.34%) (49672/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5524) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 0 |  Loss: (0.0085) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 10 |  Loss: (0.0242) | Acc: (99.15%) (1396/1408)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 20 |  Loss: (0.0203) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 30 |  Loss: (0.0206) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 40 |  Loss: (0.0192) | Acc: (99.41%) (5217/5248)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 50 |  Loss: (0.0186) | Acc: (99.45%) (6492/6528)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 60 |  Loss: (0.0202) | Acc: (99.39%) (7760/7808)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 70 |  Loss: (0.0203) | Acc: (99.37%) (9031/9088)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 80 |  Loss: (0.0200) | Acc: (99.39%) (10305/10368)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 90 |  Loss: (0.0202) | Acc: (99.36%) (11574/11648)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 100 |  Loss: (0.0202) | Acc: (99.37%) (12847/12928)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 110 |  Loss: (0.0205) | Acc: (99.35%) (14116/14208)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 120 |  Loss: (0.0208) | Acc: (99.33%) (15384/15488)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 130 |  Loss: (0.0205) | Acc: (99.35%) (16659/16768)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 140 |  Loss: (0.0211) | Acc: (99.32%) (17925/18048)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 150 |  Loss: (0.0215) | Acc: (99.30%) (19193/19328)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 160 |  Loss: (0.0214) | Acc: (99.31%) (20466/20608)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 170 |  Loss: (0.0211) | Acc: (99.33%) (21742/21888)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 180 |  Loss: (0.0212) | Acc: (99.33%) (23012/23168)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 190 |  Loss: (0.0212) | Acc: (99.33%) (24285/24448)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 200 |  Loss: (0.0210) | Acc: (99.34%) (25558/25728)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 210 |  Loss: (0.0208) | Acc: (99.36%) (26834/27008)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 220 |  Loss: (0.0205) | Acc: (99.36%) (28108/28288)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 230 |  Loss: (0.0203) | Acc: (99.37%) (29381/29568)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 240 |  Loss: (0.0205) | Acc: (99.36%) (30652/30848)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 250 |  Loss: (0.0202) | Acc: (99.37%) (31927/32128)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 260 |  Loss: (0.0203) | Acc: (99.37%) (33199/33408)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 270 |  Loss: (0.0202) | Acc: (99.38%) (34473/34688)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 280 |  Loss: (0.0203) | Acc: (99.38%) (35745/35968)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 290 |  Loss: (0.0204) | Acc: (99.38%) (37018/37248)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 300 |  Loss: (0.0204) | Acc: (99.38%) (38290/38528)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 310 |  Loss: (0.0203) | Acc: (99.38%) (39563/39808)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 320 |  Loss: (0.0205) | Acc: (99.38%) (40833/41088)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 330 |  Loss: (0.0202) | Acc: (99.40%) (42112/42368)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 340 |  Loss: (0.0203) | Acc: (99.39%) (43380/43648)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 350 |  Loss: (0.0203) | Acc: (99.39%) (44654/44928)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 360 |  Loss: (0.0202) | Acc: (99.39%) (45927/46208)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 370 |  Loss: (0.0203) | Acc: (99.39%) (47196/47488)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 380 |  Loss: (0.0203) | Acc: (99.39%) (48472/48768)\n",
      "#TRAIN: Epoch: 112 | Batch_idx: 390 |  Loss: (0.0204) | Acc: (99.39%) (49695/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5514) | Acc: (89.02%) (8902/10000)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 0 |  Loss: (0.0330) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 10 |  Loss: (0.0207) | Acc: (99.29%) (1398/1408)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 20 |  Loss: (0.0209) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 30 |  Loss: (0.0225) | Acc: (99.32%) (3941/3968)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 40 |  Loss: (0.0234) | Acc: (99.28%) (5210/5248)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 50 |  Loss: (0.0220) | Acc: (99.34%) (6485/6528)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 60 |  Loss: (0.0218) | Acc: (99.30%) (7753/7808)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 70 |  Loss: (0.0226) | Acc: (99.32%) (9026/9088)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 80 |  Loss: (0.0220) | Acc: (99.32%) (10297/10368)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 90 |  Loss: (0.0220) | Acc: (99.33%) (11570/11648)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 100 |  Loss: (0.0220) | Acc: (99.30%) (12838/12928)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 110 |  Loss: (0.0223) | Acc: (99.31%) (14110/14208)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 120 |  Loss: (0.0216) | Acc: (99.34%) (15386/15488)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 130 |  Loss: (0.0214) | Acc: (99.36%) (16660/16768)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 140 |  Loss: (0.0215) | Acc: (99.35%) (17931/18048)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 150 |  Loss: (0.0215) | Acc: (99.36%) (19205/19328)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 160 |  Loss: (0.0208) | Acc: (99.38%) (20481/20608)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 170 |  Loss: (0.0208) | Acc: (99.39%) (21754/21888)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 180 |  Loss: (0.0206) | Acc: (99.39%) (23027/23168)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 190 |  Loss: (0.0203) | Acc: (99.39%) (24300/24448)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 200 |  Loss: (0.0204) | Acc: (99.39%) (25572/25728)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 210 |  Loss: (0.0206) | Acc: (99.39%) (26843/27008)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 220 |  Loss: (0.0203) | Acc: (99.40%) (28119/28288)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 230 |  Loss: (0.0202) | Acc: (99.41%) (29393/29568)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 240 |  Loss: (0.0202) | Acc: (99.41%) (30665/30848)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 250 |  Loss: (0.0200) | Acc: (99.41%) (31938/32128)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 260 |  Loss: (0.0200) | Acc: (99.40%) (33209/33408)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 270 |  Loss: (0.0198) | Acc: (99.42%) (34486/34688)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 280 |  Loss: (0.0199) | Acc: (99.41%) (35756/35968)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 290 |  Loss: (0.0198) | Acc: (99.41%) (37028/37248)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 300 |  Loss: (0.0196) | Acc: (99.42%) (38303/38528)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 310 |  Loss: (0.0197) | Acc: (99.41%) (39574/39808)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 320 |  Loss: (0.0199) | Acc: (99.40%) (40843/41088)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 330 |  Loss: (0.0198) | Acc: (99.41%) (42117/42368)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 340 |  Loss: (0.0196) | Acc: (99.42%) (43393/43648)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 350 |  Loss: (0.0196) | Acc: (99.41%) (44663/44928)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 360 |  Loss: (0.0197) | Acc: (99.40%) (45929/46208)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 370 |  Loss: (0.0199) | Acc: (99.39%) (47198/47488)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 380 |  Loss: (0.0199) | Acc: (99.39%) (48469/48768)\n",
      "#TRAIN: Epoch: 113 | Batch_idx: 390 |  Loss: (0.0198) | Acc: (99.39%) (49696/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5603) | Acc: (89.05%) (8905/10000)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 0 |  Loss: (0.0267) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 10 |  Loss: (0.0138) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 20 |  Loss: (0.0156) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 30 |  Loss: (0.0155) | Acc: (99.45%) (3946/3968)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 40 |  Loss: (0.0167) | Acc: (99.41%) (5217/5248)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 50 |  Loss: (0.0191) | Acc: (99.36%) (6486/6528)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 60 |  Loss: (0.0194) | Acc: (99.36%) (7758/7808)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 70 |  Loss: (0.0196) | Acc: (99.34%) (9028/9088)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 80 |  Loss: (0.0203) | Acc: (99.32%) (10298/10368)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 90 |  Loss: (0.0210) | Acc: (99.29%) (11565/11648)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 100 |  Loss: (0.0209) | Acc: (99.31%) (12839/12928)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 110 |  Loss: (0.0214) | Acc: (99.30%) (14109/14208)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 120 |  Loss: (0.0210) | Acc: (99.31%) (15381/15488)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 130 |  Loss: (0.0208) | Acc: (99.33%) (16655/16768)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 140 |  Loss: (0.0205) | Acc: (99.32%) (17926/18048)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 150 |  Loss: (0.0209) | Acc: (99.31%) (19194/19328)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 160 |  Loss: (0.0210) | Acc: (99.32%) (20467/20608)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 170 |  Loss: (0.0207) | Acc: (99.32%) (21740/21888)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 180 |  Loss: (0.0203) | Acc: (99.34%) (23015/23168)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 190 |  Loss: (0.0203) | Acc: (99.35%) (24289/24448)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 200 |  Loss: (0.0201) | Acc: (99.36%) (25563/25728)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 210 |  Loss: (0.0203) | Acc: (99.35%) (26832/27008)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 220 |  Loss: (0.0204) | Acc: (99.34%) (28100/28288)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 230 |  Loss: (0.0202) | Acc: (99.35%) (29375/29568)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 240 |  Loss: (0.0202) | Acc: (99.34%) (30644/30848)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 250 |  Loss: (0.0199) | Acc: (99.36%) (31921/32128)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 260 |  Loss: (0.0199) | Acc: (99.35%) (33191/33408)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 270 |  Loss: (0.0198) | Acc: (99.35%) (34463/34688)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 280 |  Loss: (0.0202) | Acc: (99.33%) (35728/35968)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 290 |  Loss: (0.0201) | Acc: (99.33%) (36999/37248)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 300 |  Loss: (0.0199) | Acc: (99.34%) (38273/38528)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 310 |  Loss: (0.0199) | Acc: (99.34%) (39544/39808)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 320 |  Loss: (0.0199) | Acc: (99.33%) (40814/41088)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 330 |  Loss: (0.0199) | Acc: (99.33%) (42085/42368)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 340 |  Loss: (0.0199) | Acc: (99.33%) (43357/43648)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 350 |  Loss: (0.0199) | Acc: (99.34%) (44632/44928)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 360 |  Loss: (0.0197) | Acc: (99.35%) (45908/46208)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 370 |  Loss: (0.0198) | Acc: (99.35%) (47180/47488)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 380 |  Loss: (0.0197) | Acc: (99.36%) (48454/48768)\n",
      "#TRAIN: Epoch: 114 | Batch_idx: 390 |  Loss: (0.0197) | Acc: (99.35%) (49676/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5522) | Acc: (89.12%) (8912/10000)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 0 |  Loss: (0.0363) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 10 |  Loss: (0.0191) | Acc: (99.22%) (1397/1408)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 20 |  Loss: (0.0235) | Acc: (99.18%) (2666/2688)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 30 |  Loss: (0.0227) | Acc: (99.22%) (3937/3968)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 40 |  Loss: (0.0222) | Acc: (99.29%) (5211/5248)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 50 |  Loss: (0.0206) | Acc: (99.36%) (6486/6528)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 60 |  Loss: (0.0195) | Acc: (99.39%) (7760/7808)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 70 |  Loss: (0.0186) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 80 |  Loss: (0.0186) | Acc: (99.44%) (10310/10368)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 90 |  Loss: (0.0183) | Acc: (99.44%) (11583/11648)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 100 |  Loss: (0.0185) | Acc: (99.41%) (12852/12928)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 110 |  Loss: (0.0190) | Acc: (99.40%) (14123/14208)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 120 |  Loss: (0.0192) | Acc: (99.40%) (15395/15488)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 130 |  Loss: (0.0198) | Acc: (99.39%) (16665/16768)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 140 |  Loss: (0.0201) | Acc: (99.36%) (17933/18048)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 150 |  Loss: (0.0198) | Acc: (99.38%) (19208/19328)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 160 |  Loss: (0.0202) | Acc: (99.36%) (20476/20608)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 170 |  Loss: (0.0207) | Acc: (99.34%) (21743/21888)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 180 |  Loss: (0.0203) | Acc: (99.35%) (23018/23168)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 190 |  Loss: (0.0201) | Acc: (99.37%) (24293/24448)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 200 |  Loss: (0.0201) | Acc: (99.37%) (25566/25728)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 210 |  Loss: (0.0207) | Acc: (99.36%) (26835/27008)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 220 |  Loss: (0.0208) | Acc: (99.35%) (28105/28288)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 230 |  Loss: (0.0206) | Acc: (99.37%) (29381/29568)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 240 |  Loss: (0.0207) | Acc: (99.37%) (30654/30848)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 250 |  Loss: (0.0205) | Acc: (99.37%) (31926/32128)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 260 |  Loss: (0.0205) | Acc: (99.37%) (33198/33408)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 270 |  Loss: (0.0203) | Acc: (99.38%) (34472/34688)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 280 |  Loss: (0.0205) | Acc: (99.37%) (35740/35968)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 290 |  Loss: (0.0204) | Acc: (99.37%) (37015/37248)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 300 |  Loss: (0.0205) | Acc: (99.38%) (38288/38528)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 310 |  Loss: (0.0201) | Acc: (99.39%) (39564/39808)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 320 |  Loss: (0.0202) | Acc: (99.38%) (40835/41088)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 330 |  Loss: (0.0202) | Acc: (99.38%) (42106/42368)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 340 |  Loss: (0.0205) | Acc: (99.37%) (43372/43648)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 350 |  Loss: (0.0206) | Acc: (99.37%) (44643/44928)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 360 |  Loss: (0.0205) | Acc: (99.36%) (45914/46208)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 370 |  Loss: (0.0206) | Acc: (99.36%) (47184/47488)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 380 |  Loss: (0.0207) | Acc: (99.36%) (48457/48768)\n",
      "#TRAIN: Epoch: 115 | Batch_idx: 390 |  Loss: (0.0207) | Acc: (99.36%) (49678/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5559) | Acc: (88.94%) (8894/10000)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 0 |  Loss: (0.0029) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 10 |  Loss: (0.0134) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 20 |  Loss: (0.0145) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 30 |  Loss: (0.0125) | Acc: (99.65%) (3954/3968)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 40 |  Loss: (0.0131) | Acc: (99.62%) (5228/5248)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 50 |  Loss: (0.0145) | Acc: (99.59%) (6501/6528)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 60 |  Loss: (0.0139) | Acc: (99.59%) (7776/7808)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 70 |  Loss: (0.0142) | Acc: (99.60%) (9052/9088)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 80 |  Loss: (0.0147) | Acc: (99.59%) (10326/10368)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 90 |  Loss: (0.0158) | Acc: (99.53%) (11593/11648)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 100 |  Loss: (0.0165) | Acc: (99.50%) (12863/12928)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 110 |  Loss: (0.0166) | Acc: (99.50%) (14137/14208)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 120 |  Loss: (0.0164) | Acc: (99.51%) (15412/15488)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 130 |  Loss: (0.0163) | Acc: (99.52%) (16687/16768)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 140 |  Loss: (0.0168) | Acc: (99.48%) (17954/18048)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 150 |  Loss: (0.0169) | Acc: (99.48%) (19227/19328)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 160 |  Loss: (0.0176) | Acc: (99.45%) (20494/20608)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 170 |  Loss: (0.0181) | Acc: (99.44%) (21765/21888)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 180 |  Loss: (0.0179) | Acc: (99.44%) (23039/23168)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 190 |  Loss: (0.0181) | Acc: (99.43%) (24309/24448)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 200 |  Loss: (0.0180) | Acc: (99.44%) (25584/25728)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 210 |  Loss: (0.0179) | Acc: (99.45%) (26859/27008)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 220 |  Loss: (0.0181) | Acc: (99.43%) (28128/28288)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 230 |  Loss: (0.0184) | Acc: (99.44%) (29401/29568)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 240 |  Loss: (0.0184) | Acc: (99.44%) (30674/30848)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 250 |  Loss: (0.0185) | Acc: (99.43%) (31944/32128)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 260 |  Loss: (0.0185) | Acc: (99.43%) (33218/33408)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 270 |  Loss: (0.0186) | Acc: (99.42%) (34488/34688)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 280 |  Loss: (0.0186) | Acc: (99.42%) (35760/35968)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 290 |  Loss: (0.0184) | Acc: (99.43%) (37034/37248)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 300 |  Loss: (0.0188) | Acc: (99.42%) (38303/38528)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 310 |  Loss: (0.0187) | Acc: (99.42%) (39577/39808)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 320 |  Loss: (0.0187) | Acc: (99.43%) (40853/41088)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 330 |  Loss: (0.0188) | Acc: (99.42%) (42122/42368)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 340 |  Loss: (0.0188) | Acc: (99.41%) (43391/43648)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 350 |  Loss: (0.0188) | Acc: (99.41%) (44665/44928)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 360 |  Loss: (0.0189) | Acc: (99.40%) (45932/46208)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 370 |  Loss: (0.0189) | Acc: (99.40%) (47204/47488)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 380 |  Loss: (0.0188) | Acc: (99.41%) (48479/48768)\n",
      "#TRAIN: Epoch: 116 | Batch_idx: 390 |  Loss: (0.0187) | Acc: (99.41%) (49705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5584) | Acc: (89.17%) (8917/10000)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 0 |  Loss: (0.0143) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 10 |  Loss: (0.0144) | Acc: (99.72%) (1404/1408)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 20 |  Loss: (0.0197) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 30 |  Loss: (0.0181) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 40 |  Loss: (0.0188) | Acc: (99.50%) (5222/5248)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 50 |  Loss: (0.0186) | Acc: (99.46%) (6493/6528)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 60 |  Loss: (0.0191) | Acc: (99.47%) (7767/7808)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 70 |  Loss: (0.0201) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 80 |  Loss: (0.0200) | Acc: (99.42%) (10308/10368)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 90 |  Loss: (0.0208) | Acc: (99.37%) (11575/11648)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 100 |  Loss: (0.0204) | Acc: (99.40%) (12851/12928)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 110 |  Loss: (0.0207) | Acc: (99.41%) (14124/14208)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 120 |  Loss: (0.0208) | Acc: (99.41%) (15396/15488)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 130 |  Loss: (0.0213) | Acc: (99.40%) (16667/16768)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 140 |  Loss: (0.0211) | Acc: (99.38%) (17937/18048)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 150 |  Loss: (0.0204) | Acc: (99.41%) (19213/19328)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 160 |  Loss: (0.0202) | Acc: (99.40%) (20485/20608)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 170 |  Loss: (0.0198) | Acc: (99.42%) (21760/21888)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 180 |  Loss: (0.0201) | Acc: (99.37%) (23022/23168)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 190 |  Loss: (0.0199) | Acc: (99.38%) (24297/24448)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 200 |  Loss: (0.0197) | Acc: (99.38%) (25569/25728)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 210 |  Loss: (0.0197) | Acc: (99.39%) (26842/27008)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 220 |  Loss: (0.0200) | Acc: (99.37%) (28111/28288)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 230 |  Loss: (0.0197) | Acc: (99.38%) (29385/29568)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 240 |  Loss: (0.0194) | Acc: (99.39%) (30660/30848)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 250 |  Loss: (0.0193) | Acc: (99.39%) (31931/32128)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 260 |  Loss: (0.0193) | Acc: (99.39%) (33204/33408)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 270 |  Loss: (0.0191) | Acc: (99.40%) (34479/34688)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 280 |  Loss: (0.0191) | Acc: (99.39%) (35750/35968)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 290 |  Loss: (0.0190) | Acc: (99.41%) (37027/37248)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 300 |  Loss: (0.0192) | Acc: (99.40%) (38295/38528)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 310 |  Loss: (0.0193) | Acc: (99.39%) (39566/39808)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 320 |  Loss: (0.0191) | Acc: (99.40%) (40842/41088)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 330 |  Loss: (0.0190) | Acc: (99.40%) (42112/42368)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 340 |  Loss: (0.0191) | Acc: (99.40%) (43384/43648)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 350 |  Loss: (0.0191) | Acc: (99.39%) (44656/44928)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 360 |  Loss: (0.0190) | Acc: (99.40%) (45929/46208)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 370 |  Loss: (0.0190) | Acc: (99.40%) (47202/47488)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 380 |  Loss: (0.0189) | Acc: (99.40%) (48474/48768)\n",
      "#TRAIN: Epoch: 117 | Batch_idx: 390 |  Loss: (0.0189) | Acc: (99.40%) (49698/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5591) | Acc: (89.00%) (8900/10000)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 0 |  Loss: (0.0322) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 10 |  Loss: (0.0206) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 20 |  Loss: (0.0216) | Acc: (99.29%) (2669/2688)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 30 |  Loss: (0.0198) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 40 |  Loss: (0.0194) | Acc: (99.39%) (5216/5248)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 50 |  Loss: (0.0203) | Acc: (99.28%) (6481/6528)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 60 |  Loss: (0.0205) | Acc: (99.32%) (7755/7808)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 70 |  Loss: (0.0201) | Acc: (99.36%) (9030/9088)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 80 |  Loss: (0.0195) | Acc: (99.38%) (10304/10368)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 90 |  Loss: (0.0193) | Acc: (99.39%) (11577/11648)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 100 |  Loss: (0.0193) | Acc: (99.38%) (12848/12928)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 110 |  Loss: (0.0191) | Acc: (99.37%) (14119/14208)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 120 |  Loss: (0.0193) | Acc: (99.38%) (15392/15488)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 130 |  Loss: (0.0196) | Acc: (99.40%) (16667/16768)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 140 |  Loss: (0.0192) | Acc: (99.41%) (17942/18048)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 150 |  Loss: (0.0193) | Acc: (99.41%) (19213/19328)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 160 |  Loss: (0.0193) | Acc: (99.40%) (20485/20608)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 170 |  Loss: (0.0196) | Acc: (99.40%) (21756/21888)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 180 |  Loss: (0.0194) | Acc: (99.41%) (23031/23168)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 190 |  Loss: (0.0198) | Acc: (99.39%) (24300/24448)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 200 |  Loss: (0.0194) | Acc: (99.41%) (25575/25728)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 210 |  Loss: (0.0191) | Acc: (99.41%) (26850/27008)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 220 |  Loss: (0.0190) | Acc: (99.42%) (28124/28288)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 230 |  Loss: (0.0190) | Acc: (99.42%) (29396/29568)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 240 |  Loss: (0.0190) | Acc: (99.42%) (30668/30848)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 250 |  Loss: (0.0188) | Acc: (99.43%) (31945/32128)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 260 |  Loss: (0.0190) | Acc: (99.41%) (33211/33408)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 270 |  Loss: (0.0189) | Acc: (99.41%) (34484/34688)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 280 |  Loss: (0.0189) | Acc: (99.42%) (35758/35968)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 290 |  Loss: (0.0187) | Acc: (99.43%) (37035/37248)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 300 |  Loss: (0.0185) | Acc: (99.44%) (38312/38528)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 310 |  Loss: (0.0187) | Acc: (99.43%) (39582/39808)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 320 |  Loss: (0.0187) | Acc: (99.43%) (40854/41088)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 330 |  Loss: (0.0184) | Acc: (99.45%) (42133/42368)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 340 |  Loss: (0.0186) | Acc: (99.44%) (43402/43648)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 350 |  Loss: (0.0184) | Acc: (99.45%) (44680/44928)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 360 |  Loss: (0.0183) | Acc: (99.45%) (45954/46208)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 370 |  Loss: (0.0184) | Acc: (99.44%) (47223/47488)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 380 |  Loss: (0.0185) | Acc: (99.44%) (48494/48768)\n",
      "#TRAIN: Epoch: 118 | Batch_idx: 390 |  Loss: (0.0184) | Acc: (99.44%) (49719/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5587) | Acc: (89.01%) (8901/10000)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 0 |  Loss: (0.0131) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 10 |  Loss: (0.0323) | Acc: (98.65%) (1389/1408)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 20 |  Loss: (0.0273) | Acc: (98.92%) (2659/2688)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 30 |  Loss: (0.0231) | Acc: (99.17%) (3935/3968)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 40 |  Loss: (0.0213) | Acc: (99.20%) (5206/5248)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 50 |  Loss: (0.0197) | Acc: (99.31%) (6483/6528)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 60 |  Loss: (0.0195) | Acc: (99.33%) (7756/7808)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 70 |  Loss: (0.0188) | Acc: (99.38%) (9032/9088)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 80 |  Loss: (0.0185) | Acc: (99.40%) (10306/10368)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 90 |  Loss: (0.0187) | Acc: (99.38%) (11576/11648)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 100 |  Loss: (0.0183) | Acc: (99.40%) (12850/12928)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 110 |  Loss: (0.0186) | Acc: (99.39%) (14122/14208)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 120 |  Loss: (0.0187) | Acc: (99.39%) (15393/15488)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 130 |  Loss: (0.0185) | Acc: (99.40%) (16668/16768)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 140 |  Loss: (0.0184) | Acc: (99.42%) (17943/18048)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 150 |  Loss: (0.0186) | Acc: (99.43%) (19217/19328)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 160 |  Loss: (0.0190) | Acc: (99.41%) (20486/20608)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 170 |  Loss: (0.0184) | Acc: (99.43%) (21763/21888)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 180 |  Loss: (0.0186) | Acc: (99.42%) (23034/23168)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 190 |  Loss: (0.0186) | Acc: (99.42%) (24307/24448)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 200 |  Loss: (0.0185) | Acc: (99.43%) (25581/25728)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 210 |  Loss: (0.0191) | Acc: (99.40%) (26846/27008)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 220 |  Loss: (0.0190) | Acc: (99.41%) (28122/28288)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 230 |  Loss: (0.0190) | Acc: (99.41%) (29395/29568)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 240 |  Loss: (0.0190) | Acc: (99.42%) (30669/30848)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 250 |  Loss: (0.0193) | Acc: (99.40%) (31936/32128)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 260 |  Loss: (0.0192) | Acc: (99.41%) (33210/33408)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 270 |  Loss: (0.0195) | Acc: (99.40%) (34480/34688)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 280 |  Loss: (0.0194) | Acc: (99.40%) (35753/35968)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 290 |  Loss: (0.0195) | Acc: (99.40%) (37025/37248)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 300 |  Loss: (0.0195) | Acc: (99.40%) (38296/38528)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 310 |  Loss: (0.0195) | Acc: (99.40%) (39570/39808)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 320 |  Loss: (0.0195) | Acc: (99.40%) (40843/41088)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 330 |  Loss: (0.0194) | Acc: (99.40%) (42113/42368)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 340 |  Loss: (0.0193) | Acc: (99.40%) (43386/43648)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 350 |  Loss: (0.0195) | Acc: (99.39%) (44656/44928)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 360 |  Loss: (0.0195) | Acc: (99.40%) (45929/46208)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 370 |  Loss: (0.0194) | Acc: (99.40%) (47204/47488)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 380 |  Loss: (0.0193) | Acc: (99.41%) (48479/48768)\n",
      "#TRAIN: Epoch: 119 | Batch_idx: 390 |  Loss: (0.0193) | Acc: (99.41%) (49705/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5613) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 0 |  Loss: (0.0327) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 10 |  Loss: (0.0231) | Acc: (99.22%) (1397/1408)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 20 |  Loss: (0.0184) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 30 |  Loss: (0.0177) | Acc: (99.47%) (3947/3968)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 40 |  Loss: (0.0161) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 50 |  Loss: (0.0158) | Acc: (99.56%) (6499/6528)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 60 |  Loss: (0.0158) | Acc: (99.56%) (7774/7808)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 70 |  Loss: (0.0161) | Acc: (99.55%) (9047/9088)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 80 |  Loss: (0.0169) | Acc: (99.50%) (10316/10368)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 90 |  Loss: (0.0163) | Acc: (99.51%) (11591/11648)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 100 |  Loss: (0.0168) | Acc: (99.52%) (12866/12928)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 110 |  Loss: (0.0165) | Acc: (99.51%) (14139/14208)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 120 |  Loss: (0.0171) | Acc: (99.50%) (15411/15488)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 130 |  Loss: (0.0171) | Acc: (99.51%) (16685/16768)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 140 |  Loss: (0.0174) | Acc: (99.49%) (17956/18048)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 150 |  Loss: (0.0173) | Acc: (99.50%) (19231/19328)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 160 |  Loss: (0.0175) | Acc: (99.50%) (20504/20608)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 170 |  Loss: (0.0174) | Acc: (99.50%) (21778/21888)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 180 |  Loss: (0.0173) | Acc: (99.50%) (23052/23168)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 190 |  Loss: (0.0175) | Acc: (99.50%) (24325/24448)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 200 |  Loss: (0.0173) | Acc: (99.49%) (25598/25728)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 210 |  Loss: (0.0175) | Acc: (99.49%) (26869/27008)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 220 |  Loss: (0.0171) | Acc: (99.49%) (28144/28288)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 230 |  Loss: (0.0177) | Acc: (99.46%) (29409/29568)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 240 |  Loss: (0.0175) | Acc: (99.47%) (30683/30848)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 250 |  Loss: (0.0172) | Acc: (99.47%) (31959/32128)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 260 |  Loss: (0.0170) | Acc: (99.49%) (33236/33408)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 270 |  Loss: (0.0172) | Acc: (99.47%) (34505/34688)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 280 |  Loss: (0.0171) | Acc: (99.48%) (35780/35968)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 290 |  Loss: (0.0170) | Acc: (99.49%) (37057/37248)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 300 |  Loss: (0.0170) | Acc: (99.48%) (38329/38528)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 310 |  Loss: (0.0171) | Acc: (99.48%) (39600/39808)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 320 |  Loss: (0.0172) | Acc: (99.47%) (40870/41088)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 330 |  Loss: (0.0171) | Acc: (99.47%) (42145/42368)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 340 |  Loss: (0.0175) | Acc: (99.45%) (43410/43648)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 350 |  Loss: (0.0176) | Acc: (99.46%) (44684/44928)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 360 |  Loss: (0.0175) | Acc: (99.46%) (45958/46208)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 370 |  Loss: (0.0173) | Acc: (99.47%) (47235/47488)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 380 |  Loss: (0.0174) | Acc: (99.47%) (48508/48768)\n",
      "#TRAIN: Epoch: 120 | Batch_idx: 390 |  Loss: (0.0173) | Acc: (99.47%) (49737/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5620) | Acc: (89.19%) (8919/10000)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 0 |  Loss: (0.0091) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 10 |  Loss: (0.0155) | Acc: (99.50%) (1401/1408)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 20 |  Loss: (0.0165) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 30 |  Loss: (0.0183) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 40 |  Loss: (0.0176) | Acc: (99.56%) (5225/5248)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 50 |  Loss: (0.0172) | Acc: (99.60%) (6502/6528)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 60 |  Loss: (0.0194) | Acc: (99.53%) (7771/7808)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 70 |  Loss: (0.0192) | Acc: (99.53%) (9045/9088)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 80 |  Loss: (0.0182) | Acc: (99.55%) (10321/10368)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 90 |  Loss: (0.0182) | Acc: (99.56%) (11597/11648)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 100 |  Loss: (0.0177) | Acc: (99.57%) (12873/12928)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 110 |  Loss: (0.0172) | Acc: (99.58%) (14149/14208)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 120 |  Loss: (0.0166) | Acc: (99.60%) (15426/15488)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 130 |  Loss: (0.0172) | Acc: (99.58%) (16697/16768)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 140 |  Loss: (0.0174) | Acc: (99.57%) (17970/18048)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 150 |  Loss: (0.0175) | Acc: (99.57%) (19245/19328)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 160 |  Loss: (0.0170) | Acc: (99.59%) (20523/20608)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 170 |  Loss: (0.0167) | Acc: (99.60%) (21800/21888)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 180 |  Loss: (0.0168) | Acc: (99.58%) (23071/23168)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 190 |  Loss: (0.0168) | Acc: (99.58%) (24345/24448)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 200 |  Loss: (0.0172) | Acc: (99.55%) (25611/25728)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 210 |  Loss: (0.0170) | Acc: (99.54%) (26885/27008)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 220 |  Loss: (0.0169) | Acc: (99.54%) (28159/28288)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.54%) (29433/29568)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 240 |  Loss: (0.0174) | Acc: (99.53%) (30704/30848)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 250 |  Loss: (0.0171) | Acc: (99.54%) (31980/32128)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 260 |  Loss: (0.0171) | Acc: (99.53%) (33252/33408)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 270 |  Loss: (0.0172) | Acc: (99.53%) (34526/34688)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 280 |  Loss: (0.0175) | Acc: (99.52%) (35797/35968)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 290 |  Loss: (0.0172) | Acc: (99.53%) (37074/37248)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 300 |  Loss: (0.0172) | Acc: (99.53%) (38348/38528)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 310 |  Loss: (0.0173) | Acc: (99.53%) (39621/39808)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 320 |  Loss: (0.0171) | Acc: (99.54%) (40899/41088)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 330 |  Loss: (0.0171) | Acc: (99.54%) (42173/42368)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 340 |  Loss: (0.0175) | Acc: (99.53%) (43442/43648)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 350 |  Loss: (0.0175) | Acc: (99.53%) (44715/44928)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 360 |  Loss: (0.0174) | Acc: (99.52%) (45987/46208)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 370 |  Loss: (0.0172) | Acc: (99.53%) (47263/47488)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 380 |  Loss: (0.0172) | Acc: (99.53%) (48539/48768)\n",
      "#TRAIN: Epoch: 121 | Batch_idx: 390 |  Loss: (0.0172) | Acc: (99.53%) (49766/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5594) | Acc: (89.16%) (8916/10000)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 0 |  Loss: (0.0146) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 10 |  Loss: (0.0151) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 20 |  Loss: (0.0140) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 30 |  Loss: (0.0157) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 40 |  Loss: (0.0157) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 50 |  Loss: (0.0167) | Acc: (99.49%) (6495/6528)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 60 |  Loss: (0.0184) | Acc: (99.44%) (7764/7808)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 70 |  Loss: (0.0182) | Acc: (99.41%) (9034/9088)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 80 |  Loss: (0.0188) | Acc: (99.42%) (10308/10368)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 90 |  Loss: (0.0191) | Acc: (99.41%) (11579/11648)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 100 |  Loss: (0.0191) | Acc: (99.38%) (12848/12928)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 110 |  Loss: (0.0187) | Acc: (99.40%) (14123/14208)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 120 |  Loss: (0.0181) | Acc: (99.43%) (15399/15488)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 130 |  Loss: (0.0182) | Acc: (99.42%) (16670/16768)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 140 |  Loss: (0.0184) | Acc: (99.41%) (17941/18048)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 150 |  Loss: (0.0183) | Acc: (99.42%) (19215/19328)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 160 |  Loss: (0.0187) | Acc: (99.40%) (20484/20608)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 170 |  Loss: (0.0185) | Acc: (99.39%) (21755/21888)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 180 |  Loss: (0.0184) | Acc: (99.40%) (23030/23168)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 190 |  Loss: (0.0181) | Acc: (99.42%) (24306/24448)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 200 |  Loss: (0.0179) | Acc: (99.43%) (25581/25728)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 210 |  Loss: (0.0181) | Acc: (99.42%) (26852/27008)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 220 |  Loss: (0.0180) | Acc: (99.43%) (28128/28288)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 230 |  Loss: (0.0181) | Acc: (99.43%) (29398/29568)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 240 |  Loss: (0.0183) | Acc: (99.42%) (30670/30848)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 250 |  Loss: (0.0183) | Acc: (99.42%) (31942/32128)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 260 |  Loss: (0.0184) | Acc: (99.42%) (33214/33408)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 270 |  Loss: (0.0185) | Acc: (99.41%) (34485/34688)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 280 |  Loss: (0.0186) | Acc: (99.41%) (35756/35968)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 290 |  Loss: (0.0185) | Acc: (99.41%) (37030/37248)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 300 |  Loss: (0.0185) | Acc: (99.41%) (38302/38528)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 310 |  Loss: (0.0181) | Acc: (99.43%) (39581/39808)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 320 |  Loss: (0.0180) | Acc: (99.43%) (40854/41088)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 330 |  Loss: (0.0181) | Acc: (99.44%) (42129/42368)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 340 |  Loss: (0.0180) | Acc: (99.44%) (43403/43648)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 350 |  Loss: (0.0179) | Acc: (99.45%) (44679/44928)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 360 |  Loss: (0.0179) | Acc: (99.44%) (45951/46208)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 370 |  Loss: (0.0179) | Acc: (99.44%) (47222/47488)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 380 |  Loss: (0.0179) | Acc: (99.44%) (48493/48768)\n",
      "#TRAIN: Epoch: 122 | Batch_idx: 390 |  Loss: (0.0179) | Acc: (99.44%) (49718/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5638) | Acc: (89.12%) (8912/10000)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 0 |  Loss: (0.0225) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 10 |  Loss: (0.0191) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 20 |  Loss: (0.0195) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 30 |  Loss: (0.0191) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 40 |  Loss: (0.0166) | Acc: (99.47%) (5220/5248)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 50 |  Loss: (0.0182) | Acc: (99.40%) (6489/6528)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 60 |  Loss: (0.0181) | Acc: (99.44%) (7764/7808)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 70 |  Loss: (0.0179) | Acc: (99.43%) (9036/9088)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 80 |  Loss: (0.0175) | Acc: (99.45%) (10311/10368)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 90 |  Loss: (0.0178) | Acc: (99.44%) (11583/11648)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 100 |  Loss: (0.0182) | Acc: (99.44%) (12856/12928)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 110 |  Loss: (0.0181) | Acc: (99.46%) (14131/14208)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 120 |  Loss: (0.0181) | Acc: (99.46%) (15405/15488)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 130 |  Loss: (0.0175) | Acc: (99.49%) (16682/16768)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 140 |  Loss: (0.0174) | Acc: (99.48%) (17955/18048)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 150 |  Loss: (0.0170) | Acc: (99.50%) (19232/19328)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 160 |  Loss: (0.0174) | Acc: (99.50%) (20504/20608)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 170 |  Loss: (0.0176) | Acc: (99.49%) (21777/21888)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 180 |  Loss: (0.0172) | Acc: (99.49%) (23051/23168)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 190 |  Loss: (0.0174) | Acc: (99.48%) (24322/24448)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 200 |  Loss: (0.0174) | Acc: (99.48%) (25595/25728)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 210 |  Loss: (0.0172) | Acc: (99.49%) (26870/27008)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 220 |  Loss: (0.0175) | Acc: (99.48%) (28140/28288)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 230 |  Loss: (0.0175) | Acc: (99.48%) (29414/29568)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 240 |  Loss: (0.0176) | Acc: (99.47%) (30686/30848)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 250 |  Loss: (0.0175) | Acc: (99.48%) (31960/32128)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 260 |  Loss: (0.0175) | Acc: (99.48%) (33235/33408)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 270 |  Loss: (0.0175) | Acc: (99.48%) (34507/34688)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 280 |  Loss: (0.0175) | Acc: (99.48%) (35781/35968)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 290 |  Loss: (0.0173) | Acc: (99.48%) (37056/37248)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 300 |  Loss: (0.0173) | Acc: (99.49%) (38330/38528)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 310 |  Loss: (0.0174) | Acc: (99.48%) (39602/39808)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 320 |  Loss: (0.0173) | Acc: (99.49%) (40879/41088)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 330 |  Loss: (0.0172) | Acc: (99.49%) (42153/42368)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 340 |  Loss: (0.0172) | Acc: (99.49%) (43426/43648)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 350 |  Loss: (0.0173) | Acc: (99.49%) (44697/44928)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 360 |  Loss: (0.0174) | Acc: (99.48%) (45969/46208)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 370 |  Loss: (0.0173) | Acc: (99.49%) (47245/47488)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 380 |  Loss: (0.0174) | Acc: (99.48%) (48516/48768)\n",
      "#TRAIN: Epoch: 123 | Batch_idx: 390 |  Loss: (0.0174) | Acc: (99.49%) (49744/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5617) | Acc: (88.88%) (8888/10000)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 0 |  Loss: (0.0111) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 10 |  Loss: (0.0228) | Acc: (99.22%) (1397/1408)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 20 |  Loss: (0.0170) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 30 |  Loss: (0.0146) | Acc: (99.60%) (3952/3968)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 40 |  Loss: (0.0145) | Acc: (99.60%) (5227/5248)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 50 |  Loss: (0.0157) | Acc: (99.53%) (6497/6528)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 60 |  Loss: (0.0159) | Acc: (99.50%) (7769/7808)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 70 |  Loss: (0.0162) | Acc: (99.48%) (9041/9088)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 80 |  Loss: (0.0160) | Acc: (99.49%) (10315/10368)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 90 |  Loss: (0.0169) | Acc: (99.43%) (11582/11648)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 100 |  Loss: (0.0171) | Acc: (99.41%) (12852/12928)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 110 |  Loss: (0.0181) | Acc: (99.39%) (14121/14208)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 120 |  Loss: (0.0186) | Acc: (99.37%) (15390/15488)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 130 |  Loss: (0.0183) | Acc: (99.37%) (16663/16768)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 140 |  Loss: (0.0176) | Acc: (99.40%) (17940/18048)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 150 |  Loss: (0.0175) | Acc: (99.43%) (19217/19328)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 160 |  Loss: (0.0174) | Acc: (99.42%) (20488/20608)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 170 |  Loss: (0.0176) | Acc: (99.42%) (21762/21888)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 180 |  Loss: (0.0174) | Acc: (99.44%) (23038/23168)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 190 |  Loss: (0.0174) | Acc: (99.43%) (24308/24448)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 200 |  Loss: (0.0174) | Acc: (99.44%) (25583/25728)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 210 |  Loss: (0.0175) | Acc: (99.43%) (26855/27008)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 220 |  Loss: (0.0172) | Acc: (99.44%) (28130/28288)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.45%) (29404/29568)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 240 |  Loss: (0.0171) | Acc: (99.45%) (30678/30848)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 250 |  Loss: (0.0173) | Acc: (99.44%) (31949/32128)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 260 |  Loss: (0.0172) | Acc: (99.45%) (33224/33408)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 270 |  Loss: (0.0171) | Acc: (99.46%) (34499/34688)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 280 |  Loss: (0.0172) | Acc: (99.45%) (35770/35968)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 290 |  Loss: (0.0171) | Acc: (99.45%) (37044/37248)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 300 |  Loss: (0.0171) | Acc: (99.45%) (38317/38528)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 310 |  Loss: (0.0174) | Acc: (99.43%) (39583/39808)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 320 |  Loss: (0.0172) | Acc: (99.45%) (40860/41088)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 330 |  Loss: (0.0172) | Acc: (99.45%) (42135/42368)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 340 |  Loss: (0.0174) | Acc: (99.45%) (43406/43648)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 350 |  Loss: (0.0175) | Acc: (99.44%) (44675/44928)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 360 |  Loss: (0.0175) | Acc: (99.43%) (45945/46208)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 370 |  Loss: (0.0175) | Acc: (99.43%) (47218/47488)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 380 |  Loss: (0.0173) | Acc: (99.44%) (48494/48768)\n",
      "#TRAIN: Epoch: 124 | Batch_idx: 390 |  Loss: (0.0172) | Acc: (99.44%) (49722/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5615) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 0 |  Loss: (0.0232) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 10 |  Loss: (0.0190) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 20 |  Loss: (0.0189) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 30 |  Loss: (0.0190) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 40 |  Loss: (0.0186) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 50 |  Loss: (0.0191) | Acc: (99.53%) (6497/6528)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 60 |  Loss: (0.0190) | Acc: (99.47%) (7767/7808)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 70 |  Loss: (0.0186) | Acc: (99.47%) (9040/9088)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 80 |  Loss: (0.0190) | Acc: (99.46%) (10312/10368)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 90 |  Loss: (0.0193) | Acc: (99.41%) (11579/11648)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 100 |  Loss: (0.0201) | Acc: (99.37%) (12846/12928)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 110 |  Loss: (0.0197) | Acc: (99.38%) (14120/14208)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 120 |  Loss: (0.0189) | Acc: (99.41%) (15397/15488)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 130 |  Loss: (0.0187) | Acc: (99.41%) (16669/16768)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 140 |  Loss: (0.0184) | Acc: (99.41%) (17942/18048)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 150 |  Loss: (0.0185) | Acc: (99.42%) (19215/19328)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 160 |  Loss: (0.0179) | Acc: (99.44%) (20492/20608)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 170 |  Loss: (0.0185) | Acc: (99.43%) (21763/21888)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 180 |  Loss: (0.0182) | Acc: (99.43%) (23036/23168)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 190 |  Loss: (0.0183) | Acc: (99.42%) (24307/24448)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 200 |  Loss: (0.0185) | Acc: (99.41%) (25577/25728)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 210 |  Loss: (0.0184) | Acc: (99.41%) (26848/27008)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 220 |  Loss: (0.0183) | Acc: (99.42%) (28123/28288)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 230 |  Loss: (0.0181) | Acc: (99.43%) (29400/29568)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 240 |  Loss: (0.0178) | Acc: (99.44%) (30675/30848)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 250 |  Loss: (0.0181) | Acc: (99.44%) (31947/32128)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 260 |  Loss: (0.0180) | Acc: (99.43%) (33218/33408)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 270 |  Loss: (0.0180) | Acc: (99.43%) (34491/34688)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 280 |  Loss: (0.0179) | Acc: (99.43%) (35763/35968)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 290 |  Loss: (0.0182) | Acc: (99.43%) (37034/37248)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 300 |  Loss: (0.0181) | Acc: (99.43%) (38308/38528)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 310 |  Loss: (0.0181) | Acc: (99.44%) (39584/39808)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 320 |  Loss: (0.0181) | Acc: (99.44%) (40858/41088)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 330 |  Loss: (0.0182) | Acc: (99.44%) (42130/42368)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 340 |  Loss: (0.0182) | Acc: (99.44%) (43403/43648)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 350 |  Loss: (0.0181) | Acc: (99.44%) (44676/44928)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 360 |  Loss: (0.0179) | Acc: (99.44%) (45951/46208)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 370 |  Loss: (0.0178) | Acc: (99.45%) (47226/47488)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 380 |  Loss: (0.0179) | Acc: (99.45%) (48498/48768)\n",
      "#TRAIN: Epoch: 125 | Batch_idx: 390 |  Loss: (0.0180) | Acc: (99.44%) (49721/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5613) | Acc: (89.14%) (8914/10000)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 0 |  Loss: (0.0063) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 10 |  Loss: (0.0142) | Acc: (99.72%) (1404/1408)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 20 |  Loss: (0.0127) | Acc: (99.74%) (2681/2688)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 30 |  Loss: (0.0118) | Acc: (99.75%) (3958/3968)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 40 |  Loss: (0.0130) | Acc: (99.68%) (5231/5248)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 50 |  Loss: (0.0155) | Acc: (99.60%) (6502/6528)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 60 |  Loss: (0.0157) | Acc: (99.59%) (7776/7808)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 70 |  Loss: (0.0160) | Acc: (99.59%) (9051/9088)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 80 |  Loss: (0.0161) | Acc: (99.58%) (10324/10368)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 90 |  Loss: (0.0162) | Acc: (99.55%) (11596/11648)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 100 |  Loss: (0.0160) | Acc: (99.55%) (12870/12928)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 110 |  Loss: (0.0164) | Acc: (99.54%) (14142/14208)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 120 |  Loss: (0.0158) | Acc: (99.56%) (15420/15488)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 130 |  Loss: (0.0161) | Acc: (99.56%) (16694/16768)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 140 |  Loss: (0.0165) | Acc: (99.54%) (17965/18048)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 150 |  Loss: (0.0162) | Acc: (99.56%) (19242/19328)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 160 |  Loss: (0.0171) | Acc: (99.52%) (20509/20608)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 170 |  Loss: (0.0172) | Acc: (99.52%) (21782/21888)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 180 |  Loss: (0.0169) | Acc: (99.52%) (23056/23168)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 190 |  Loss: (0.0171) | Acc: (99.51%) (24327/24448)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 200 |  Loss: (0.0173) | Acc: (99.49%) (25596/25728)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 210 |  Loss: (0.0172) | Acc: (99.48%) (26867/27008)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 220 |  Loss: (0.0170) | Acc: (99.49%) (28143/28288)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 230 |  Loss: (0.0168) | Acc: (99.49%) (29417/29568)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 240 |  Loss: (0.0166) | Acc: (99.49%) (30691/30848)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 250 |  Loss: (0.0163) | Acc: (99.50%) (31968/32128)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 260 |  Loss: (0.0163) | Acc: (99.51%) (33243/33408)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 270 |  Loss: (0.0165) | Acc: (99.49%) (34512/34688)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 280 |  Loss: (0.0167) | Acc: (99.48%) (35782/35968)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 290 |  Loss: (0.0167) | Acc: (99.48%) (37053/37248)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 300 |  Loss: (0.0168) | Acc: (99.47%) (38325/38528)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 310 |  Loss: (0.0168) | Acc: (99.48%) (39601/39808)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 320 |  Loss: (0.0168) | Acc: (99.48%) (40875/41088)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 330 |  Loss: (0.0168) | Acc: (99.49%) (42150/42368)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 340 |  Loss: (0.0167) | Acc: (99.49%) (43426/43648)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 350 |  Loss: (0.0169) | Acc: (99.49%) (44697/44928)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 360 |  Loss: (0.0169) | Acc: (99.49%) (45973/46208)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.49%) (47245/47488)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 380 |  Loss: (0.0168) | Acc: (99.49%) (48517/48768)\n",
      "#TRAIN: Epoch: 126 | Batch_idx: 390 |  Loss: (0.0169) | Acc: (99.48%) (49742/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5572) | Acc: (89.08%) (8908/10000)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 0 |  Loss: (0.0235) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 10 |  Loss: (0.0195) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 20 |  Loss: (0.0188) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 30 |  Loss: (0.0195) | Acc: (99.37%) (3943/3968)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 40 |  Loss: (0.0212) | Acc: (99.37%) (5215/5248)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 50 |  Loss: (0.0206) | Acc: (99.40%) (6489/6528)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 60 |  Loss: (0.0192) | Acc: (99.45%) (7765/7808)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 70 |  Loss: (0.0182) | Acc: (99.47%) (9040/9088)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 80 |  Loss: (0.0187) | Acc: (99.43%) (10309/10368)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 90 |  Loss: (0.0176) | Acc: (99.46%) (11585/11648)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 100 |  Loss: (0.0181) | Acc: (99.45%) (12857/12928)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 110 |  Loss: (0.0185) | Acc: (99.42%) (14126/14208)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 120 |  Loss: (0.0190) | Acc: (99.42%) (15398/15488)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 130 |  Loss: (0.0191) | Acc: (99.42%) (16671/16768)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 140 |  Loss: (0.0190) | Acc: (99.42%) (17943/18048)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 150 |  Loss: (0.0189) | Acc: (99.43%) (19217/19328)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 160 |  Loss: (0.0182) | Acc: (99.45%) (20495/20608)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 170 |  Loss: (0.0180) | Acc: (99.44%) (21765/21888)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 180 |  Loss: (0.0177) | Acc: (99.46%) (23042/23168)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 190 |  Loss: (0.0177) | Acc: (99.46%) (24316/24448)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 200 |  Loss: (0.0175) | Acc: (99.46%) (25590/25728)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 210 |  Loss: (0.0177) | Acc: (99.45%) (26860/27008)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 220 |  Loss: (0.0177) | Acc: (99.45%) (28132/28288)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 230 |  Loss: (0.0175) | Acc: (99.47%) (29410/29568)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 240 |  Loss: (0.0175) | Acc: (99.46%) (30682/30848)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 250 |  Loss: (0.0175) | Acc: (99.47%) (31957/32128)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 260 |  Loss: (0.0174) | Acc: (99.47%) (33232/33408)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 270 |  Loss: (0.0174) | Acc: (99.47%) (34504/34688)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 280 |  Loss: (0.0175) | Acc: (99.46%) (35775/35968)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 290 |  Loss: (0.0173) | Acc: (99.47%) (37050/37248)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 300 |  Loss: (0.0173) | Acc: (99.48%) (38326/38528)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 310 |  Loss: (0.0174) | Acc: (99.47%) (39597/39808)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 320 |  Loss: (0.0171) | Acc: (99.47%) (40872/41088)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 330 |  Loss: (0.0171) | Acc: (99.48%) (42148/42368)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 340 |  Loss: (0.0171) | Acc: (99.48%) (43421/43648)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 350 |  Loss: (0.0171) | Acc: (99.48%) (44696/44928)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.48%) (45968/46208)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 370 |  Loss: (0.0170) | Acc: (99.49%) (47244/47488)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 380 |  Loss: (0.0171) | Acc: (99.48%) (48513/48768)\n",
      "#TRAIN: Epoch: 127 | Batch_idx: 390 |  Loss: (0.0171) | Acc: (99.48%) (49740/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5605) | Acc: (89.11%) (8911/10000)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 0 |  Loss: (0.0179) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 10 |  Loss: (0.0226) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 20 |  Loss: (0.0171) | Acc: (99.55%) (2676/2688)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 30 |  Loss: (0.0172) | Acc: (99.45%) (3946/3968)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 40 |  Loss: (0.0159) | Acc: (99.43%) (5218/5248)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 50 |  Loss: (0.0158) | Acc: (99.43%) (6491/6528)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 60 |  Loss: (0.0158) | Acc: (99.45%) (7765/7808)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 70 |  Loss: (0.0157) | Acc: (99.47%) (9040/9088)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 80 |  Loss: (0.0153) | Acc: (99.51%) (10317/10368)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 90 |  Loss: (0.0155) | Acc: (99.48%) (11587/11648)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 100 |  Loss: (0.0156) | Acc: (99.46%) (12858/12928)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 110 |  Loss: (0.0164) | Acc: (99.43%) (14127/14208)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 120 |  Loss: (0.0162) | Acc: (99.44%) (15402/15488)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 130 |  Loss: (0.0165) | Acc: (99.43%) (16673/16768)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 140 |  Loss: (0.0163) | Acc: (99.45%) (17949/18048)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 150 |  Loss: (0.0164) | Acc: (99.44%) (19220/19328)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 160 |  Loss: (0.0163) | Acc: (99.44%) (20492/20608)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 170 |  Loss: (0.0165) | Acc: (99.43%) (21764/21888)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 180 |  Loss: (0.0165) | Acc: (99.43%) (23037/23168)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 190 |  Loss: (0.0163) | Acc: (99.45%) (24313/24448)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 200 |  Loss: (0.0167) | Acc: (99.42%) (25580/25728)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 210 |  Loss: (0.0166) | Acc: (99.43%) (26854/27008)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 220 |  Loss: (0.0164) | Acc: (99.44%) (28129/28288)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 230 |  Loss: (0.0162) | Acc: (99.44%) (29402/29568)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 240 |  Loss: (0.0163) | Acc: (99.44%) (30674/30848)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 250 |  Loss: (0.0162) | Acc: (99.44%) (31949/32128)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 260 |  Loss: (0.0163) | Acc: (99.43%) (33219/33408)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 270 |  Loss: (0.0162) | Acc: (99.44%) (34494/34688)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 280 |  Loss: (0.0161) | Acc: (99.45%) (35770/35968)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 290 |  Loss: (0.0160) | Acc: (99.46%) (37045/37248)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 300 |  Loss: (0.0161) | Acc: (99.45%) (38318/38528)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 310 |  Loss: (0.0161) | Acc: (99.45%) (39591/39808)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 320 |  Loss: (0.0159) | Acc: (99.47%) (40870/41088)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 330 |  Loss: (0.0159) | Acc: (99.47%) (42143/42368)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 340 |  Loss: (0.0160) | Acc: (99.47%) (43417/43648)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 350 |  Loss: (0.0159) | Acc: (99.47%) (44690/44928)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 360 |  Loss: (0.0161) | Acc: (99.47%) (45962/46208)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 370 |  Loss: (0.0160) | Acc: (99.47%) (47236/47488)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 380 |  Loss: (0.0160) | Acc: (99.47%) (48509/48768)\n",
      "#TRAIN: Epoch: 128 | Batch_idx: 390 |  Loss: (0.0161) | Acc: (99.47%) (49733/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5615) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 0 |  Loss: (0.0143) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 10 |  Loss: (0.0184) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 20 |  Loss: (0.0172) | Acc: (99.63%) (2678/2688)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 30 |  Loss: (0.0176) | Acc: (99.60%) (3952/3968)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 40 |  Loss: (0.0160) | Acc: (99.62%) (5228/5248)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 50 |  Loss: (0.0155) | Acc: (99.60%) (6502/6528)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 60 |  Loss: (0.0153) | Acc: (99.59%) (7776/7808)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 70 |  Loss: (0.0153) | Acc: (99.61%) (9053/9088)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 80 |  Loss: (0.0154) | Acc: (99.61%) (10328/10368)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 90 |  Loss: (0.0155) | Acc: (99.60%) (11601/11648)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 100 |  Loss: (0.0160) | Acc: (99.56%) (12871/12928)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 110 |  Loss: (0.0162) | Acc: (99.54%) (14143/14208)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 120 |  Loss: (0.0162) | Acc: (99.52%) (15414/15488)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 130 |  Loss: (0.0166) | Acc: (99.50%) (16684/16768)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 140 |  Loss: (0.0163) | Acc: (99.50%) (17958/18048)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 150 |  Loss: (0.0164) | Acc: (99.49%) (19229/19328)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 160 |  Loss: (0.0167) | Acc: (99.47%) (20499/20608)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 170 |  Loss: (0.0172) | Acc: (99.47%) (21771/21888)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 180 |  Loss: (0.0171) | Acc: (99.47%) (23045/23168)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 190 |  Loss: (0.0174) | Acc: (99.46%) (24316/24448)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 200 |  Loss: (0.0172) | Acc: (99.47%) (25591/25728)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 210 |  Loss: (0.0172) | Acc: (99.47%) (26864/27008)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 220 |  Loss: (0.0174) | Acc: (99.46%) (28136/28288)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 230 |  Loss: (0.0172) | Acc: (99.48%) (29413/29568)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 240 |  Loss: (0.0171) | Acc: (99.47%) (30686/30848)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 250 |  Loss: (0.0170) | Acc: (99.48%) (31961/32128)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 260 |  Loss: (0.0169) | Acc: (99.48%) (33235/33408)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 270 |  Loss: (0.0169) | Acc: (99.48%) (34507/34688)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 280 |  Loss: (0.0170) | Acc: (99.47%) (35778/35968)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 290 |  Loss: (0.0172) | Acc: (99.47%) (37051/37248)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 300 |  Loss: (0.0171) | Acc: (99.47%) (38325/38528)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 310 |  Loss: (0.0170) | Acc: (99.47%) (39599/39808)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.48%) (40873/41088)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 330 |  Loss: (0.0173) | Acc: (99.46%) (42141/42368)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 340 |  Loss: (0.0174) | Acc: (99.46%) (43414/43648)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 350 |  Loss: (0.0173) | Acc: (99.47%) (44689/44928)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 360 |  Loss: (0.0172) | Acc: (99.48%) (45967/46208)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 370 |  Loss: (0.0171) | Acc: (99.49%) (47244/47488)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 380 |  Loss: (0.0171) | Acc: (99.48%) (48516/48768)\n",
      "#TRAIN: Epoch: 129 | Batch_idx: 390 |  Loss: (0.0173) | Acc: (99.47%) (49737/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5593) | Acc: (89.18%) (8918/10000)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 0 |  Loss: (0.0186) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 10 |  Loss: (0.0135) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 20 |  Loss: (0.0149) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 30 |  Loss: (0.0146) | Acc: (99.47%) (3947/3968)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 40 |  Loss: (0.0142) | Acc: (99.50%) (5222/5248)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 50 |  Loss: (0.0139) | Acc: (99.51%) (6496/6528)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 60 |  Loss: (0.0139) | Acc: (99.54%) (7772/7808)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 70 |  Loss: (0.0138) | Acc: (99.55%) (9047/9088)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 80 |  Loss: (0.0141) | Acc: (99.56%) (10322/10368)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 90 |  Loss: (0.0148) | Acc: (99.54%) (11594/11648)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 100 |  Loss: (0.0151) | Acc: (99.54%) (12868/12928)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 110 |  Loss: (0.0152) | Acc: (99.54%) (14142/14208)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 120 |  Loss: (0.0153) | Acc: (99.54%) (15416/15488)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 130 |  Loss: (0.0153) | Acc: (99.54%) (16691/16768)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 140 |  Loss: (0.0150) | Acc: (99.55%) (17967/18048)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 150 |  Loss: (0.0149) | Acc: (99.55%) (19241/19328)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 160 |  Loss: (0.0151) | Acc: (99.55%) (20516/20608)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 170 |  Loss: (0.0149) | Acc: (99.56%) (21792/21888)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 180 |  Loss: (0.0148) | Acc: (99.57%) (23069/23168)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 190 |  Loss: (0.0150) | Acc: (99.57%) (24344/24448)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 200 |  Loss: (0.0151) | Acc: (99.57%) (25618/25728)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 210 |  Loss: (0.0151) | Acc: (99.57%) (26892/27008)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 220 |  Loss: (0.0154) | Acc: (99.56%) (28163/28288)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 230 |  Loss: (0.0156) | Acc: (99.54%) (29433/29568)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 240 |  Loss: (0.0157) | Acc: (99.54%) (30706/30848)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 250 |  Loss: (0.0156) | Acc: (99.54%) (31979/32128)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 260 |  Loss: (0.0159) | Acc: (99.52%) (33246/33408)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 270 |  Loss: (0.0157) | Acc: (99.52%) (34522/34688)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 280 |  Loss: (0.0158) | Acc: (99.52%) (35794/35968)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 290 |  Loss: (0.0158) | Acc: (99.52%) (37068/37248)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 300 |  Loss: (0.0159) | Acc: (99.51%) (38338/38528)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 310 |  Loss: (0.0159) | Acc: (99.51%) (39613/39808)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 320 |  Loss: (0.0158) | Acc: (99.51%) (40887/41088)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 330 |  Loss: (0.0158) | Acc: (99.51%) (42162/42368)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 340 |  Loss: (0.0158) | Acc: (99.51%) (43435/43648)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 350 |  Loss: (0.0157) | Acc: (99.51%) (44707/44928)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 360 |  Loss: (0.0157) | Acc: (99.51%) (45983/46208)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 370 |  Loss: (0.0160) | Acc: (99.50%) (47251/47488)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 380 |  Loss: (0.0161) | Acc: (99.50%) (48524/48768)\n",
      "#TRAIN: Epoch: 130 | Batch_idx: 390 |  Loss: (0.0163) | Acc: (99.49%) (49746/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5636) | Acc: (89.00%) (8900/10000)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 0 |  Loss: (0.0073) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 10 |  Loss: (0.0125) | Acc: (99.79%) (1405/1408)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 20 |  Loss: (0.0132) | Acc: (99.63%) (2678/2688)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 30 |  Loss: (0.0130) | Acc: (99.57%) (3951/3968)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 40 |  Loss: (0.0140) | Acc: (99.58%) (5226/5248)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 50 |  Loss: (0.0139) | Acc: (99.57%) (6500/6528)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 60 |  Loss: (0.0142) | Acc: (99.58%) (7775/7808)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 70 |  Loss: (0.0146) | Acc: (99.59%) (9051/9088)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 80 |  Loss: (0.0146) | Acc: (99.59%) (10325/10368)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 90 |  Loss: (0.0146) | Acc: (99.57%) (11598/11648)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 100 |  Loss: (0.0143) | Acc: (99.59%) (12875/12928)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 110 |  Loss: (0.0141) | Acc: (99.60%) (14151/14208)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 120 |  Loss: (0.0144) | Acc: (99.57%) (15422/15488)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 130 |  Loss: (0.0155) | Acc: (99.58%) (16697/16768)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 140 |  Loss: (0.0159) | Acc: (99.57%) (17970/18048)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 150 |  Loss: (0.0160) | Acc: (99.56%) (19243/19328)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 160 |  Loss: (0.0156) | Acc: (99.57%) (20519/20608)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 170 |  Loss: (0.0156) | Acc: (99.57%) (21794/21888)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 180 |  Loss: (0.0156) | Acc: (99.57%) (23068/23168)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 190 |  Loss: (0.0155) | Acc: (99.57%) (24343/24448)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 200 |  Loss: (0.0158) | Acc: (99.57%) (25618/25728)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 210 |  Loss: (0.0157) | Acc: (99.57%) (26893/27008)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 220 |  Loss: (0.0155) | Acc: (99.58%) (28170/28288)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 230 |  Loss: (0.0161) | Acc: (99.55%) (29434/29568)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 240 |  Loss: (0.0160) | Acc: (99.55%) (30709/30848)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 250 |  Loss: (0.0158) | Acc: (99.55%) (31984/32128)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 260 |  Loss: (0.0157) | Acc: (99.55%) (33259/33408)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 270 |  Loss: (0.0156) | Acc: (99.56%) (34535/34688)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 280 |  Loss: (0.0156) | Acc: (99.56%) (35808/35968)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 290 |  Loss: (0.0159) | Acc: (99.54%) (37078/37248)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 300 |  Loss: (0.0160) | Acc: (99.54%) (38349/38528)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 310 |  Loss: (0.0159) | Acc: (99.54%) (39624/39808)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 320 |  Loss: (0.0159) | Acc: (99.54%) (40897/41088)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 330 |  Loss: (0.0158) | Acc: (99.53%) (42170/42368)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 340 |  Loss: (0.0161) | Acc: (99.53%) (43442/43648)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 350 |  Loss: (0.0160) | Acc: (99.52%) (44714/44928)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 360 |  Loss: (0.0159) | Acc: (99.53%) (45993/46208)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 370 |  Loss: (0.0159) | Acc: (99.53%) (47267/47488)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 380 |  Loss: (0.0159) | Acc: (99.53%) (48540/48768)\n",
      "#TRAIN: Epoch: 131 | Batch_idx: 390 |  Loss: (0.0162) | Acc: (99.52%) (49760/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5597) | Acc: (89.04%) (8904/10000)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 0 |  Loss: (0.0092) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 10 |  Loss: (0.0227) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 20 |  Loss: (0.0215) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 30 |  Loss: (0.0219) | Acc: (99.45%) (3946/3968)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 40 |  Loss: (0.0220) | Acc: (99.43%) (5218/5248)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 50 |  Loss: (0.0216) | Acc: (99.42%) (6490/6528)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 60 |  Loss: (0.0225) | Acc: (99.35%) (7757/7808)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 70 |  Loss: (0.0204) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 80 |  Loss: (0.0190) | Acc: (99.48%) (10314/10368)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 90 |  Loss: (0.0182) | Acc: (99.52%) (11592/11648)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 100 |  Loss: (0.0177) | Acc: (99.53%) (12867/12928)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 110 |  Loss: (0.0179) | Acc: (99.51%) (14139/14208)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 120 |  Loss: (0.0179) | Acc: (99.50%) (15411/15488)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 130 |  Loss: (0.0174) | Acc: (99.52%) (16687/16768)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 140 |  Loss: (0.0175) | Acc: (99.50%) (17957/18048)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 150 |  Loss: (0.0175) | Acc: (99.49%) (19229/19328)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 160 |  Loss: (0.0176) | Acc: (99.48%) (20501/20608)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 170 |  Loss: (0.0176) | Acc: (99.47%) (21772/21888)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 180 |  Loss: (0.0176) | Acc: (99.47%) (23045/23168)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 190 |  Loss: (0.0173) | Acc: (99.49%) (24323/24448)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 200 |  Loss: (0.0177) | Acc: (99.47%) (25591/25728)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 210 |  Loss: (0.0177) | Acc: (99.46%) (26862/27008)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 220 |  Loss: (0.0178) | Acc: (99.44%) (28131/28288)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 230 |  Loss: (0.0179) | Acc: (99.45%) (29405/29568)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 240 |  Loss: (0.0178) | Acc: (99.46%) (30680/30848)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 250 |  Loss: (0.0179) | Acc: (99.46%) (31953/32128)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 260 |  Loss: (0.0176) | Acc: (99.47%) (33231/33408)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 270 |  Loss: (0.0176) | Acc: (99.47%) (34505/34688)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 280 |  Loss: (0.0176) | Acc: (99.47%) (35779/35968)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 290 |  Loss: (0.0179) | Acc: (99.47%) (37049/37248)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 300 |  Loss: (0.0178) | Acc: (99.46%) (38321/38528)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 310 |  Loss: (0.0176) | Acc: (99.47%) (39596/39808)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 320 |  Loss: (0.0178) | Acc: (99.46%) (40868/41088)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 330 |  Loss: (0.0180) | Acc: (99.45%) (42136/42368)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 340 |  Loss: (0.0182) | Acc: (99.44%) (43402/43648)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 350 |  Loss: (0.0181) | Acc: (99.44%) (44676/44928)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 360 |  Loss: (0.0181) | Acc: (99.44%) (45947/46208)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 370 |  Loss: (0.0180) | Acc: (99.44%) (47220/47488)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 380 |  Loss: (0.0179) | Acc: (99.44%) (48493/48768)\n",
      "#TRAIN: Epoch: 132 | Batch_idx: 390 |  Loss: (0.0179) | Acc: (99.44%) (49719/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5619) | Acc: (89.06%) (8906/10000)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 0 |  Loss: (0.0009) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 10 |  Loss: (0.0140) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 20 |  Loss: (0.0115) | Acc: (99.63%) (2678/2688)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 30 |  Loss: (0.0109) | Acc: (99.65%) (3954/3968)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 40 |  Loss: (0.0122) | Acc: (99.62%) (5228/5248)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 50 |  Loss: (0.0125) | Acc: (99.62%) (6503/6528)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 60 |  Loss: (0.0130) | Acc: (99.64%) (7780/7808)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 70 |  Loss: (0.0142) | Acc: (99.64%) (9055/9088)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 80 |  Loss: (0.0142) | Acc: (99.60%) (10327/10368)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 90 |  Loss: (0.0147) | Acc: (99.61%) (11602/11648)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 100 |  Loss: (0.0141) | Acc: (99.61%) (12878/12928)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 110 |  Loss: (0.0140) | Acc: (99.61%) (14153/14208)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 120 |  Loss: (0.0136) | Acc: (99.63%) (15430/15488)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 130 |  Loss: (0.0139) | Acc: (99.61%) (16703/16768)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 140 |  Loss: (0.0138) | Acc: (99.61%) (17977/18048)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 150 |  Loss: (0.0143) | Acc: (99.58%) (19247/19328)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 160 |  Loss: (0.0141) | Acc: (99.59%) (20523/20608)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 170 |  Loss: (0.0141) | Acc: (99.59%) (21798/21888)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 180 |  Loss: (0.0138) | Acc: (99.61%) (23077/23168)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 190 |  Loss: (0.0142) | Acc: (99.60%) (24351/24448)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 200 |  Loss: (0.0140) | Acc: (99.61%) (25628/25728)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 210 |  Loss: (0.0144) | Acc: (99.59%) (26897/27008)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 220 |  Loss: (0.0144) | Acc: (99.59%) (28171/28288)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 230 |  Loss: (0.0145) | Acc: (99.58%) (29443/29568)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 240 |  Loss: (0.0143) | Acc: (99.59%) (30723/30848)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 250 |  Loss: (0.0142) | Acc: (99.60%) (32001/32128)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 260 |  Loss: (0.0142) | Acc: (99.60%) (33275/33408)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 270 |  Loss: (0.0146) | Acc: (99.59%) (34545/34688)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 280 |  Loss: (0.0144) | Acc: (99.60%) (35823/35968)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 290 |  Loss: (0.0145) | Acc: (99.59%) (37097/37248)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 300 |  Loss: (0.0146) | Acc: (99.59%) (38369/38528)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 310 |  Loss: (0.0148) | Acc: (99.58%) (39641/39808)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 320 |  Loss: (0.0148) | Acc: (99.58%) (40914/41088)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 330 |  Loss: (0.0149) | Acc: (99.57%) (42187/42368)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 340 |  Loss: (0.0148) | Acc: (99.57%) (43460/43648)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 350 |  Loss: (0.0150) | Acc: (99.56%) (44730/44928)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 360 |  Loss: (0.0150) | Acc: (99.55%) (46002/46208)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 370 |  Loss: (0.0151) | Acc: (99.55%) (47273/47488)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 380 |  Loss: (0.0152) | Acc: (99.54%) (48545/48768)\n",
      "#TRAIN: Epoch: 133 | Batch_idx: 390 |  Loss: (0.0152) | Acc: (99.54%) (49770/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5571) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 0 |  Loss: (0.0185) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 10 |  Loss: (0.0152) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 20 |  Loss: (0.0188) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 30 |  Loss: (0.0185) | Acc: (99.37%) (3943/3968)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 40 |  Loss: (0.0176) | Acc: (99.47%) (5220/5248)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 50 |  Loss: (0.0178) | Acc: (99.45%) (6492/6528)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 60 |  Loss: (0.0174) | Acc: (99.45%) (7765/7808)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 70 |  Loss: (0.0164) | Acc: (99.49%) (9042/9088)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 80 |  Loss: (0.0170) | Acc: (99.48%) (10314/10368)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 90 |  Loss: (0.0172) | Acc: (99.44%) (11583/11648)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 100 |  Loss: (0.0180) | Acc: (99.44%) (12856/12928)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 110 |  Loss: (0.0180) | Acc: (99.45%) (14130/14208)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 120 |  Loss: (0.0178) | Acc: (99.45%) (15403/15488)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 130 |  Loss: (0.0173) | Acc: (99.48%) (16681/16768)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 140 |  Loss: (0.0175) | Acc: (99.47%) (17952/18048)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 150 |  Loss: (0.0175) | Acc: (99.48%) (19227/19328)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 160 |  Loss: (0.0174) | Acc: (99.49%) (20503/20608)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 170 |  Loss: (0.0173) | Acc: (99.49%) (21777/21888)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 180 |  Loss: (0.0173) | Acc: (99.46%) (23044/23168)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 190 |  Loss: (0.0174) | Acc: (99.46%) (24315/24448)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 200 |  Loss: (0.0175) | Acc: (99.47%) (25591/25728)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 210 |  Loss: (0.0173) | Acc: (99.47%) (26866/27008)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 220 |  Loss: (0.0172) | Acc: (99.47%) (28139/28288)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 230 |  Loss: (0.0170) | Acc: (99.48%) (29414/29568)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 240 |  Loss: (0.0171) | Acc: (99.48%) (30687/30848)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 250 |  Loss: (0.0168) | Acc: (99.49%) (31963/32128)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 260 |  Loss: (0.0169) | Acc: (99.49%) (33236/33408)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 270 |  Loss: (0.0168) | Acc: (99.49%) (34511/34688)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 280 |  Loss: (0.0166) | Acc: (99.49%) (35786/35968)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 290 |  Loss: (0.0164) | Acc: (99.50%) (37062/37248)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 300 |  Loss: (0.0164) | Acc: (99.50%) (38334/38528)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 310 |  Loss: (0.0166) | Acc: (99.50%) (39607/39808)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.49%) (40879/41088)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 330 |  Loss: (0.0170) | Acc: (99.49%) (42153/42368)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 340 |  Loss: (0.0170) | Acc: (99.49%) (43426/43648)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 350 |  Loss: (0.0172) | Acc: (99.48%) (44694/44928)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.48%) (45970/46208)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 370 |  Loss: (0.0172) | Acc: (99.48%) (47243/47488)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 380 |  Loss: (0.0173) | Acc: (99.48%) (48515/48768)\n",
      "#TRAIN: Epoch: 134 | Batch_idx: 390 |  Loss: (0.0171) | Acc: (99.48%) (49742/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5607) | Acc: (89.12%) (8912/10000)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 0 |  Loss: (0.0114) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 10 |  Loss: (0.0096) | Acc: (99.72%) (1404/1408)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 20 |  Loss: (0.0119) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 30 |  Loss: (0.0127) | Acc: (99.62%) (3953/3968)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 40 |  Loss: (0.0130) | Acc: (99.62%) (5228/5248)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 50 |  Loss: (0.0146) | Acc: (99.57%) (6500/6528)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 60 |  Loss: (0.0136) | Acc: (99.63%) (7779/7808)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 70 |  Loss: (0.0144) | Acc: (99.58%) (9050/9088)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 80 |  Loss: (0.0145) | Acc: (99.59%) (10325/10368)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 90 |  Loss: (0.0150) | Acc: (99.58%) (11599/11648)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 100 |  Loss: (0.0148) | Acc: (99.61%) (12877/12928)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 110 |  Loss: (0.0155) | Acc: (99.58%) (14149/14208)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 120 |  Loss: (0.0151) | Acc: (99.60%) (15426/15488)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 130 |  Loss: (0.0148) | Acc: (99.61%) (16703/16768)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 140 |  Loss: (0.0146) | Acc: (99.61%) (17977/18048)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 150 |  Loss: (0.0147) | Acc: (99.60%) (19251/19328)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 160 |  Loss: (0.0151) | Acc: (99.59%) (20524/20608)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 170 |  Loss: (0.0150) | Acc: (99.59%) (21799/21888)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 180 |  Loss: (0.0159) | Acc: (99.56%) (23065/23168)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 190 |  Loss: (0.0157) | Acc: (99.56%) (24341/24448)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 200 |  Loss: (0.0155) | Acc: (99.57%) (25617/25728)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 210 |  Loss: (0.0154) | Acc: (99.58%) (26894/27008)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 220 |  Loss: (0.0155) | Acc: (99.58%) (28168/28288)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 230 |  Loss: (0.0157) | Acc: (99.56%) (29439/29568)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 240 |  Loss: (0.0154) | Acc: (99.57%) (30715/30848)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 250 |  Loss: (0.0157) | Acc: (99.56%) (31988/32128)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 260 |  Loss: (0.0159) | Acc: (99.55%) (33257/33408)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 270 |  Loss: (0.0157) | Acc: (99.55%) (34532/34688)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 280 |  Loss: (0.0160) | Acc: (99.54%) (35803/35968)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 290 |  Loss: (0.0160) | Acc: (99.54%) (37078/37248)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 300 |  Loss: (0.0160) | Acc: (99.54%) (38351/38528)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 310 |  Loss: (0.0162) | Acc: (99.53%) (39622/39808)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 320 |  Loss: (0.0162) | Acc: (99.53%) (40895/41088)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 330 |  Loss: (0.0162) | Acc: (99.54%) (42171/42368)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 340 |  Loss: (0.0164) | Acc: (99.53%) (43442/43648)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 350 |  Loss: (0.0165) | Acc: (99.52%) (44712/44928)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 360 |  Loss: (0.0165) | Acc: (99.52%) (45984/46208)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.50%) (47252/47488)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 380 |  Loss: (0.0168) | Acc: (99.50%) (48525/48768)\n",
      "#TRAIN: Epoch: 135 | Batch_idx: 390 |  Loss: (0.0170) | Acc: (99.50%) (49748/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5617) | Acc: (89.05%) (8905/10000)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 0 |  Loss: (0.0033) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 10 |  Loss: (0.0166) | Acc: (99.50%) (1401/1408)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 20 |  Loss: (0.0130) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 30 |  Loss: (0.0147) | Acc: (99.47%) (3947/3968)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 40 |  Loss: (0.0157) | Acc: (99.41%) (5217/5248)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 50 |  Loss: (0.0163) | Acc: (99.39%) (6488/6528)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 60 |  Loss: (0.0169) | Acc: (99.41%) (7762/7808)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 70 |  Loss: (0.0175) | Acc: (99.43%) (9036/9088)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 80 |  Loss: (0.0181) | Acc: (99.39%) (10305/10368)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 90 |  Loss: (0.0179) | Acc: (99.39%) (11577/11648)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 100 |  Loss: (0.0181) | Acc: (99.37%) (12847/12928)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 110 |  Loss: (0.0180) | Acc: (99.39%) (14121/14208)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 120 |  Loss: (0.0186) | Acc: (99.36%) (15389/15488)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 130 |  Loss: (0.0182) | Acc: (99.38%) (16664/16768)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 140 |  Loss: (0.0183) | Acc: (99.38%) (17937/18048)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 150 |  Loss: (0.0184) | Acc: (99.38%) (19209/19328)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 160 |  Loss: (0.0187) | Acc: (99.37%) (20479/20608)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 170 |  Loss: (0.0183) | Acc: (99.40%) (21756/21888)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 180 |  Loss: (0.0181) | Acc: (99.40%) (23030/23168)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 190 |  Loss: (0.0181) | Acc: (99.40%) (24302/24448)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 200 |  Loss: (0.0179) | Acc: (99.41%) (25577/25728)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 210 |  Loss: (0.0179) | Acc: (99.41%) (26850/27008)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 220 |  Loss: (0.0179) | Acc: (99.41%) (28122/28288)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 230 |  Loss: (0.0175) | Acc: (99.43%) (29399/29568)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 240 |  Loss: (0.0175) | Acc: (99.43%) (30672/30848)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 250 |  Loss: (0.0175) | Acc: (99.43%) (31945/32128)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 260 |  Loss: (0.0172) | Acc: (99.45%) (33223/33408)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 270 |  Loss: (0.0171) | Acc: (99.45%) (34496/34688)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 280 |  Loss: (0.0169) | Acc: (99.46%) (35772/35968)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 290 |  Loss: (0.0168) | Acc: (99.46%) (37048/37248)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 300 |  Loss: (0.0167) | Acc: (99.47%) (38322/38528)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 310 |  Loss: (0.0166) | Acc: (99.47%) (39598/39808)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 320 |  Loss: (0.0167) | Acc: (99.47%) (40871/41088)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 330 |  Loss: (0.0167) | Acc: (99.48%) (42146/42368)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 340 |  Loss: (0.0167) | Acc: (99.48%) (43421/43648)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 350 |  Loss: (0.0169) | Acc: (99.47%) (44688/44928)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 360 |  Loss: (0.0168) | Acc: (99.47%) (45962/46208)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 370 |  Loss: (0.0167) | Acc: (99.47%) (47237/47488)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 380 |  Loss: (0.0167) | Acc: (99.47%) (48511/48768)\n",
      "#TRAIN: Epoch: 136 | Batch_idx: 390 |  Loss: (0.0168) | Acc: (99.47%) (49734/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5602) | Acc: (89.24%) (8924/10000)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 0 |  Loss: (0.0211) | Acc: (98.44%) (126/128)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 10 |  Loss: (0.0130) | Acc: (99.50%) (1401/1408)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 20 |  Loss: (0.0158) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 30 |  Loss: (0.0169) | Acc: (99.45%) (3946/3968)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 40 |  Loss: (0.0176) | Acc: (99.45%) (5219/5248)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 50 |  Loss: (0.0171) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 60 |  Loss: (0.0170) | Acc: (99.51%) (7770/7808)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 70 |  Loss: (0.0177) | Acc: (99.53%) (9045/9088)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 80 |  Loss: (0.0174) | Acc: (99.54%) (10320/10368)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 90 |  Loss: (0.0176) | Acc: (99.51%) (11591/11648)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 100 |  Loss: (0.0181) | Acc: (99.47%) (12860/12928)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 110 |  Loss: (0.0177) | Acc: (99.47%) (14133/14208)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 120 |  Loss: (0.0174) | Acc: (99.49%) (15409/15488)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 130 |  Loss: (0.0170) | Acc: (99.50%) (16684/16768)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 140 |  Loss: (0.0177) | Acc: (99.47%) (17952/18048)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 150 |  Loss: (0.0171) | Acc: (99.49%) (19230/19328)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 160 |  Loss: (0.0169) | Acc: (99.51%) (20506/20608)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 170 |  Loss: (0.0170) | Acc: (99.50%) (21779/21888)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 180 |  Loss: (0.0170) | Acc: (99.50%) (23052/23168)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 190 |  Loss: (0.0167) | Acc: (99.52%) (24330/24448)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 200 |  Loss: (0.0165) | Acc: (99.53%) (25606/25728)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 210 |  Loss: (0.0164) | Acc: (99.53%) (26882/27008)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 220 |  Loss: (0.0167) | Acc: (99.52%) (28153/28288)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 230 |  Loss: (0.0167) | Acc: (99.52%) (29426/29568)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 240 |  Loss: (0.0165) | Acc: (99.52%) (30701/30848)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 250 |  Loss: (0.0167) | Acc: (99.51%) (31970/32128)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 260 |  Loss: (0.0168) | Acc: (99.52%) (33246/33408)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 270 |  Loss: (0.0173) | Acc: (99.49%) (34512/34688)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 280 |  Loss: (0.0172) | Acc: (99.49%) (35784/35968)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 290 |  Loss: (0.0172) | Acc: (99.49%) (37058/37248)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 300 |  Loss: (0.0173) | Acc: (99.48%) (38329/38528)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 310 |  Loss: (0.0172) | Acc: (99.49%) (39603/39808)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 320 |  Loss: (0.0172) | Acc: (99.49%) (40879/41088)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 330 |  Loss: (0.0170) | Acc: (99.49%) (42154/42368)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 340 |  Loss: (0.0172) | Acc: (99.49%) (43427/43648)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 350 |  Loss: (0.0171) | Acc: (99.49%) (44701/44928)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.50%) (45976/46208)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.51%) (47255/47488)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 380 |  Loss: (0.0168) | Acc: (99.51%) (48531/48768)\n",
      "#TRAIN: Epoch: 137 | Batch_idx: 390 |  Loss: (0.0167) | Acc: (99.52%) (49758/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5591) | Acc: (89.14%) (8914/10000)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 0 |  Loss: (0.0109) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 10 |  Loss: (0.0136) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 20 |  Loss: (0.0155) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 30 |  Loss: (0.0132) | Acc: (99.62%) (3953/3968)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 40 |  Loss: (0.0134) | Acc: (99.62%) (5228/5248)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 50 |  Loss: (0.0148) | Acc: (99.54%) (6498/6528)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 60 |  Loss: (0.0163) | Acc: (99.50%) (7769/7808)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 70 |  Loss: (0.0166) | Acc: (99.47%) (9040/9088)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 80 |  Loss: (0.0164) | Acc: (99.49%) (10315/10368)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 90 |  Loss: (0.0172) | Acc: (99.47%) (11586/11648)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 100 |  Loss: (0.0168) | Acc: (99.47%) (12860/12928)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 110 |  Loss: (0.0164) | Acc: (99.49%) (14136/14208)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 120 |  Loss: (0.0166) | Acc: (99.50%) (15410/15488)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 130 |  Loss: (0.0162) | Acc: (99.52%) (16688/16768)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 140 |  Loss: (0.0161) | Acc: (99.53%) (17963/18048)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 150 |  Loss: (0.0171) | Acc: (99.50%) (19231/19328)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 160 |  Loss: (0.0173) | Acc: (99.49%) (20503/20608)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 170 |  Loss: (0.0171) | Acc: (99.50%) (21778/21888)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 180 |  Loss: (0.0171) | Acc: (99.50%) (23052/23168)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 190 |  Loss: (0.0168) | Acc: (99.51%) (24329/24448)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 200 |  Loss: (0.0170) | Acc: (99.48%) (25595/25728)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 210 |  Loss: (0.0171) | Acc: (99.48%) (26867/27008)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 220 |  Loss: (0.0173) | Acc: (99.46%) (28135/28288)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 230 |  Loss: (0.0174) | Acc: (99.46%) (29407/29568)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 240 |  Loss: (0.0174) | Acc: (99.46%) (30681/30848)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 250 |  Loss: (0.0174) | Acc: (99.46%) (31954/32128)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 260 |  Loss: (0.0172) | Acc: (99.46%) (33229/33408)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 270 |  Loss: (0.0175) | Acc: (99.46%) (34499/34688)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 280 |  Loss: (0.0172) | Acc: (99.46%) (35775/35968)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 290 |  Loss: (0.0172) | Acc: (99.47%) (37050/37248)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 300 |  Loss: (0.0171) | Acc: (99.47%) (38323/38528)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 310 |  Loss: (0.0173) | Acc: (99.45%) (39590/39808)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 320 |  Loss: (0.0172) | Acc: (99.45%) (40863/41088)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 330 |  Loss: (0.0171) | Acc: (99.45%) (42137/42368)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 340 |  Loss: (0.0171) | Acc: (99.45%) (43409/43648)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 350 |  Loss: (0.0172) | Acc: (99.45%) (44680/44928)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 360 |  Loss: (0.0173) | Acc: (99.45%) (45952/46208)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 370 |  Loss: (0.0173) | Acc: (99.45%) (47226/47488)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 380 |  Loss: (0.0174) | Acc: (99.45%) (48500/48768)\n",
      "#TRAIN: Epoch: 138 | Batch_idx: 390 |  Loss: (0.0175) | Acc: (99.45%) (49725/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5617) | Acc: (89.24%) (8924/10000)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 0 |  Loss: (0.0050) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 10 |  Loss: (0.0150) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 20 |  Loss: (0.0161) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 30 |  Loss: (0.0142) | Acc: (99.60%) (3952/3968)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 40 |  Loss: (0.0145) | Acc: (99.60%) (5227/5248)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 50 |  Loss: (0.0150) | Acc: (99.56%) (6499/6528)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 60 |  Loss: (0.0153) | Acc: (99.53%) (7771/7808)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 70 |  Loss: (0.0154) | Acc: (99.50%) (9043/9088)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 80 |  Loss: (0.0155) | Acc: (99.51%) (10317/10368)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 90 |  Loss: (0.0154) | Acc: (99.49%) (11589/11648)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 100 |  Loss: (0.0153) | Acc: (99.51%) (12865/12928)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 110 |  Loss: (0.0156) | Acc: (99.50%) (14137/14208)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 120 |  Loss: (0.0158) | Acc: (99.50%) (15411/15488)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 130 |  Loss: (0.0160) | Acc: (99.51%) (16685/16768)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 140 |  Loss: (0.0161) | Acc: (99.49%) (17956/18048)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 150 |  Loss: (0.0165) | Acc: (99.47%) (19226/19328)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 160 |  Loss: (0.0167) | Acc: (99.46%) (20496/20608)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 170 |  Loss: (0.0165) | Acc: (99.46%) (21770/21888)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 180 |  Loss: (0.0164) | Acc: (99.46%) (23043/23168)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 190 |  Loss: (0.0162) | Acc: (99.48%) (24320/24448)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 200 |  Loss: (0.0161) | Acc: (99.48%) (25595/25728)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 210 |  Loss: (0.0159) | Acc: (99.49%) (26871/27008)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 220 |  Loss: (0.0160) | Acc: (99.49%) (28144/28288)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 230 |  Loss: (0.0159) | Acc: (99.49%) (29417/29568)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 240 |  Loss: (0.0162) | Acc: (99.48%) (30687/30848)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 250 |  Loss: (0.0166) | Acc: (99.45%) (31952/32128)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 260 |  Loss: (0.0166) | Acc: (99.45%) (33224/33408)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 270 |  Loss: (0.0167) | Acc: (99.44%) (34495/34688)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 280 |  Loss: (0.0165) | Acc: (99.45%) (35770/35968)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 290 |  Loss: (0.0167) | Acc: (99.45%) (37043/37248)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 300 |  Loss: (0.0170) | Acc: (99.44%) (38314/38528)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 310 |  Loss: (0.0170) | Acc: (99.45%) (39588/39808)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.44%) (40858/41088)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 330 |  Loss: (0.0170) | Acc: (99.45%) (42135/42368)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 340 |  Loss: (0.0168) | Acc: (99.46%) (43412/43648)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 350 |  Loss: (0.0170) | Acc: (99.46%) (44684/44928)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.45%) (45956/46208)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 370 |  Loss: (0.0172) | Acc: (99.45%) (47227/47488)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 380 |  Loss: (0.0171) | Acc: (99.45%) (48502/48768)\n",
      "#TRAIN: Epoch: 139 | Batch_idx: 390 |  Loss: (0.0170) | Acc: (99.46%) (49729/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5642) | Acc: (89.08%) (8908/10000)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 0 |  Loss: (0.0100) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 10 |  Loss: (0.0123) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 20 |  Loss: (0.0154) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 30 |  Loss: (0.0155) | Acc: (99.55%) (3950/3968)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 40 |  Loss: (0.0164) | Acc: (99.47%) (5220/5248)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 50 |  Loss: (0.0171) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 60 |  Loss: (0.0164) | Acc: (99.51%) (7770/7808)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 70 |  Loss: (0.0165) | Acc: (99.47%) (9040/9088)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 80 |  Loss: (0.0170) | Acc: (99.47%) (10313/10368)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 90 |  Loss: (0.0169) | Acc: (99.48%) (11587/11648)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 100 |  Loss: (0.0170) | Acc: (99.49%) (12862/12928)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 110 |  Loss: (0.0168) | Acc: (99.49%) (14136/14208)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 120 |  Loss: (0.0166) | Acc: (99.50%) (15411/15488)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 130 |  Loss: (0.0167) | Acc: (99.51%) (16686/16768)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 140 |  Loss: (0.0168) | Acc: (99.49%) (17956/18048)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 150 |  Loss: (0.0167) | Acc: (99.49%) (19230/19328)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 160 |  Loss: (0.0165) | Acc: (99.49%) (20503/20608)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 170 |  Loss: (0.0161) | Acc: (99.51%) (21780/21888)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 180 |  Loss: (0.0162) | Acc: (99.50%) (23053/23168)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 190 |  Loss: (0.0160) | Acc: (99.50%) (24326/24448)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 200 |  Loss: (0.0158) | Acc: (99.51%) (25601/25728)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 210 |  Loss: (0.0155) | Acc: (99.52%) (26878/27008)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 220 |  Loss: (0.0159) | Acc: (99.51%) (28148/28288)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 230 |  Loss: (0.0160) | Acc: (99.51%) (29423/29568)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 240 |  Loss: (0.0156) | Acc: (99.52%) (30700/30848)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 250 |  Loss: (0.0158) | Acc: (99.51%) (31972/32128)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 260 |  Loss: (0.0159) | Acc: (99.51%) (33245/33408)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 270 |  Loss: (0.0159) | Acc: (99.51%) (34518/34688)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 280 |  Loss: (0.0159) | Acc: (99.51%) (35792/35968)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 290 |  Loss: (0.0160) | Acc: (99.51%) (37064/37248)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 300 |  Loss: (0.0160) | Acc: (99.50%) (38337/38528)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 310 |  Loss: (0.0161) | Acc: (99.50%) (39608/39808)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 320 |  Loss: (0.0164) | Acc: (99.48%) (40876/41088)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 330 |  Loss: (0.0165) | Acc: (99.48%) (42148/42368)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 340 |  Loss: (0.0165) | Acc: (99.48%) (43420/43648)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 350 |  Loss: (0.0163) | Acc: (99.49%) (44699/44928)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 360 |  Loss: (0.0165) | Acc: (99.48%) (45968/46208)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 370 |  Loss: (0.0164) | Acc: (99.48%) (47242/47488)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 380 |  Loss: (0.0165) | Acc: (99.48%) (48512/48768)\n",
      "#TRAIN: Epoch: 140 | Batch_idx: 390 |  Loss: (0.0168) | Acc: (99.47%) (49736/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5667) | Acc: (89.01%) (8901/10000)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 0 |  Loss: (0.0040) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 10 |  Loss: (0.0143) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 20 |  Loss: (0.0137) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 30 |  Loss: (0.0147) | Acc: (99.55%) (3950/3968)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 40 |  Loss: (0.0160) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 50 |  Loss: (0.0153) | Acc: (99.59%) (6501/6528)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 60 |  Loss: (0.0162) | Acc: (99.54%) (7772/7808)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 70 |  Loss: (0.0155) | Acc: (99.55%) (9047/9088)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 80 |  Loss: (0.0160) | Acc: (99.52%) (10318/10368)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 90 |  Loss: (0.0158) | Acc: (99.50%) (11590/11648)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 100 |  Loss: (0.0161) | Acc: (99.50%) (12863/12928)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 110 |  Loss: (0.0158) | Acc: (99.51%) (14138/14208)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 120 |  Loss: (0.0159) | Acc: (99.49%) (15409/15488)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 130 |  Loss: (0.0158) | Acc: (99.49%) (16682/16768)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 140 |  Loss: (0.0161) | Acc: (99.47%) (17952/18048)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 150 |  Loss: (0.0158) | Acc: (99.48%) (19228/19328)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 160 |  Loss: (0.0157) | Acc: (99.48%) (20500/20608)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 170 |  Loss: (0.0153) | Acc: (99.50%) (21778/21888)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 180 |  Loss: (0.0154) | Acc: (99.49%) (23051/23168)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 190 |  Loss: (0.0150) | Acc: (99.52%) (24331/24448)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 200 |  Loss: (0.0151) | Acc: (99.52%) (25604/25728)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 210 |  Loss: (0.0151) | Acc: (99.51%) (26877/27008)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 220 |  Loss: (0.0151) | Acc: (99.52%) (28152/28288)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 230 |  Loss: (0.0151) | Acc: (99.52%) (29425/29568)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 240 |  Loss: (0.0153) | Acc: (99.51%) (30698/30848)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 250 |  Loss: (0.0155) | Acc: (99.51%) (31971/32128)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 260 |  Loss: (0.0158) | Acc: (99.50%) (33240/33408)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 270 |  Loss: (0.0157) | Acc: (99.50%) (34515/34688)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 280 |  Loss: (0.0156) | Acc: (99.51%) (35790/35968)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 290 |  Loss: (0.0155) | Acc: (99.51%) (37066/37248)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 300 |  Loss: (0.0153) | Acc: (99.51%) (38340/38528)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 310 |  Loss: (0.0153) | Acc: (99.51%) (39613/39808)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 320 |  Loss: (0.0154) | Acc: (99.50%) (40884/41088)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 330 |  Loss: (0.0157) | Acc: (99.49%) (42154/42368)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 340 |  Loss: (0.0157) | Acc: (99.50%) (43429/43648)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 350 |  Loss: (0.0157) | Acc: (99.50%) (44703/44928)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 360 |  Loss: (0.0157) | Acc: (99.50%) (45977/46208)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 370 |  Loss: (0.0159) | Acc: (99.49%) (47248/47488)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 380 |  Loss: (0.0159) | Acc: (99.50%) (48524/48768)\n",
      "#TRAIN: Epoch: 141 | Batch_idx: 390 |  Loss: (0.0159) | Acc: (99.49%) (49746/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5590) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 0 |  Loss: (0.0092) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 10 |  Loss: (0.0172) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 20 |  Loss: (0.0157) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 30 |  Loss: (0.0153) | Acc: (99.42%) (3945/3968)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 40 |  Loss: (0.0158) | Acc: (99.45%) (5219/5248)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 50 |  Loss: (0.0154) | Acc: (99.43%) (6491/6528)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 60 |  Loss: (0.0163) | Acc: (99.45%) (7765/7808)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 70 |  Loss: (0.0169) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 80 |  Loss: (0.0167) | Acc: (99.44%) (10310/10368)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 90 |  Loss: (0.0166) | Acc: (99.46%) (11585/11648)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 100 |  Loss: (0.0162) | Acc: (99.47%) (12860/12928)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 110 |  Loss: (0.0165) | Acc: (99.45%) (14130/14208)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 120 |  Loss: (0.0165) | Acc: (99.46%) (15404/15488)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 130 |  Loss: (0.0172) | Acc: (99.45%) (16676/16768)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 140 |  Loss: (0.0172) | Acc: (99.45%) (17948/18048)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 150 |  Loss: (0.0172) | Acc: (99.43%) (19218/19328)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 160 |  Loss: (0.0171) | Acc: (99.44%) (20492/20608)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 170 |  Loss: (0.0168) | Acc: (99.46%) (21769/21888)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 180 |  Loss: (0.0166) | Acc: (99.47%) (23046/23168)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 190 |  Loss: (0.0164) | Acc: (99.48%) (24321/24448)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 200 |  Loss: (0.0162) | Acc: (99.48%) (25595/25728)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 210 |  Loss: (0.0159) | Acc: (99.50%) (26872/27008)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 220 |  Loss: (0.0157) | Acc: (99.49%) (28145/28288)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 230 |  Loss: (0.0154) | Acc: (99.51%) (29422/29568)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 240 |  Loss: (0.0159) | Acc: (99.48%) (30687/30848)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 250 |  Loss: (0.0158) | Acc: (99.48%) (31962/32128)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 260 |  Loss: (0.0161) | Acc: (99.48%) (33233/33408)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 270 |  Loss: (0.0159) | Acc: (99.48%) (34507/34688)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 280 |  Loss: (0.0158) | Acc: (99.48%) (35782/35968)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 290 |  Loss: (0.0156) | Acc: (99.49%) (37058/37248)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 300 |  Loss: (0.0158) | Acc: (99.48%) (38329/38528)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 310 |  Loss: (0.0158) | Acc: (99.48%) (39601/39808)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 320 |  Loss: (0.0158) | Acc: (99.48%) (40876/41088)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 330 |  Loss: (0.0158) | Acc: (99.48%) (42149/42368)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 340 |  Loss: (0.0159) | Acc: (99.48%) (43423/43648)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 350 |  Loss: (0.0160) | Acc: (99.49%) (44697/44928)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 360 |  Loss: (0.0160) | Acc: (99.48%) (45970/46208)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 370 |  Loss: (0.0162) | Acc: (99.48%) (47240/47488)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 380 |  Loss: (0.0163) | Acc: (99.48%) (48512/48768)\n",
      "#TRAIN: Epoch: 142 | Batch_idx: 390 |  Loss: (0.0164) | Acc: (99.47%) (49737/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5646) | Acc: (89.19%) (8919/10000)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 0 |  Loss: (0.0146) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 10 |  Loss: (0.0122) | Acc: (99.72%) (1404/1408)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 20 |  Loss: (0.0119) | Acc: (99.67%) (2679/2688)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 30 |  Loss: (0.0131) | Acc: (99.65%) (3954/3968)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 40 |  Loss: (0.0134) | Acc: (99.68%) (5231/5248)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 50 |  Loss: (0.0134) | Acc: (99.68%) (6507/6528)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 60 |  Loss: (0.0136) | Acc: (99.64%) (7780/7808)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 70 |  Loss: (0.0133) | Acc: (99.65%) (9056/9088)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 80 |  Loss: (0.0133) | Acc: (99.65%) (10332/10368)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 90 |  Loss: (0.0135) | Acc: (99.64%) (11606/11648)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 100 |  Loss: (0.0137) | Acc: (99.64%) (12881/12928)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 110 |  Loss: (0.0140) | Acc: (99.60%) (14151/14208)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 120 |  Loss: (0.0143) | Acc: (99.58%) (15423/15488)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 130 |  Loss: (0.0139) | Acc: (99.60%) (16701/16768)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 140 |  Loss: (0.0146) | Acc: (99.57%) (17971/18048)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 150 |  Loss: (0.0148) | Acc: (99.56%) (19242/19328)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 160 |  Loss: (0.0146) | Acc: (99.56%) (20517/20608)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 170 |  Loss: (0.0148) | Acc: (99.55%) (21790/21888)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 180 |  Loss: (0.0152) | Acc: (99.54%) (23062/23168)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 190 |  Loss: (0.0155) | Acc: (99.53%) (24333/24448)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 200 |  Loss: (0.0160) | Acc: (99.51%) (25601/25728)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 210 |  Loss: (0.0161) | Acc: (99.50%) (26874/27008)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 220 |  Loss: (0.0158) | Acc: (99.52%) (28151/28288)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 230 |  Loss: (0.0161) | Acc: (99.50%) (29421/29568)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 240 |  Loss: (0.0162) | Acc: (99.50%) (30694/30848)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 250 |  Loss: (0.0161) | Acc: (99.50%) (31967/32128)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 260 |  Loss: (0.0159) | Acc: (99.51%) (33245/33408)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 270 |  Loss: (0.0161) | Acc: (99.51%) (34518/34688)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 280 |  Loss: (0.0161) | Acc: (99.51%) (35793/35968)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 290 |  Loss: (0.0163) | Acc: (99.51%) (37067/37248)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 300 |  Loss: (0.0166) | Acc: (99.50%) (38334/38528)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 310 |  Loss: (0.0165) | Acc: (99.50%) (39607/39808)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 320 |  Loss: (0.0165) | Acc: (99.49%) (40877/41088)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 330 |  Loss: (0.0166) | Acc: (99.49%) (42151/42368)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 340 |  Loss: (0.0166) | Acc: (99.49%) (43426/43648)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 350 |  Loss: (0.0166) | Acc: (99.49%) (44700/44928)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 360 |  Loss: (0.0165) | Acc: (99.49%) (45972/46208)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 370 |  Loss: (0.0165) | Acc: (99.49%) (47248/47488)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 380 |  Loss: (0.0164) | Acc: (99.50%) (48522/48768)\n",
      "#TRAIN: Epoch: 143 | Batch_idx: 390 |  Loss: (0.0164) | Acc: (99.50%) (49751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5622) | Acc: (89.16%) (8916/10000)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 0 |  Loss: (0.0226) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 10 |  Loss: (0.0100) | Acc: (99.79%) (1405/1408)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 20 |  Loss: (0.0149) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 30 |  Loss: (0.0154) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 40 |  Loss: (0.0156) | Acc: (99.47%) (5220/5248)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 50 |  Loss: (0.0152) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 60 |  Loss: (0.0154) | Acc: (99.47%) (7767/7808)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 70 |  Loss: (0.0151) | Acc: (99.48%) (9041/9088)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 80 |  Loss: (0.0148) | Acc: (99.50%) (10316/10368)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 90 |  Loss: (0.0154) | Acc: (99.47%) (11586/11648)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 100 |  Loss: (0.0164) | Acc: (99.43%) (12854/12928)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 110 |  Loss: (0.0178) | Acc: (99.37%) (14119/14208)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 120 |  Loss: (0.0174) | Acc: (99.40%) (15395/15488)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 130 |  Loss: (0.0173) | Acc: (99.40%) (16668/16768)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 140 |  Loss: (0.0174) | Acc: (99.41%) (17941/18048)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 150 |  Loss: (0.0172) | Acc: (99.41%) (19214/19328)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 160 |  Loss: (0.0167) | Acc: (99.44%) (20492/20608)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 170 |  Loss: (0.0166) | Acc: (99.44%) (21766/21888)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 180 |  Loss: (0.0164) | Acc: (99.45%) (23041/23168)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 190 |  Loss: (0.0167) | Acc: (99.45%) (24313/24448)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 200 |  Loss: (0.0166) | Acc: (99.44%) (25585/25728)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 210 |  Loss: (0.0166) | Acc: (99.45%) (26860/27008)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 220 |  Loss: (0.0166) | Acc: (99.46%) (28134/28288)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 230 |  Loss: (0.0167) | Acc: (99.46%) (29408/29568)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 240 |  Loss: (0.0165) | Acc: (99.47%) (30684/30848)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 250 |  Loss: (0.0165) | Acc: (99.47%) (31958/32128)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 260 |  Loss: (0.0164) | Acc: (99.48%) (33234/33408)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 270 |  Loss: (0.0163) | Acc: (99.48%) (34508/34688)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 280 |  Loss: (0.0162) | Acc: (99.48%) (35782/35968)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 290 |  Loss: (0.0160) | Acc: (99.49%) (37059/37248)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 300 |  Loss: (0.0159) | Acc: (99.50%) (38335/38528)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 310 |  Loss: (0.0158) | Acc: (99.50%) (39609/39808)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 320 |  Loss: (0.0161) | Acc: (99.50%) (40881/41088)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 330 |  Loss: (0.0163) | Acc: (99.49%) (42152/42368)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 340 |  Loss: (0.0161) | Acc: (99.50%) (43428/43648)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 350 |  Loss: (0.0162) | Acc: (99.49%) (44699/44928)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 360 |  Loss: (0.0161) | Acc: (99.49%) (45974/46208)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 370 |  Loss: (0.0161) | Acc: (99.48%) (47243/47488)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 380 |  Loss: (0.0160) | Acc: (99.49%) (48520/48768)\n",
      "#TRAIN: Epoch: 144 | Batch_idx: 390 |  Loss: (0.0160) | Acc: (99.49%) (49746/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5610) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 0 |  Loss: (0.0046) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 10 |  Loss: (0.0186) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 20 |  Loss: (0.0177) | Acc: (99.40%) (2672/2688)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 30 |  Loss: (0.0179) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 40 |  Loss: (0.0171) | Acc: (99.37%) (5215/5248)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 50 |  Loss: (0.0171) | Acc: (99.37%) (6487/6528)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 60 |  Loss: (0.0175) | Acc: (99.35%) (7757/7808)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 70 |  Loss: (0.0183) | Acc: (99.30%) (9024/9088)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 80 |  Loss: (0.0175) | Acc: (99.33%) (10299/10368)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 90 |  Loss: (0.0176) | Acc: (99.35%) (11572/11648)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 100 |  Loss: (0.0175) | Acc: (99.36%) (12845/12928)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 110 |  Loss: (0.0178) | Acc: (99.35%) (14116/14208)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 120 |  Loss: (0.0175) | Acc: (99.39%) (15394/15488)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 130 |  Loss: (0.0180) | Acc: (99.39%) (16666/16768)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 140 |  Loss: (0.0183) | Acc: (99.37%) (17935/18048)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 150 |  Loss: (0.0178) | Acc: (99.39%) (19211/19328)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 160 |  Loss: (0.0175) | Acc: (99.41%) (20486/20608)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 170 |  Loss: (0.0172) | Acc: (99.42%) (21762/21888)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 180 |  Loss: (0.0172) | Acc: (99.42%) (23034/23168)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 190 |  Loss: (0.0170) | Acc: (99.43%) (24309/24448)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 200 |  Loss: (0.0168) | Acc: (99.44%) (25584/25728)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 210 |  Loss: (0.0171) | Acc: (99.43%) (26854/27008)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 220 |  Loss: (0.0171) | Acc: (99.43%) (28126/28288)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.43%) (29400/29568)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 240 |  Loss: (0.0172) | Acc: (99.42%) (30669/30848)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 250 |  Loss: (0.0171) | Acc: (99.42%) (31943/32128)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 260 |  Loss: (0.0168) | Acc: (99.44%) (33222/33408)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 270 |  Loss: (0.0167) | Acc: (99.45%) (34497/34688)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 280 |  Loss: (0.0168) | Acc: (99.45%) (35770/35968)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 290 |  Loss: (0.0168) | Acc: (99.44%) (37040/37248)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 300 |  Loss: (0.0168) | Acc: (99.44%) (38314/38528)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 310 |  Loss: (0.0169) | Acc: (99.44%) (39584/39808)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 320 |  Loss: (0.0169) | Acc: (99.43%) (40852/41088)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 330 |  Loss: (0.0169) | Acc: (99.43%) (42125/42368)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 340 |  Loss: (0.0171) | Acc: (99.42%) (43396/43648)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 350 |  Loss: (0.0170) | Acc: (99.43%) (44671/44928)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 360 |  Loss: (0.0169) | Acc: (99.43%) (45946/46208)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.43%) (47218/47488)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 380 |  Loss: (0.0167) | Acc: (99.44%) (48495/48768)\n",
      "#TRAIN: Epoch: 145 | Batch_idx: 390 |  Loss: (0.0166) | Acc: (99.45%) (49724/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5622) | Acc: (89.12%) (8912/10000)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 0 |  Loss: (0.0029) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 10 |  Loss: (0.0091) | Acc: (99.86%) (1406/1408)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 20 |  Loss: (0.0113) | Acc: (99.78%) (2682/2688)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 30 |  Loss: (0.0120) | Acc: (99.72%) (3957/3968)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 40 |  Loss: (0.0134) | Acc: (99.71%) (5233/5248)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 50 |  Loss: (0.0127) | Acc: (99.69%) (6508/6528)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 60 |  Loss: (0.0146) | Acc: (99.63%) (7779/7808)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 70 |  Loss: (0.0139) | Acc: (99.66%) (9057/9088)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 80 |  Loss: (0.0141) | Acc: (99.62%) (10329/10368)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 90 |  Loss: (0.0144) | Acc: (99.59%) (11600/11648)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 100 |  Loss: (0.0138) | Acc: (99.63%) (12880/12928)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 110 |  Loss: (0.0144) | Acc: (99.61%) (14152/14208)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 120 |  Loss: (0.0147) | Acc: (99.59%) (15425/15488)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 130 |  Loss: (0.0150) | Acc: (99.58%) (16697/16768)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 140 |  Loss: (0.0155) | Acc: (99.55%) (17967/18048)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 150 |  Loss: (0.0153) | Acc: (99.56%) (19243/19328)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 160 |  Loss: (0.0153) | Acc: (99.56%) (20517/20608)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 170 |  Loss: (0.0164) | Acc: (99.52%) (21784/21888)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 180 |  Loss: (0.0164) | Acc: (99.51%) (23055/23168)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 190 |  Loss: (0.0165) | Acc: (99.51%) (24327/24448)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 200 |  Loss: (0.0167) | Acc: (99.49%) (25598/25728)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 210 |  Loss: (0.0173) | Acc: (99.48%) (26867/27008)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 220 |  Loss: (0.0170) | Acc: (99.49%) (28144/28288)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.49%) (29417/29568)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 240 |  Loss: (0.0171) | Acc: (99.48%) (30689/30848)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 250 |  Loss: (0.0171) | Acc: (99.49%) (31964/32128)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 260 |  Loss: (0.0169) | Acc: (99.49%) (33237/33408)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 270 |  Loss: (0.0170) | Acc: (99.48%) (34509/34688)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 280 |  Loss: (0.0170) | Acc: (99.48%) (35781/35968)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 290 |  Loss: (0.0168) | Acc: (99.48%) (37055/37248)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 300 |  Loss: (0.0169) | Acc: (99.48%) (38328/38528)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 310 |  Loss: (0.0170) | Acc: (99.48%) (39602/39808)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 320 |  Loss: (0.0169) | Acc: (99.48%) (40874/41088)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 330 |  Loss: (0.0167) | Acc: (99.48%) (42148/42368)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 340 |  Loss: (0.0167) | Acc: (99.48%) (43419/43648)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 350 |  Loss: (0.0167) | Acc: (99.47%) (44692/44928)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 360 |  Loss: (0.0169) | Acc: (99.47%) (45963/46208)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.47%) (47235/47488)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 380 |  Loss: (0.0169) | Acc: (99.47%) (48508/48768)\n",
      "#TRAIN: Epoch: 146 | Batch_idx: 390 |  Loss: (0.0168) | Acc: (99.47%) (49734/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5609) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 0 |  Loss: (0.0144) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 10 |  Loss: (0.0117) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 20 |  Loss: (0.0127) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 30 |  Loss: (0.0169) | Acc: (99.40%) (3944/3968)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 40 |  Loss: (0.0171) | Acc: (99.41%) (5217/5248)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 50 |  Loss: (0.0164) | Acc: (99.46%) (6493/6528)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 60 |  Loss: (0.0165) | Acc: (99.47%) (7767/7808)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 70 |  Loss: (0.0165) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 80 |  Loss: (0.0177) | Acc: (99.39%) (10305/10368)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 90 |  Loss: (0.0181) | Acc: (99.36%) (11573/11648)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 100 |  Loss: (0.0184) | Acc: (99.36%) (12845/12928)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 110 |  Loss: (0.0181) | Acc: (99.37%) (14118/14208)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 120 |  Loss: (0.0181) | Acc: (99.37%) (15390/15488)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 130 |  Loss: (0.0183) | Acc: (99.39%) (16665/16768)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 140 |  Loss: (0.0185) | Acc: (99.38%) (17937/18048)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 150 |  Loss: (0.0185) | Acc: (99.39%) (19211/19328)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 160 |  Loss: (0.0187) | Acc: (99.39%) (20482/20608)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 170 |  Loss: (0.0187) | Acc: (99.39%) (21754/21888)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 180 |  Loss: (0.0182) | Acc: (99.40%) (23029/23168)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 190 |  Loss: (0.0180) | Acc: (99.42%) (24305/24448)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 200 |  Loss: (0.0178) | Acc: (99.41%) (25577/25728)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 210 |  Loss: (0.0174) | Acc: (99.43%) (26854/27008)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 220 |  Loss: (0.0173) | Acc: (99.43%) (28127/28288)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 230 |  Loss: (0.0173) | Acc: (99.43%) (29400/29568)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 240 |  Loss: (0.0172) | Acc: (99.44%) (30674/30848)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 250 |  Loss: (0.0172) | Acc: (99.44%) (31949/32128)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 260 |  Loss: (0.0174) | Acc: (99.44%) (33220/33408)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 270 |  Loss: (0.0172) | Acc: (99.44%) (34495/34688)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 280 |  Loss: (0.0171) | Acc: (99.45%) (35770/35968)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 290 |  Loss: (0.0169) | Acc: (99.46%) (37046/37248)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 300 |  Loss: (0.0170) | Acc: (99.45%) (38315/38528)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 310 |  Loss: (0.0170) | Acc: (99.45%) (39588/39808)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 320 |  Loss: (0.0171) | Acc: (99.45%) (40861/41088)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 330 |  Loss: (0.0173) | Acc: (99.45%) (42134/42368)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 340 |  Loss: (0.0172) | Acc: (99.45%) (43410/43648)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 350 |  Loss: (0.0173) | Acc: (99.46%) (44684/44928)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.46%) (45960/46208)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 370 |  Loss: (0.0169) | Acc: (99.47%) (47237/47488)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 380 |  Loss: (0.0169) | Acc: (99.47%) (48510/48768)\n",
      "#TRAIN: Epoch: 147 | Batch_idx: 390 |  Loss: (0.0169) | Acc: (99.47%) (49733/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5580) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 0 |  Loss: (0.0202) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 10 |  Loss: (0.0165) | Acc: (99.50%) (1401/1408)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 20 |  Loss: (0.0144) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 30 |  Loss: (0.0144) | Acc: (99.55%) (3950/3968)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 40 |  Loss: (0.0152) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 50 |  Loss: (0.0149) | Acc: (99.54%) (6498/6528)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 60 |  Loss: (0.0147) | Acc: (99.54%) (7772/7808)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 70 |  Loss: (0.0145) | Acc: (99.55%) (9047/9088)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 80 |  Loss: (0.0144) | Acc: (99.58%) (10324/10368)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 90 |  Loss: (0.0146) | Acc: (99.56%) (11597/11648)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 100 |  Loss: (0.0141) | Acc: (99.59%) (12875/12928)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 110 |  Loss: (0.0141) | Acc: (99.59%) (14150/14208)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 120 |  Loss: (0.0139) | Acc: (99.61%) (15427/15488)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 130 |  Loss: (0.0138) | Acc: (99.61%) (16703/16768)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 140 |  Loss: (0.0142) | Acc: (99.60%) (17975/18048)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 150 |  Loss: (0.0143) | Acc: (99.58%) (19247/19328)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 160 |  Loss: (0.0139) | Acc: (99.61%) (20527/20608)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 170 |  Loss: (0.0140) | Acc: (99.62%) (21804/21888)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 180 |  Loss: (0.0140) | Acc: (99.62%) (23080/23168)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 190 |  Loss: (0.0142) | Acc: (99.60%) (24351/24448)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 200 |  Loss: (0.0141) | Acc: (99.60%) (25626/25728)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 210 |  Loss: (0.0140) | Acc: (99.60%) (26901/27008)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 220 |  Loss: (0.0141) | Acc: (99.59%) (28173/28288)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 230 |  Loss: (0.0143) | Acc: (99.58%) (29444/29568)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 240 |  Loss: (0.0148) | Acc: (99.57%) (30715/30848)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 250 |  Loss: (0.0148) | Acc: (99.57%) (31990/32128)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 260 |  Loss: (0.0149) | Acc: (99.57%) (33264/33408)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 270 |  Loss: (0.0149) | Acc: (99.57%) (34539/34688)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 280 |  Loss: (0.0150) | Acc: (99.57%) (35813/35968)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 290 |  Loss: (0.0150) | Acc: (99.57%) (37086/37248)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 300 |  Loss: (0.0153) | Acc: (99.56%) (38357/38528)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 310 |  Loss: (0.0153) | Acc: (99.56%) (39633/39808)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 320 |  Loss: (0.0151) | Acc: (99.56%) (40909/41088)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 330 |  Loss: (0.0150) | Acc: (99.57%) (42186/42368)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 340 |  Loss: (0.0151) | Acc: (99.56%) (43458/43648)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 350 |  Loss: (0.0150) | Acc: (99.57%) (44736/44928)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 360 |  Loss: (0.0152) | Acc: (99.57%) (46008/46208)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 370 |  Loss: (0.0151) | Acc: (99.57%) (47284/47488)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 380 |  Loss: (0.0152) | Acc: (99.56%) (48554/48768)\n",
      "#TRAIN: Epoch: 148 | Batch_idx: 390 |  Loss: (0.0151) | Acc: (99.56%) (49782/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5625) | Acc: (89.05%) (8905/10000)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 0 |  Loss: (0.0079) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 10 |  Loss: (0.0230) | Acc: (99.08%) (1395/1408)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 20 |  Loss: (0.0196) | Acc: (99.18%) (2666/2688)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 30 |  Loss: (0.0172) | Acc: (99.34%) (3942/3968)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 40 |  Loss: (0.0174) | Acc: (99.37%) (5215/5248)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 50 |  Loss: (0.0174) | Acc: (99.37%) (6487/6528)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 60 |  Loss: (0.0173) | Acc: (99.40%) (7761/7808)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 70 |  Loss: (0.0178) | Acc: (99.39%) (9033/9088)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 80 |  Loss: (0.0175) | Acc: (99.40%) (10306/10368)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 90 |  Loss: (0.0167) | Acc: (99.46%) (11585/11648)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 100 |  Loss: (0.0173) | Acc: (99.45%) (12857/12928)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 110 |  Loss: (0.0172) | Acc: (99.46%) (14131/14208)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 120 |  Loss: (0.0173) | Acc: (99.45%) (15403/15488)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 130 |  Loss: (0.0175) | Acc: (99.44%) (16674/16768)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 140 |  Loss: (0.0180) | Acc: (99.43%) (17946/18048)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 150 |  Loss: (0.0181) | Acc: (99.45%) (19221/19328)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 160 |  Loss: (0.0180) | Acc: (99.45%) (20494/20608)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 170 |  Loss: (0.0178) | Acc: (99.45%) (21767/21888)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 180 |  Loss: (0.0178) | Acc: (99.44%) (23039/23168)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 190 |  Loss: (0.0175) | Acc: (99.45%) (24314/24448)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 200 |  Loss: (0.0174) | Acc: (99.46%) (25588/25728)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 210 |  Loss: (0.0172) | Acc: (99.46%) (26862/27008)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 220 |  Loss: (0.0169) | Acc: (99.47%) (28137/28288)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 230 |  Loss: (0.0167) | Acc: (99.48%) (29413/29568)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 240 |  Loss: (0.0166) | Acc: (99.48%) (30689/30848)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 250 |  Loss: (0.0166) | Acc: (99.49%) (31964/32128)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 260 |  Loss: (0.0166) | Acc: (99.49%) (33239/33408)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 270 |  Loss: (0.0170) | Acc: (99.48%) (34509/34688)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 280 |  Loss: (0.0167) | Acc: (99.50%) (35787/35968)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 290 |  Loss: (0.0168) | Acc: (99.50%) (37060/37248)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 300 |  Loss: (0.0169) | Acc: (99.49%) (38332/38528)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 310 |  Loss: (0.0168) | Acc: (99.50%) (39608/39808)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 320 |  Loss: (0.0168) | Acc: (99.49%) (40878/41088)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 330 |  Loss: (0.0166) | Acc: (99.50%) (42155/42368)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 340 |  Loss: (0.0164) | Acc: (99.51%) (43432/43648)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 350 |  Loss: (0.0164) | Acc: (99.50%) (44705/44928)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 360 |  Loss: (0.0166) | Acc: (99.49%) (45974/46208)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 370 |  Loss: (0.0166) | Acc: (99.49%) (47248/47488)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 380 |  Loss: (0.0167) | Acc: (99.50%) (48523/48768)\n",
      "#TRAIN: Epoch: 149 | Batch_idx: 390 |  Loss: (0.0168) | Acc: (99.50%) (49748/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5624) | Acc: (89.09%) (8909/10000)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 0 |  Loss: (0.0132) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 10 |  Loss: (0.0157) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 20 |  Loss: (0.0162) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 30 |  Loss: (0.0153) | Acc: (99.47%) (3947/3968)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 40 |  Loss: (0.0161) | Acc: (99.49%) (5221/5248)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 50 |  Loss: (0.0163) | Acc: (99.51%) (6496/6528)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 60 |  Loss: (0.0160) | Acc: (99.51%) (7770/7808)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 70 |  Loss: (0.0169) | Acc: (99.45%) (9038/9088)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 80 |  Loss: (0.0166) | Acc: (99.45%) (10311/10368)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 90 |  Loss: (0.0158) | Acc: (99.50%) (11590/11648)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 100 |  Loss: (0.0153) | Acc: (99.54%) (12868/12928)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 110 |  Loss: (0.0148) | Acc: (99.55%) (14144/14208)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 120 |  Loss: (0.0147) | Acc: (99.55%) (15419/15488)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 130 |  Loss: (0.0143) | Acc: (99.57%) (16696/16768)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 140 |  Loss: (0.0143) | Acc: (99.56%) (17969/18048)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 150 |  Loss: (0.0146) | Acc: (99.57%) (19244/19328)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 160 |  Loss: (0.0141) | Acc: (99.59%) (20524/20608)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 170 |  Loss: (0.0140) | Acc: (99.60%) (21801/21888)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 180 |  Loss: (0.0141) | Acc: (99.60%) (23075/23168)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 190 |  Loss: (0.0145) | Acc: (99.58%) (24346/24448)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 200 |  Loss: (0.0148) | Acc: (99.57%) (25617/25728)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 210 |  Loss: (0.0148) | Acc: (99.56%) (26890/27008)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 220 |  Loss: (0.0148) | Acc: (99.57%) (28166/28288)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 230 |  Loss: (0.0148) | Acc: (99.57%) (29442/29568)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 240 |  Loss: (0.0148) | Acc: (99.57%) (30716/30848)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 250 |  Loss: (0.0147) | Acc: (99.57%) (31991/32128)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 260 |  Loss: (0.0147) | Acc: (99.57%) (33266/33408)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 270 |  Loss: (0.0149) | Acc: (99.58%) (34541/34688)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 280 |  Loss: (0.0147) | Acc: (99.59%) (35819/35968)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 290 |  Loss: (0.0146) | Acc: (99.58%) (37091/37248)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 300 |  Loss: (0.0146) | Acc: (99.58%) (38365/38528)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 310 |  Loss: (0.0145) | Acc: (99.58%) (39642/39808)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 320 |  Loss: (0.0145) | Acc: (99.58%) (40915/41088)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 330 |  Loss: (0.0148) | Acc: (99.57%) (42186/42368)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 340 |  Loss: (0.0148) | Acc: (99.56%) (43458/43648)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 350 |  Loss: (0.0148) | Acc: (99.56%) (44732/44928)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 360 |  Loss: (0.0150) | Acc: (99.56%) (46005/46208)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 370 |  Loss: (0.0152) | Acc: (99.56%) (47277/47488)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 380 |  Loss: (0.0151) | Acc: (99.56%) (48554/48768)\n",
      "#TRAIN: Epoch: 150 | Batch_idx: 390 |  Loss: (0.0153) | Acc: (99.55%) (49776/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5609) | Acc: (89.10%) (8910/10000)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 0 |  Loss: (0.0056) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 10 |  Loss: (0.0117) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 20 |  Loss: (0.0181) | Acc: (99.37%) (2671/2688)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 30 |  Loss: (0.0182) | Acc: (99.37%) (3943/3968)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 40 |  Loss: (0.0188) | Acc: (99.35%) (5214/5248)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 50 |  Loss: (0.0170) | Acc: (99.43%) (6491/6528)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 60 |  Loss: (0.0166) | Acc: (99.44%) (7764/7808)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 70 |  Loss: (0.0167) | Acc: (99.44%) (9037/9088)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 80 |  Loss: (0.0172) | Acc: (99.41%) (10307/10368)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 90 |  Loss: (0.0176) | Acc: (99.39%) (11577/11648)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 100 |  Loss: (0.0179) | Acc: (99.40%) (12850/12928)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 110 |  Loss: (0.0180) | Acc: (99.39%) (14122/14208)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 120 |  Loss: (0.0182) | Acc: (99.40%) (15395/15488)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 130 |  Loss: (0.0183) | Acc: (99.39%) (16666/16768)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 140 |  Loss: (0.0178) | Acc: (99.41%) (17941/18048)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 150 |  Loss: (0.0177) | Acc: (99.43%) (19217/19328)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 160 |  Loss: (0.0177) | Acc: (99.44%) (20492/20608)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 170 |  Loss: (0.0175) | Acc: (99.45%) (21767/21888)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 180 |  Loss: (0.0177) | Acc: (99.44%) (23039/23168)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 190 |  Loss: (0.0176) | Acc: (99.44%) (24312/24448)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 200 |  Loss: (0.0174) | Acc: (99.45%) (25587/25728)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 210 |  Loss: (0.0173) | Acc: (99.46%) (26862/27008)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 220 |  Loss: (0.0172) | Acc: (99.47%) (28138/28288)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.47%) (29412/29568)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 240 |  Loss: (0.0170) | Acc: (99.48%) (30688/30848)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 250 |  Loss: (0.0169) | Acc: (99.49%) (31964/32128)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 260 |  Loss: (0.0168) | Acc: (99.49%) (33239/33408)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 270 |  Loss: (0.0171) | Acc: (99.49%) (34511/34688)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 280 |  Loss: (0.0170) | Acc: (99.49%) (35786/35968)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 290 |  Loss: (0.0170) | Acc: (99.50%) (37061/37248)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 300 |  Loss: (0.0169) | Acc: (99.50%) (38335/38528)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 310 |  Loss: (0.0170) | Acc: (99.49%) (39605/39808)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.49%) (40878/41088)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 330 |  Loss: (0.0170) | Acc: (99.49%) (42150/42368)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 340 |  Loss: (0.0171) | Acc: (99.48%) (43420/43648)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 350 |  Loss: (0.0171) | Acc: (99.47%) (44691/44928)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 360 |  Loss: (0.0169) | Acc: (99.48%) (45966/46208)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 370 |  Loss: (0.0170) | Acc: (99.47%) (47238/47488)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 380 |  Loss: (0.0169) | Acc: (99.47%) (48511/48768)\n",
      "#TRAIN: Epoch: 151 | Batch_idx: 390 |  Loss: (0.0169) | Acc: (99.47%) (49736/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5582) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 0 |  Loss: (0.0138) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 10 |  Loss: (0.0183) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 20 |  Loss: (0.0145) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 30 |  Loss: (0.0179) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 40 |  Loss: (0.0163) | Acc: (99.58%) (5226/5248)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 50 |  Loss: (0.0150) | Acc: (99.59%) (6501/6528)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 60 |  Loss: (0.0150) | Acc: (99.60%) (7777/7808)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 70 |  Loss: (0.0155) | Acc: (99.60%) (9052/9088)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 80 |  Loss: (0.0162) | Acc: (99.59%) (10325/10368)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 90 |  Loss: (0.0162) | Acc: (99.57%) (11598/11648)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 100 |  Loss: (0.0167) | Acc: (99.54%) (12868/12928)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 110 |  Loss: (0.0170) | Acc: (99.52%) (14140/14208)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 120 |  Loss: (0.0168) | Acc: (99.53%) (15415/15488)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 130 |  Loss: (0.0171) | Acc: (99.52%) (16687/16768)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 140 |  Loss: (0.0172) | Acc: (99.51%) (17959/18048)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 150 |  Loss: (0.0174) | Acc: (99.50%) (19232/19328)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 160 |  Loss: (0.0173) | Acc: (99.51%) (20507/20608)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 170 |  Loss: (0.0179) | Acc: (99.49%) (21777/21888)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 180 |  Loss: (0.0177) | Acc: (99.49%) (23051/23168)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 190 |  Loss: (0.0179) | Acc: (99.49%) (24324/24448)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 200 |  Loss: (0.0178) | Acc: (99.50%) (25600/25728)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 210 |  Loss: (0.0177) | Acc: (99.51%) (26875/27008)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 220 |  Loss: (0.0173) | Acc: (99.53%) (28154/28288)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 230 |  Loss: (0.0171) | Acc: (99.53%) (29429/29568)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 240 |  Loss: (0.0171) | Acc: (99.53%) (30703/30848)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 250 |  Loss: (0.0172) | Acc: (99.52%) (31973/32128)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 260 |  Loss: (0.0173) | Acc: (99.52%) (33246/33408)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 270 |  Loss: (0.0173) | Acc: (99.51%) (34517/34688)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 280 |  Loss: (0.0171) | Acc: (99.51%) (35791/35968)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 290 |  Loss: (0.0170) | Acc: (99.52%) (37068/37248)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 300 |  Loss: (0.0170) | Acc: (99.52%) (38342/38528)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 310 |  Loss: (0.0171) | Acc: (99.51%) (39613/39808)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.51%) (40886/41088)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 330 |  Loss: (0.0168) | Acc: (99.51%) (42162/42368)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 340 |  Loss: (0.0168) | Acc: (99.51%) (43432/43648)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 350 |  Loss: (0.0169) | Acc: (99.50%) (44705/44928)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 360 |  Loss: (0.0168) | Acc: (99.51%) (45980/46208)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 370 |  Loss: (0.0170) | Acc: (99.50%) (47250/47488)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 380 |  Loss: (0.0169) | Acc: (99.50%) (48525/48768)\n",
      "#TRAIN: Epoch: 152 | Batch_idx: 390 |  Loss: (0.0170) | Acc: (99.50%) (49751/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5626) | Acc: (89.13%) (8913/10000)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 0 |  Loss: (0.0046) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 10 |  Loss: (0.0169) | Acc: (99.43%) (1400/1408)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 20 |  Loss: (0.0203) | Acc: (99.44%) (2673/2688)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 30 |  Loss: (0.0181) | Acc: (99.45%) (3946/3968)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 40 |  Loss: (0.0186) | Acc: (99.43%) (5218/5248)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 50 |  Loss: (0.0164) | Acc: (99.49%) (6495/6528)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 60 |  Loss: (0.0161) | Acc: (99.49%) (7768/7808)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 70 |  Loss: (0.0149) | Acc: (99.55%) (9047/9088)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 80 |  Loss: (0.0149) | Acc: (99.57%) (10323/10368)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 90 |  Loss: (0.0145) | Acc: (99.60%) (11601/11648)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 100 |  Loss: (0.0153) | Acc: (99.55%) (12870/12928)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 110 |  Loss: (0.0157) | Acc: (99.53%) (14141/14208)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 120 |  Loss: (0.0159) | Acc: (99.51%) (15412/15488)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 130 |  Loss: (0.0159) | Acc: (99.51%) (16686/16768)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 140 |  Loss: (0.0159) | Acc: (99.51%) (17960/18048)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 150 |  Loss: (0.0159) | Acc: (99.52%) (19235/19328)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 160 |  Loss: (0.0160) | Acc: (99.52%) (20509/20608)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 170 |  Loss: (0.0164) | Acc: (99.48%) (21775/21888)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 180 |  Loss: (0.0164) | Acc: (99.48%) (23048/23168)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 190 |  Loss: (0.0163) | Acc: (99.49%) (24323/24448)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 200 |  Loss: (0.0162) | Acc: (99.49%) (25597/25728)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 210 |  Loss: (0.0164) | Acc: (99.49%) (26870/27008)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 220 |  Loss: (0.0161) | Acc: (99.50%) (28147/28288)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 230 |  Loss: (0.0159) | Acc: (99.51%) (29422/29568)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 240 |  Loss: (0.0159) | Acc: (99.51%) (30697/30848)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 250 |  Loss: (0.0156) | Acc: (99.51%) (31972/32128)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 260 |  Loss: (0.0156) | Acc: (99.52%) (33247/33408)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 270 |  Loss: (0.0155) | Acc: (99.52%) (34522/34688)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 280 |  Loss: (0.0155) | Acc: (99.52%) (35797/35968)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 290 |  Loss: (0.0155) | Acc: (99.52%) (37071/37248)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 300 |  Loss: (0.0154) | Acc: (99.53%) (38346/38528)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 310 |  Loss: (0.0155) | Acc: (99.52%) (39618/39808)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 320 |  Loss: (0.0156) | Acc: (99.52%) (40889/41088)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 330 |  Loss: (0.0157) | Acc: (99.51%) (42160/42368)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 340 |  Loss: (0.0158) | Acc: (99.51%) (43434/43648)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 350 |  Loss: (0.0160) | Acc: (99.50%) (44704/44928)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 360 |  Loss: (0.0160) | Acc: (99.50%) (45977/46208)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 370 |  Loss: (0.0159) | Acc: (99.51%) (47253/47488)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 380 |  Loss: (0.0159) | Acc: (99.51%) (48528/48768)\n",
      "#TRAIN: Epoch: 153 | Batch_idx: 390 |  Loss: (0.0158) | Acc: (99.52%) (49758/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5643) | Acc: (89.11%) (8911/10000)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 0 |  Loss: (0.0083) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 10 |  Loss: (0.0147) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 20 |  Loss: (0.0157) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 30 |  Loss: (0.0167) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 40 |  Loss: (0.0162) | Acc: (99.54%) (5224/5248)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 50 |  Loss: (0.0171) | Acc: (99.49%) (6495/6528)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 60 |  Loss: (0.0167) | Acc: (99.50%) (7769/7808)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 70 |  Loss: (0.0169) | Acc: (99.45%) (9038/9088)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 80 |  Loss: (0.0166) | Acc: (99.47%) (10313/10368)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 90 |  Loss: (0.0160) | Acc: (99.50%) (11590/11648)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 100 |  Loss: (0.0156) | Acc: (99.53%) (12867/12928)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 110 |  Loss: (0.0154) | Acc: (99.54%) (14143/14208)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 120 |  Loss: (0.0149) | Acc: (99.55%) (15419/15488)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 130 |  Loss: (0.0151) | Acc: (99.56%) (16694/16768)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 140 |  Loss: (0.0153) | Acc: (99.56%) (17968/18048)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 150 |  Loss: (0.0154) | Acc: (99.55%) (19241/19328)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 160 |  Loss: (0.0154) | Acc: (99.53%) (20512/20608)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 170 |  Loss: (0.0153) | Acc: (99.53%) (21786/21888)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 180 |  Loss: (0.0154) | Acc: (99.52%) (23057/23168)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 190 |  Loss: (0.0150) | Acc: (99.54%) (24336/24448)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 200 |  Loss: (0.0153) | Acc: (99.53%) (25608/25728)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 210 |  Loss: (0.0154) | Acc: (99.53%) (26882/27008)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 220 |  Loss: (0.0156) | Acc: (99.52%) (28152/28288)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 230 |  Loss: (0.0156) | Acc: (99.52%) (29427/29568)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 240 |  Loss: (0.0153) | Acc: (99.54%) (30705/30848)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 250 |  Loss: (0.0155) | Acc: (99.53%) (31978/32128)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 260 |  Loss: (0.0156) | Acc: (99.53%) (33251/33408)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 270 |  Loss: (0.0153) | Acc: (99.54%) (34527/34688)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 280 |  Loss: (0.0153) | Acc: (99.54%) (35801/35968)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 290 |  Loss: (0.0151) | Acc: (99.54%) (37078/37248)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 300 |  Loss: (0.0152) | Acc: (99.54%) (38351/38528)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 310 |  Loss: (0.0150) | Acc: (99.55%) (39629/39808)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 320 |  Loss: (0.0151) | Acc: (99.55%) (40902/41088)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 330 |  Loss: (0.0150) | Acc: (99.55%) (42177/42368)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 340 |  Loss: (0.0149) | Acc: (99.55%) (43452/43648)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 350 |  Loss: (0.0148) | Acc: (99.55%) (44726/44928)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 360 |  Loss: (0.0148) | Acc: (99.55%) (46000/46208)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 370 |  Loss: (0.0148) | Acc: (99.55%) (47275/47488)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 380 |  Loss: (0.0147) | Acc: (99.55%) (48548/48768)\n",
      "#TRAIN: Epoch: 154 | Batch_idx: 390 |  Loss: (0.0148) | Acc: (99.54%) (49771/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5641) | Acc: (89.07%) (8907/10000)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 0 |  Loss: (0.0090) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 10 |  Loss: (0.0106) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 20 |  Loss: (0.0131) | Acc: (99.55%) (2676/2688)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 30 |  Loss: (0.0143) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 40 |  Loss: (0.0148) | Acc: (99.54%) (5224/5248)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 50 |  Loss: (0.0143) | Acc: (99.59%) (6501/6528)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 60 |  Loss: (0.0135) | Acc: (99.63%) (7779/7808)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 70 |  Loss: (0.0137) | Acc: (99.63%) (9054/9088)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 80 |  Loss: (0.0134) | Acc: (99.63%) (10330/10368)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 90 |  Loss: (0.0146) | Acc: (99.60%) (11601/11648)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 100 |  Loss: (0.0140) | Acc: (99.62%) (12879/12928)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 110 |  Loss: (0.0145) | Acc: (99.60%) (14151/14208)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 120 |  Loss: (0.0142) | Acc: (99.61%) (15427/15488)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 130 |  Loss: (0.0140) | Acc: (99.62%) (16704/16768)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 140 |  Loss: (0.0137) | Acc: (99.62%) (17980/18048)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 150 |  Loss: (0.0145) | Acc: (99.59%) (19248/19328)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 160 |  Loss: (0.0143) | Acc: (99.59%) (20524/20608)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 170 |  Loss: (0.0141) | Acc: (99.59%) (21799/21888)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 180 |  Loss: (0.0140) | Acc: (99.59%) (23073/23168)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 190 |  Loss: (0.0139) | Acc: (99.60%) (24349/24448)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 200 |  Loss: (0.0143) | Acc: (99.59%) (25622/25728)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 210 |  Loss: (0.0146) | Acc: (99.57%) (26893/27008)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 220 |  Loss: (0.0147) | Acc: (99.57%) (28166/28288)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 230 |  Loss: (0.0147) | Acc: (99.56%) (29439/29568)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 240 |  Loss: (0.0151) | Acc: (99.55%) (30708/30848)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 250 |  Loss: (0.0150) | Acc: (99.55%) (31984/32128)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 260 |  Loss: (0.0149) | Acc: (99.55%) (33258/33408)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 270 |  Loss: (0.0149) | Acc: (99.56%) (34534/34688)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 280 |  Loss: (0.0150) | Acc: (99.56%) (35809/35968)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 290 |  Loss: (0.0150) | Acc: (99.55%) (37081/37248)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 300 |  Loss: (0.0149) | Acc: (99.56%) (38357/38528)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 310 |  Loss: (0.0147) | Acc: (99.57%) (39635/39808)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 320 |  Loss: (0.0147) | Acc: (99.57%) (40910/41088)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 330 |  Loss: (0.0146) | Acc: (99.57%) (42185/42368)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 340 |  Loss: (0.0145) | Acc: (99.57%) (43461/43648)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 350 |  Loss: (0.0146) | Acc: (99.57%) (44736/44928)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 360 |  Loss: (0.0145) | Acc: (99.57%) (46011/46208)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 370 |  Loss: (0.0148) | Acc: (99.56%) (47277/47488)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 380 |  Loss: (0.0146) | Acc: (99.56%) (48554/48768)\n",
      "#TRAIN: Epoch: 155 | Batch_idx: 390 |  Loss: (0.0146) | Acc: (99.56%) (49781/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5614) | Acc: (89.07%) (8907/10000)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 0 |  Loss: (0.0059) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 10 |  Loss: (0.0110) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 20 |  Loss: (0.0135) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 30 |  Loss: (0.0132) | Acc: (99.62%) (3953/3968)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 40 |  Loss: (0.0137) | Acc: (99.60%) (5227/5248)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 50 |  Loss: (0.0132) | Acc: (99.62%) (6503/6528)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 60 |  Loss: (0.0138) | Acc: (99.58%) (7775/7808)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 70 |  Loss: (0.0151) | Acc: (99.58%) (9050/9088)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 80 |  Loss: (0.0159) | Acc: (99.55%) (10321/10368)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 90 |  Loss: (0.0161) | Acc: (99.51%) (11591/11648)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 100 |  Loss: (0.0157) | Acc: (99.52%) (12866/12928)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 110 |  Loss: (0.0160) | Acc: (99.49%) (14136/14208)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 120 |  Loss: (0.0160) | Acc: (99.47%) (15406/15488)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 130 |  Loss: (0.0163) | Acc: (99.48%) (16681/16768)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 140 |  Loss: (0.0163) | Acc: (99.49%) (17956/18048)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 150 |  Loss: (0.0164) | Acc: (99.49%) (19230/19328)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 160 |  Loss: (0.0161) | Acc: (99.50%) (20504/20608)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 170 |  Loss: (0.0164) | Acc: (99.49%) (21777/21888)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 180 |  Loss: (0.0165) | Acc: (99.49%) (23051/23168)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 190 |  Loss: (0.0162) | Acc: (99.51%) (24327/24448)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 200 |  Loss: (0.0162) | Acc: (99.51%) (25602/25728)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 210 |  Loss: (0.0166) | Acc: (99.51%) (26876/27008)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 220 |  Loss: (0.0164) | Acc: (99.52%) (28151/28288)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 230 |  Loss: (0.0162) | Acc: (99.53%) (29428/29568)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 240 |  Loss: (0.0161) | Acc: (99.52%) (30701/30848)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 250 |  Loss: (0.0160) | Acc: (99.53%) (31977/32128)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 260 |  Loss: (0.0158) | Acc: (99.54%) (33254/33408)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 270 |  Loss: (0.0158) | Acc: (99.53%) (34526/34688)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 280 |  Loss: (0.0161) | Acc: (99.51%) (35792/35968)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 290 |  Loss: (0.0160) | Acc: (99.51%) (37067/37248)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 300 |  Loss: (0.0159) | Acc: (99.52%) (38343/38528)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 310 |  Loss: (0.0158) | Acc: (99.52%) (39615/39808)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 320 |  Loss: (0.0158) | Acc: (99.52%) (40889/41088)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 330 |  Loss: (0.0158) | Acc: (99.52%) (42164/42368)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 340 |  Loss: (0.0158) | Acc: (99.51%) (43436/43648)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 350 |  Loss: (0.0157) | Acc: (99.52%) (44712/44928)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 360 |  Loss: (0.0157) | Acc: (99.52%) (45986/46208)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 370 |  Loss: (0.0157) | Acc: (99.52%) (47261/47488)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 380 |  Loss: (0.0159) | Acc: (99.52%) (48533/48768)\n",
      "#TRAIN: Epoch: 156 | Batch_idx: 390 |  Loss: (0.0159) | Acc: (99.52%) (49758/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5645) | Acc: (89.11%) (8911/10000)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 0 |  Loss: (0.0128) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 10 |  Loss: (0.0134) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 20 |  Loss: (0.0151) | Acc: (99.55%) (2676/2688)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 30 |  Loss: (0.0151) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 40 |  Loss: (0.0147) | Acc: (99.47%) (5220/5248)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 50 |  Loss: (0.0153) | Acc: (99.42%) (6490/6528)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 60 |  Loss: (0.0157) | Acc: (99.41%) (7762/7808)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 70 |  Loss: (0.0151) | Acc: (99.46%) (9039/9088)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 80 |  Loss: (0.0156) | Acc: (99.45%) (10311/10368)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 90 |  Loss: (0.0159) | Acc: (99.43%) (11582/11648)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 100 |  Loss: (0.0163) | Acc: (99.44%) (12856/12928)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 110 |  Loss: (0.0165) | Acc: (99.46%) (14131/14208)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 120 |  Loss: (0.0165) | Acc: (99.47%) (15406/15488)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 130 |  Loss: (0.0164) | Acc: (99.48%) (16681/16768)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 140 |  Loss: (0.0165) | Acc: (99.48%) (17955/18048)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 150 |  Loss: (0.0159) | Acc: (99.51%) (19233/19328)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 160 |  Loss: (0.0161) | Acc: (99.49%) (20502/20608)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 170 |  Loss: (0.0162) | Acc: (99.48%) (21775/21888)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 180 |  Loss: (0.0159) | Acc: (99.49%) (23051/23168)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 190 |  Loss: (0.0158) | Acc: (99.51%) (24327/24448)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 200 |  Loss: (0.0161) | Acc: (99.50%) (25599/25728)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 210 |  Loss: (0.0163) | Acc: (99.49%) (26869/27008)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 220 |  Loss: (0.0162) | Acc: (99.49%) (28144/28288)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 230 |  Loss: (0.0161) | Acc: (99.50%) (29419/29568)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 240 |  Loss: (0.0163) | Acc: (99.49%) (30692/30848)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 250 |  Loss: (0.0163) | Acc: (99.50%) (31967/32128)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 260 |  Loss: (0.0164) | Acc: (99.50%) (33240/33408)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 270 |  Loss: (0.0165) | Acc: (99.49%) (34512/34688)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 280 |  Loss: (0.0164) | Acc: (99.50%) (35787/35968)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 290 |  Loss: (0.0164) | Acc: (99.50%) (37060/37248)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 300 |  Loss: (0.0164) | Acc: (99.49%) (38333/38528)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 310 |  Loss: (0.0162) | Acc: (99.50%) (39610/39808)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 320 |  Loss: (0.0162) | Acc: (99.50%) (40883/41088)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 330 |  Loss: (0.0162) | Acc: (99.50%) (42157/42368)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 340 |  Loss: (0.0163) | Acc: (99.50%) (43428/43648)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 350 |  Loss: (0.0162) | Acc: (99.51%) (44706/44928)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 360 |  Loss: (0.0161) | Acc: (99.51%) (45983/46208)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 370 |  Loss: (0.0161) | Acc: (99.51%) (47257/47488)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 380 |  Loss: (0.0162) | Acc: (99.51%) (48528/48768)\n",
      "#TRAIN: Epoch: 157 | Batch_idx: 390 |  Loss: (0.0162) | Acc: (99.51%) (49753/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5668) | Acc: (88.94%) (8894/10000)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 0 |  Loss: (0.0075) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 10 |  Loss: (0.0205) | Acc: (99.36%) (1399/1408)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 20 |  Loss: (0.0197) | Acc: (99.33%) (2670/2688)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 30 |  Loss: (0.0172) | Acc: (99.42%) (3945/3968)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 40 |  Loss: (0.0150) | Acc: (99.52%) (5223/5248)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 50 |  Loss: (0.0163) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 60 |  Loss: (0.0151) | Acc: (99.53%) (7771/7808)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 70 |  Loss: (0.0150) | Acc: (99.53%) (9045/9088)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 80 |  Loss: (0.0152) | Acc: (99.53%) (10319/10368)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 90 |  Loss: (0.0150) | Acc: (99.54%) (11594/11648)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 100 |  Loss: (0.0156) | Acc: (99.50%) (12863/12928)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 110 |  Loss: (0.0157) | Acc: (99.51%) (14138/14208)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 120 |  Loss: (0.0152) | Acc: (99.52%) (15413/15488)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 130 |  Loss: (0.0155) | Acc: (99.51%) (16685/16768)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 140 |  Loss: (0.0155) | Acc: (99.51%) (17959/18048)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 150 |  Loss: (0.0154) | Acc: (99.50%) (19232/19328)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 160 |  Loss: (0.0154) | Acc: (99.50%) (20505/20608)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 170 |  Loss: (0.0151) | Acc: (99.51%) (21781/21888)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 180 |  Loss: (0.0153) | Acc: (99.51%) (23055/23168)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 190 |  Loss: (0.0153) | Acc: (99.51%) (24327/24448)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 200 |  Loss: (0.0150) | Acc: (99.52%) (25605/25728)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 210 |  Loss: (0.0149) | Acc: (99.52%) (26878/27008)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 220 |  Loss: (0.0149) | Acc: (99.52%) (28153/28288)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 230 |  Loss: (0.0151) | Acc: (99.52%) (29426/29568)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 240 |  Loss: (0.0152) | Acc: (99.52%) (30701/30848)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 250 |  Loss: (0.0151) | Acc: (99.53%) (31978/32128)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 260 |  Loss: (0.0153) | Acc: (99.52%) (33248/33408)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 270 |  Loss: (0.0156) | Acc: (99.51%) (34518/34688)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 280 |  Loss: (0.0156) | Acc: (99.51%) (35793/35968)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 290 |  Loss: (0.0154) | Acc: (99.52%) (37069/37248)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 300 |  Loss: (0.0153) | Acc: (99.52%) (38343/38528)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 310 |  Loss: (0.0154) | Acc: (99.52%) (39616/39808)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 320 |  Loss: (0.0156) | Acc: (99.51%) (40888/41088)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 330 |  Loss: (0.0156) | Acc: (99.51%) (42161/42368)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 340 |  Loss: (0.0155) | Acc: (99.51%) (43435/43648)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 350 |  Loss: (0.0154) | Acc: (99.52%) (44712/44928)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 360 |  Loss: (0.0155) | Acc: (99.52%) (45986/46208)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 370 |  Loss: (0.0153) | Acc: (99.53%) (47263/47488)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 380 |  Loss: (0.0154) | Acc: (99.53%) (48537/48768)\n",
      "#TRAIN: Epoch: 158 | Batch_idx: 390 |  Loss: (0.0154) | Acc: (99.53%) (49764/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5673) | Acc: (89.07%) (8907/10000)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 0 |  Loss: (0.0090) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 10 |  Loss: (0.0118) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 20 |  Loss: (0.0147) | Acc: (99.55%) (2676/2688)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 30 |  Loss: (0.0172) | Acc: (99.50%) (3948/3968)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 40 |  Loss: (0.0184) | Acc: (99.43%) (5218/5248)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 50 |  Loss: (0.0177) | Acc: (99.46%) (6493/6528)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 60 |  Loss: (0.0166) | Acc: (99.50%) (7769/7808)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 70 |  Loss: (0.0161) | Acc: (99.54%) (9046/9088)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 80 |  Loss: (0.0160) | Acc: (99.53%) (10319/10368)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 90 |  Loss: (0.0159) | Acc: (99.49%) (11589/11648)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 100 |  Loss: (0.0159) | Acc: (99.52%) (12866/12928)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 110 |  Loss: (0.0157) | Acc: (99.52%) (14140/14208)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 120 |  Loss: (0.0157) | Acc: (99.51%) (15412/15488)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 130 |  Loss: (0.0156) | Acc: (99.52%) (16687/16768)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 140 |  Loss: (0.0153) | Acc: (99.53%) (17964/18048)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 150 |  Loss: (0.0154) | Acc: (99.53%) (19238/19328)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 160 |  Loss: (0.0154) | Acc: (99.55%) (20515/20608)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 170 |  Loss: (0.0155) | Acc: (99.54%) (21788/21888)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 180 |  Loss: (0.0156) | Acc: (99.54%) (23061/23168)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 190 |  Loss: (0.0154) | Acc: (99.54%) (24336/24448)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 200 |  Loss: (0.0155) | Acc: (99.53%) (25608/25728)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 210 |  Loss: (0.0155) | Acc: (99.53%) (26881/27008)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 220 |  Loss: (0.0156) | Acc: (99.51%) (28150/28288)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 230 |  Loss: (0.0155) | Acc: (99.52%) (29425/29568)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 240 |  Loss: (0.0157) | Acc: (99.51%) (30698/30848)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 250 |  Loss: (0.0157) | Acc: (99.51%) (31972/32128)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 260 |  Loss: (0.0155) | Acc: (99.52%) (33248/33408)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 270 |  Loss: (0.0157) | Acc: (99.52%) (34520/34688)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 280 |  Loss: (0.0157) | Acc: (99.52%) (35796/35968)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 290 |  Loss: (0.0156) | Acc: (99.52%) (37071/37248)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 300 |  Loss: (0.0156) | Acc: (99.53%) (38345/38528)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 310 |  Loss: (0.0157) | Acc: (99.53%) (39619/39808)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 320 |  Loss: (0.0156) | Acc: (99.53%) (40894/41088)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 330 |  Loss: (0.0155) | Acc: (99.53%) (42168/42368)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 340 |  Loss: (0.0155) | Acc: (99.53%) (43442/43648)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 350 |  Loss: (0.0155) | Acc: (99.53%) (44717/44928)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 360 |  Loss: (0.0153) | Acc: (99.54%) (45996/46208)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 370 |  Loss: (0.0154) | Acc: (99.53%) (47267/47488)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 380 |  Loss: (0.0156) | Acc: (99.53%) (48538/48768)\n",
      "#TRAIN: Epoch: 159 | Batch_idx: 390 |  Loss: (0.0156) | Acc: (99.52%) (49762/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5604) | Acc: (89.06%) (8906/10000)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 0 |  Loss: (0.0398) | Acc: (97.66%) (125/128)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 10 |  Loss: (0.0131) | Acc: (99.57%) (1402/1408)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 20 |  Loss: (0.0156) | Acc: (99.59%) (2677/2688)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 30 |  Loss: (0.0164) | Acc: (99.57%) (3951/3968)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 40 |  Loss: (0.0167) | Acc: (99.54%) (5224/5248)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 50 |  Loss: (0.0165) | Acc: (99.54%) (6498/6528)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 60 |  Loss: (0.0157) | Acc: (99.56%) (7774/7808)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 70 |  Loss: (0.0159) | Acc: (99.54%) (9046/9088)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 80 |  Loss: (0.0163) | Acc: (99.53%) (10319/10368)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 90 |  Loss: (0.0163) | Acc: (99.51%) (11591/11648)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 100 |  Loss: (0.0164) | Acc: (99.48%) (12861/12928)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 110 |  Loss: (0.0165) | Acc: (99.48%) (14134/14208)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 120 |  Loss: (0.0160) | Acc: (99.48%) (15408/15488)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 130 |  Loss: (0.0162) | Acc: (99.48%) (16680/16768)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 140 |  Loss: (0.0158) | Acc: (99.50%) (17957/18048)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 150 |  Loss: (0.0160) | Acc: (99.48%) (19228/19328)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 160 |  Loss: (0.0157) | Acc: (99.50%) (20504/20608)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 170 |  Loss: (0.0154) | Acc: (99.51%) (21781/21888)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 180 |  Loss: (0.0151) | Acc: (99.53%) (23059/23168)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 190 |  Loss: (0.0153) | Acc: (99.53%) (24334/24448)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 200 |  Loss: (0.0159) | Acc: (99.53%) (25606/25728)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 210 |  Loss: (0.0161) | Acc: (99.51%) (26877/27008)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 220 |  Loss: (0.0165) | Acc: (99.49%) (28144/28288)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 230 |  Loss: (0.0166) | Acc: (99.48%) (29413/29568)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 240 |  Loss: (0.0166) | Acc: (99.48%) (30688/30848)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 250 |  Loss: (0.0164) | Acc: (99.49%) (31965/32128)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 260 |  Loss: (0.0163) | Acc: (99.50%) (33240/33408)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 270 |  Loss: (0.0162) | Acc: (99.50%) (34514/34688)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 280 |  Loss: (0.0160) | Acc: (99.51%) (35791/35968)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 290 |  Loss: (0.0157) | Acc: (99.52%) (37070/37248)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 300 |  Loss: (0.0158) | Acc: (99.52%) (38343/38528)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 310 |  Loss: (0.0158) | Acc: (99.52%) (39615/39808)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 320 |  Loss: (0.0159) | Acc: (99.52%) (40890/41088)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 330 |  Loss: (0.0159) | Acc: (99.52%) (42165/42368)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 340 |  Loss: (0.0160) | Acc: (99.53%) (43441/43648)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 350 |  Loss: (0.0159) | Acc: (99.52%) (44714/44928)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 360 |  Loss: (0.0160) | Acc: (99.52%) (45984/46208)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 370 |  Loss: (0.0160) | Acc: (99.52%) (47260/47488)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 380 |  Loss: (0.0158) | Acc: (99.52%) (48536/48768)\n",
      "#TRAIN: Epoch: 160 | Batch_idx: 390 |  Loss: (0.0161) | Acc: (99.52%) (49760/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5690) | Acc: (89.11%) (8911/10000)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 0 |  Loss: (0.0088) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 10 |  Loss: (0.0106) | Acc: (99.79%) (1405/1408)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 20 |  Loss: (0.0154) | Acc: (99.70%) (2680/2688)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 30 |  Loss: (0.0170) | Acc: (99.57%) (3951/3968)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 40 |  Loss: (0.0172) | Acc: (99.54%) (5224/5248)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 50 |  Loss: (0.0155) | Acc: (99.57%) (6500/6528)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 60 |  Loss: (0.0165) | Acc: (99.51%) (7770/7808)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 70 |  Loss: (0.0177) | Acc: (99.49%) (9042/9088)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 80 |  Loss: (0.0167) | Acc: (99.56%) (10322/10368)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 90 |  Loss: (0.0162) | Acc: (99.58%) (11599/11648)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 100 |  Loss: (0.0167) | Acc: (99.53%) (12867/12928)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 110 |  Loss: (0.0166) | Acc: (99.53%) (14141/14208)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 120 |  Loss: (0.0165) | Acc: (99.52%) (15414/15488)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 130 |  Loss: (0.0167) | Acc: (99.52%) (16687/16768)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 140 |  Loss: (0.0162) | Acc: (99.53%) (17964/18048)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 150 |  Loss: (0.0164) | Acc: (99.53%) (19238/19328)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 160 |  Loss: (0.0160) | Acc: (99.54%) (20514/20608)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 170 |  Loss: (0.0159) | Acc: (99.55%) (21789/21888)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 180 |  Loss: (0.0157) | Acc: (99.54%) (23062/23168)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 190 |  Loss: (0.0154) | Acc: (99.55%) (24337/24448)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 200 |  Loss: (0.0152) | Acc: (99.56%) (25614/25728)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 210 |  Loss: (0.0152) | Acc: (99.56%) (26889/27008)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 220 |  Loss: (0.0150) | Acc: (99.57%) (28167/28288)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 230 |  Loss: (0.0149) | Acc: (99.57%) (29442/29568)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 240 |  Loss: (0.0150) | Acc: (99.56%) (30712/30848)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 250 |  Loss: (0.0150) | Acc: (99.56%) (31987/32128)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 260 |  Loss: (0.0155) | Acc: (99.54%) (33254/33408)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 270 |  Loss: (0.0156) | Acc: (99.53%) (34526/34688)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 280 |  Loss: (0.0154) | Acc: (99.53%) (35800/35968)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 290 |  Loss: (0.0154) | Acc: (99.52%) (37068/37248)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 300 |  Loss: (0.0155) | Acc: (99.52%) (38342/38528)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 310 |  Loss: (0.0153) | Acc: (99.52%) (39617/39808)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 320 |  Loss: (0.0155) | Acc: (99.52%) (40892/41088)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 330 |  Loss: (0.0154) | Acc: (99.52%) (42166/42368)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 340 |  Loss: (0.0154) | Acc: (99.53%) (43441/43648)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 350 |  Loss: (0.0154) | Acc: (99.53%) (44716/44928)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 360 |  Loss: (0.0154) | Acc: (99.53%) (45990/46208)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 370 |  Loss: (0.0155) | Acc: (99.52%) (47259/47488)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 380 |  Loss: (0.0154) | Acc: (99.52%) (48535/48768)\n",
      "#TRAIN: Epoch: 161 | Batch_idx: 390 |  Loss: (0.0153) | Acc: (99.53%) (49765/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5650) | Acc: (88.97%) (8897/10000)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 0 |  Loss: (0.0080) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 10 |  Loss: (0.0188) | Acc: (99.50%) (1401/1408)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 20 |  Loss: (0.0166) | Acc: (99.52%) (2675/2688)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 30 |  Loss: (0.0156) | Acc: (99.52%) (3949/3968)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 40 |  Loss: (0.0152) | Acc: (99.54%) (5224/5248)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 50 |  Loss: (0.0185) | Acc: (99.48%) (6494/6528)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 60 |  Loss: (0.0173) | Acc: (99.53%) (7771/7808)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 70 |  Loss: (0.0172) | Acc: (99.53%) (9045/9088)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 80 |  Loss: (0.0173) | Acc: (99.50%) (10316/10368)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 90 |  Loss: (0.0166) | Acc: (99.51%) (11591/11648)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 100 |  Loss: (0.0166) | Acc: (99.50%) (12863/12928)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 110 |  Loss: (0.0169) | Acc: (99.48%) (14134/14208)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 120 |  Loss: (0.0165) | Acc: (99.50%) (15411/15488)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 130 |  Loss: (0.0164) | Acc: (99.48%) (16681/16768)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 140 |  Loss: (0.0162) | Acc: (99.47%) (17952/18048)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 150 |  Loss: (0.0162) | Acc: (99.46%) (19224/19328)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 160 |  Loss: (0.0164) | Acc: (99.44%) (20493/20608)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 170 |  Loss: (0.0164) | Acc: (99.46%) (21770/21888)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 180 |  Loss: (0.0162) | Acc: (99.47%) (23045/23168)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 190 |  Loss: (0.0163) | Acc: (99.46%) (24315/24448)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 200 |  Loss: (0.0160) | Acc: (99.48%) (25593/25728)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 210 |  Loss: (0.0159) | Acc: (99.47%) (26864/27008)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 220 |  Loss: (0.0158) | Acc: (99.47%) (28138/28288)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 230 |  Loss: (0.0157) | Acc: (99.48%) (29414/29568)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 240 |  Loss: (0.0156) | Acc: (99.48%) (30687/30848)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 250 |  Loss: (0.0155) | Acc: (99.48%) (31962/32128)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 260 |  Loss: (0.0153) | Acc: (99.49%) (33237/33408)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 270 |  Loss: (0.0152) | Acc: (99.50%) (34513/34688)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 280 |  Loss: (0.0150) | Acc: (99.51%) (35792/35968)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 290 |  Loss: (0.0150) | Acc: (99.51%) (37066/37248)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 300 |  Loss: (0.0150) | Acc: (99.50%) (38337/38528)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 310 |  Loss: (0.0153) | Acc: (99.49%) (39606/39808)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 320 |  Loss: (0.0153) | Acc: (99.49%) (40880/41088)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 330 |  Loss: (0.0152) | Acc: (99.50%) (42157/42368)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 340 |  Loss: (0.0151) | Acc: (99.50%) (43430/43648)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 350 |  Loss: (0.0151) | Acc: (99.50%) (44705/44928)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 360 |  Loss: (0.0150) | Acc: (99.51%) (45980/46208)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 370 |  Loss: (0.0152) | Acc: (99.51%) (47254/47488)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 380 |  Loss: (0.0152) | Acc: (99.51%) (48529/48768)\n",
      "#TRAIN: Epoch: 162 | Batch_idx: 390 |  Loss: (0.0153) | Acc: (99.51%) (49753/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5620) | Acc: (89.04%) (8904/10000)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 0 |  Loss: (0.0266) | Acc: (99.22%) (127/128)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 10 |  Loss: (0.0138) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 20 |  Loss: (0.0156) | Acc: (99.48%) (2674/2688)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 30 |  Loss: (0.0164) | Acc: (99.47%) (3947/3968)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 40 |  Loss: (0.0140) | Acc: (99.56%) (5225/5248)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 50 |  Loss: (0.0142) | Acc: (99.57%) (6500/6528)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 60 |  Loss: (0.0145) | Acc: (99.56%) (7774/7808)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 70 |  Loss: (0.0142) | Acc: (99.54%) (9046/9088)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 80 |  Loss: (0.0144) | Acc: (99.55%) (10321/10368)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 90 |  Loss: (0.0142) | Acc: (99.57%) (11598/11648)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 100 |  Loss: (0.0151) | Acc: (99.53%) (12867/12928)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 110 |  Loss: (0.0149) | Acc: (99.54%) (14142/14208)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 120 |  Loss: (0.0151) | Acc: (99.54%) (15417/15488)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 130 |  Loss: (0.0151) | Acc: (99.56%) (16694/16768)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 140 |  Loss: (0.0146) | Acc: (99.57%) (17971/18048)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 150 |  Loss: (0.0151) | Acc: (99.56%) (19242/19328)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 160 |  Loss: (0.0160) | Acc: (99.51%) (20508/20608)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 170 |  Loss: (0.0157) | Acc: (99.52%) (21782/21888)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 180 |  Loss: (0.0160) | Acc: (99.49%) (23049/23168)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 190 |  Loss: (0.0160) | Acc: (99.48%) (24320/24448)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 200 |  Loss: (0.0162) | Acc: (99.47%) (25592/25728)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 210 |  Loss: (0.0164) | Acc: (99.47%) (26864/27008)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 220 |  Loss: (0.0164) | Acc: (99.47%) (28138/28288)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 230 |  Loss: (0.0170) | Acc: (99.46%) (29407/29568)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 240 |  Loss: (0.0168) | Acc: (99.46%) (30681/30848)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 250 |  Loss: (0.0168) | Acc: (99.46%) (31955/32128)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 260 |  Loss: (0.0167) | Acc: (99.47%) (33230/33408)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 270 |  Loss: (0.0168) | Acc: (99.47%) (34504/34688)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 280 |  Loss: (0.0166) | Acc: (99.48%) (35780/35968)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 290 |  Loss: (0.0166) | Acc: (99.48%) (37053/37248)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 300 |  Loss: (0.0166) | Acc: (99.47%) (38325/38528)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 310 |  Loss: (0.0165) | Acc: (99.48%) (39600/39808)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 320 |  Loss: (0.0165) | Acc: (99.48%) (40873/41088)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 330 |  Loss: (0.0165) | Acc: (99.47%) (42144/42368)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 340 |  Loss: (0.0164) | Acc: (99.47%) (43418/43648)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 350 |  Loss: (0.0163) | Acc: (99.47%) (44691/44928)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 360 |  Loss: (0.0165) | Acc: (99.47%) (45965/46208)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 370 |  Loss: (0.0163) | Acc: (99.48%) (47242/47488)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 380 |  Loss: (0.0163) | Acc: (99.48%) (48516/48768)\n",
      "#TRAIN: Epoch: 163 | Batch_idx: 390 |  Loss: (0.0164) | Acc: (99.48%) (49741/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5629) | Acc: (89.17%) (8917/10000)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 0 |  Loss: (0.0095) | Acc: (100.00%) (128/128)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 10 |  Loss: (0.0134) | Acc: (99.64%) (1403/1408)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 20 |  Loss: (0.0127) | Acc: (99.70%) (2680/2688)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 30 |  Loss: (0.0133) | Acc: (99.62%) (3953/3968)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 40 |  Loss: (0.0138) | Acc: (99.60%) (5227/5248)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 50 |  Loss: (0.0166) | Acc: (99.49%) (6495/6528)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 60 |  Loss: (0.0175) | Acc: (99.47%) (7767/7808)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 70 |  Loss: (0.0172) | Acc: (99.49%) (9042/9088)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 80 |  Loss: (0.0174) | Acc: (99.49%) (10315/10368)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 90 |  Loss: (0.0173) | Acc: (99.48%) (11587/11648)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 100 |  Loss: (0.0170) | Acc: (99.48%) (12861/12928)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 110 |  Loss: (0.0173) | Acc: (99.47%) (14133/14208)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 120 |  Loss: (0.0169) | Acc: (99.50%) (15410/15488)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 130 |  Loss: (0.0175) | Acc: (99.48%) (16680/16768)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 140 |  Loss: (0.0172) | Acc: (99.47%) (17953/18048)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 150 |  Loss: (0.0180) | Acc: (99.46%) (19223/19328)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 160 |  Loss: (0.0184) | Acc: (99.45%) (20494/20608)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 170 |  Loss: (0.0188) | Acc: (99.43%) (21763/21888)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 180 |  Loss: (0.0186) | Acc: (99.43%) (23036/23168)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 190 |  Loss: (0.0188) | Acc: (99.44%) (24310/24448)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 200 |  Loss: (0.0190) | Acc: (99.44%) (25585/25728)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 210 |  Loss: (0.0187) | Acc: (99.45%) (26859/27008)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 220 |  Loss: (0.0184) | Acc: (99.46%) (28135/28288)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 230 |  Loss: (0.0180) | Acc: (99.48%) (29413/29568)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 240 |  Loss: (0.0182) | Acc: (99.47%) (30685/30848)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 250 |  Loss: (0.0180) | Acc: (99.48%) (31961/32128)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 260 |  Loss: (0.0180) | Acc: (99.48%) (33235/33408)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 270 |  Loss: (0.0178) | Acc: (99.49%) (34512/34688)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 280 |  Loss: (0.0175) | Acc: (99.50%) (35789/35968)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 290 |  Loss: (0.0174) | Acc: (99.50%) (37062/37248)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 300 |  Loss: (0.0175) | Acc: (99.50%) (38334/38528)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 310 |  Loss: (0.0176) | Acc: (99.49%) (39606/39808)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 320 |  Loss: (0.0175) | Acc: (99.49%) (40880/41088)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 330 |  Loss: (0.0175) | Acc: (99.48%) (42148/42368)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 340 |  Loss: (0.0173) | Acc: (99.48%) (43422/43648)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 350 |  Loss: (0.0173) | Acc: (99.48%) (44694/44928)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 360 |  Loss: (0.0173) | Acc: (99.48%) (45966/46208)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 370 |  Loss: (0.0172) | Acc: (99.48%) (47241/47488)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 380 |  Loss: (0.0171) | Acc: (99.48%) (48516/48768)\n",
      "#TRAIN: Epoch: 164 | Batch_idx: 390 |  Loss: (0.0170) | Acc: (99.49%) (49743/50000)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.5645) | Acc: (89.10%) (8910/10000)\n",
      "3 hours 0 mins 31 secs for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_modelsVGGonly'\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
    "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VGG()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('#TRAIN: Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 165):\n",
    "\n",
    "    if epoch < 80:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 120:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test()  \n",
    " \n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8b5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
